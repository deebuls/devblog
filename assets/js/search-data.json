{
  
    
        "post0": {
            "title": "Different types of uncertainty in DNN systems?",
            "content": "What are the uncertainties in DNN? . This question is easily answered if you have read a little bit of DNN literature. The most popular answer is . Aleatoric uncertainty | Epistemic uncertainty | . An additional statement is always added to these different types of uncertainty which will claim that . Aleatoric uncertatinty is model uncertainty | Epistemic uncertatinty is data uncertainty | . But what actually are these values and what do they explain ? Lets do a brief overview of different literature to answer these questions. . Alternative definitions . “Aleatory uncertainty is also referred to in the literature as variability, irreducible uncertainty, inherent uncertainty, and stochastic uncertainty. Epistemic uncertainty is also termed reducible uncertainty, subjective uncertainty, and state-of-knowledge uncertainty.” [2] . “Aleatory uncertainties are described as arising from inherent variabilities or randomness in systems, whereas epistemic uncertainties are due to imperfect knowledge.” [1] . Important points . Epistemic sources of uncertatinty is reducible but aleatory uncertainty is not reducible[1] | References . [1] Probability is Perfect, but we Can’t Elicitit Perfectly Anthony O’Hagan &amp; Jeremy E. Oakley http://www.yaroslavvb.com/papers/epistemic.pdf . [2] Challenge problems: uncertainty in system response given uncertain parameters Author links open overlay panelWilliam L.Oberkam https://www.sciencedirect.com/science/article/pii/S0951832004000493 .",
            "url": "https://deebuls.github.io/devblog/uncertainty/2020/05/29/Explanation-DNN-Uncertainty.html",
            "relUrl": "/uncertainty/2020/05/29/Explanation-DNN-Uncertainty.html",
            "date": " • May 29, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Regression Uncertainty",
            "content": "Regression . In machine learning problems we take input data $x$ and use it to predict the output data $y$. If the predicted data is continuous or discrete. When the output data $y$ is continuous then the problem is called regression while if its is discrete then the problem is called classification . . Deep neural networks have prooved to be a good tool for solving regression tasks. Currently there are new improvements in making the regression task robust by not only predicting the output data $y$ but also the corresponding uncertainty. . In this blog we will look into 2 methods of representing uncertatinty from literature. . Epistemic uncertatinty (Data uncertatinty) [1] | Aleotary uncertainty (Model uncertatinty) [2] | . Hierarchical Modelling . In regression task can be modelled in 3 different methods based on which all uncertainty needs to be accounted for during architecture of the deep network. The different layers and their hierarchichal structre is represented in the image above. . Name loss function uncertatinty output distribution . Deterministic Regression | mean square error | not modeled | $ hat{y}$ | - | . Likelihood Regression | Maximum likelihood estimation | data | $ hat{y}$, $ sigma^2$ | Normal | . Evidential Regression | Maximum likelihood estimation | data + model | $ hat{y}$,$ alpha$, $ beta$, $ lambda$ | Normal inverse gamma | . Dataset . We will start by creating a one dimension synthetic regression dataset. The synthetic dataset has added priori noise. The dataset is generated from $$ y = frac{ sin(3x)}{3x} + epsilon $$ where $ epsilon sim mathcal{N}(0, 0.02)$ . The training dataset consist of $x$ values between $-3 leq x leq 3$; while the test dataset consist of values outside of training $-4 leq x leq 4$ . Model . We will be using a common model for all the different learning formulation. . # this is one way to define a network class Net(torch.nn.Module): def __init__(self, n_feature, n_hidden, n_output): super(Net, self).__init__() self.hidden = torch.nn.Linear(n_feature, n_hidden) # hidden layer self.hidden_2 = torch.nn.Linear(n_hidden, n_hidden) self.predict = torch.nn.Linear(n_hidden, n_output) # output layer def forward(self, x): x = self.hidden(x) x = F.relu(self.hidden_2(x)) # activation function for hidden layer x = self.predict(x) # linear output return x . Deterministic Regression . Loss function . Mean square error $$ mathcal{L} = frac{1}{2} vert vert y_i - hat{y} vert vert ^2$$ . Output . The model will have single output $ hat{y}$ . msenet = Net(n_feature=1, n_hidden=10, n_output=1) # define the network print(msenet) # net architecture #optimizer = torch.optim.SGD(net.parameters(), lr=0.01) optimizer = torch.optim.Adam(msenet.parameters(), lr=0.001) loss_func = torch.nn.MSELoss() # this is for regression mean squared loss # train the network for t in range(10000): prediction = msenet(x) # input x and predict based on x loss = loss_func(prediction, y) # must be (1. nn output, 2. target) optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if t%1000 == 0: print(loss) prediction = msenet(test_x) plot_prediction(test_x, test_y, prediction) . Net( (hidden): Linear(in_features=1, out_features=10, bias=True) (hidden_2): Linear(in_features=10, out_features=10, bias=True) (predict): Linear(in_features=10, out_features=1, bias=True) ) tensor(0.5153, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0478, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0023, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) . Likelihood Regression . Loss function . In here we will use the maximum likelihood estimation error, which is also called as the Negative log likelihood. . $$ mathcal{L} = - left(-0.5 log(2 pi) - 0.5 sum log( sigma^2) - 0.5 sum frac{(y - hat{y})^2}{ sigma^2} right) $$ . Output . We will create a deep net with 2 output variables. . predicted output $ hat{y}$ | the variance of the normal distribution $ sigma^2$ | . Note . We aer also adding a regularizer $ frac{ | y - hat{y} | }{ sigma^2}$. This helps to regluarize the learning. It is inversely related to $ sigma^2$ and it scales based on distance from the actual value. . The regularizer ensures that values predicted far from true values have higher variance.So for values near to the predicted it gives less loss but values far away from prediction gives a large loss. . # Loss Function class MLELoss(torch.nn.Module): def __init__(self, weight=None, size_average=True): super(MLELoss, self).__init__() def forward(self, inputs, targets, smooth=1): targets = targets.view(-1) #converting variable to single dimension mu = inputs[:,0].view(-1) #extracting mu and sigma_2 logsigma_2 = inputs[:,1].view(-1) #logsigma and exp drives the variable to positive values always sigma_2 = torch.exp(logsigma_2) kl_divergence = (targets - mu)**2/sigma_2 #Regularizer mse = -0.5 * torch.sum(((targets - mu)**2)/sigma_2) sigma_trace = -0.5 * torch.sum(sigma_2) log2pi = -0.5 * np.log(2 * np.pi) J = mse + sigma_trace + log2pi loss = -J + kl_divergence.sum() return loss mlenet = Net(n_feature=1, n_hidden=10, n_output=2) # define the network print(mlenet) # net architecture optimizer = torch.optim.Adam(mlenet.parameters(), lr=0.001) loss_func = MLELoss() # this is for regression mean squared loss # train the network for t in range(10000): prediction = mlenet(x) # input x and predict based on x loss = loss_func(prediction, y) # must be (1. nn output, 2. target) optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if t%1000 == 0: print(loss) prediction = mlenet(test_x) mu = prediction[:,0] sigma2 = torch.exp(prediction[:,1]) sigma = torch.sqrt(sigma2) plot_prediction(test_x, test_y, mu, sigma) . Net( (hidden): Linear(in_features=1, out_features=10, bias=True) (hidden_2): Linear(in_features=10, out_features=10, bias=True) (predict): Linear(in_features=10, out_features=2, bias=True) ) tensor(68.1900, grad_fn=&lt;AddBackward0&gt;) tensor(6.4617, grad_fn=&lt;AddBackward0&gt;) tensor(6.1683, grad_fn=&lt;AddBackward0&gt;) tensor(6.1068, grad_fn=&lt;AddBackward0&gt;) tensor(6.0325, grad_fn=&lt;AddBackward0&gt;) tensor(5.9853, grad_fn=&lt;AddBackward0&gt;) tensor(5.9657, grad_fn=&lt;AddBackward0&gt;) tensor(5.9485, grad_fn=&lt;AddBackward0&gt;) tensor(5.9929, grad_fn=&lt;AddBackward0&gt;) tensor(5.9366, grad_fn=&lt;AddBackward0&gt;) . Evidential Regression . Evidential regression is based on paper [2] (Amini &amp; e.t.al, 2019), which is based on the ideas of [3, 4] that if we represent the output of the model with a higher order data distribution its possible to model the data and model uncertainties. . Loss . $$ mathcal{L} = left( frac{ Gamma( alpha - 0.5)}{4 Gamma( alpha) lambda sqrt beta} right) left( 2 beta(1 + lambda) + (2 alpha -1) lambda(y_i - hat{y})^2 right)$$ . output . The model has 4 outputs . $ hat{y}$ | $ alpha$ | $ beta$ | $ lambda$ | . Regularizer . Regularizer is required to penalize the loss function for OOD predictions. . $$ | y - hat{y} | ^2 (2 alpha + lambda)$$ . class EvidentialLoss(torch.nn.Module): def __init__(self, weight=None, size_average=True): super(EvidentialLoss, self).__init__() def forward(self, inputs, targets, smooth=1): targets = targets.view(-1) y = inputs[:,0].view(-1) #first column is mu,delta, predicted value loga = inputs[:,1].view(-1) #alpha logb = inputs[:,2].view(-1) #beta logl = inputs[:,3].view(-1) #lamda a = torch.exp(loga) b = torch.exp(logb) l = torch.exp(logl) term1 = (torch.exp(torch.lgamma(a - 0.5)))/(4 * torch.exp(torch.lgamma(a)) * l * torch.sqrt(b)) #print(&quot;term1 :&quot;, term1) term2 = 2 * b *(1 + l) + (2*a - 1)*l*(y - targets)**2 #print(&quot;term2 :&quot;, term2) J = term1 * term2 #print(&quot;J :&quot;, J) Kl_divergence = torch.abs(y - targets) * (2*a + l) #Kl_divergence = ((y - targets)**2) * (2*a + l) #print (&quot;KL &quot;,Kl_divergence.data.numpy()) loss = J + Kl_divergence #print (&quot;loss :&quot;, loss) return loss.mean() evnet = Net(n_feature=1, n_hidden=10, n_output=4) # define the network print(evnet) # net architecture optimizer = torch.optim.Adam(evnet.parameters(), lr=0.001) loss_func = EvidentialLoss() # train the network for t in range(20000): prediction = evnet(x) # input x and predict based on x loss = loss_func(prediction, y) # must be (1. nn output, 2. target) optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if t%10000 == 0: print(loss) prediction = evnet(test_x) mu = prediction[:,0].view(-1) #first column is mu,delta, predicted value a = prediction[:,1].view(-1) #alpha b = prediction[:,2].view(-1) #beta l = prediction[:,3].view(-1) #lamda a = torch.exp(a); b = torch.exp(b); l = torch.exp(l) var = b / ((a -1)*l) #epistemic/ model/prediciton uncertaitnty e = b / (a - 1) # aleatoric uncertainty/ data uncertainty plot_prediction(test_x, test_y, mu, var, e) . Net( (hidden): Linear(in_features=1, out_features=10, bias=True) (hidden_2): Linear(in_features=10, out_features=10, bias=True) (predict): Linear(in_features=10, out_features=4, bias=True) ) tensor(3.1019, grad_fn=&lt;MeanBackward0&gt;) tensor(0.1492, grad_fn=&lt;MeanBackward0&gt;) . Conclusions . Loss functions of maximum likelihood and evidence have been implemented from formulas from the paper. . Important: Leasons Learned - how to treat loss function when one of the output variable is always positive. (log and exponential to rescue) -The difference between optimize and loss function and do you sum and add constant ones or do you add constant in all equation and sum. | The trianing is working on some runs not all. | The model learns the function easily but learning the uncertainty in unkonw region takes longer number of iterations. Needs to be further investigated. . Note: ToDo : Test functions for the loss function written . Note: ToDo : What is stopping condition? when to stop learning? . Note: ToDo : Replace the regularizer in regularizer in likelihood loss with actual regularizer. . Warning: Why is uncertatiny different for both the models. Evidential model seems to be very confident in the known region, which seems to be fishy . Warning: The loss function doesnt give same results all the time, so needs to be further investigated | . References . [1] Kendall, Alex, and Yarin Gal. “What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?” ArXiv:1703.04977 [Cs], October 5, 2017. http://arxiv.org/abs/1703.04977. . [2] Amini, Alexander, Wilko Schwarting, Ava Soleimany, and Daniela Rus. “Deep Evidential Regression.” ArXiv:1910.02600 [Cs, Stat], October 7, 2019. http://arxiv.org/abs/1910.02600. . [3] Sensoy, Murat, Lance Kaplan, and Melih Kandemir. “Evidential Deep Learning to Quantify Classification Uncertainty,” n.d., 11. . [4] Malinin, Andrey, and Mark Gales. &quot;Predictive uncertainty estimation via prior networks.&quot; Advances in Neural Information Processing Systems. 2018. .",
            "url": "https://deebuls.github.io/devblog/probability/regression/uncertainty/2020/05/27/uncertainty-regression.html",
            "relUrl": "/probability/regression/uncertainty/2020/05/27/uncertainty-regression.html",
            "date": " • May 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Dataset shift",
            "content": "Dataset shift . Dataset shift is still an unsolved problems when it comes to deploying learning models “in the wild”. . There are 2 different categories of dataset shift . Co-variate shift | label shift | . Let $ mathbb{S} $ be the source data distribution and $ mathbb{T} $ be the target data distribution. If we denote the input variables as x and output variables as y, then . Covariate shift . s(x)≠t(x)s(x) neq t(x)s(x)​=t(x) input distribution of both source and target are different . but . s(y∣x)=t(y∣x)s(y|x) = t(y|x)s(y∣x)=t(y∣x) conditional output distirbution is invariant to dataset shift. . Label shift . s(y)≠t(y)s(y) neq t(y)s(y)​=t(y) output distribution of both source and target are different . but . s(x∣y)=t(x∣y)s(x|y) = t(x|y)s(x∣y)=t(x∣y) conditional input distirbution is invariant to dataset shift .   Covariate Shift Label Shift . input distribution | $s(x) neq t(x)$ | $?$ | . output distribution | $?$ | $s(y) neq t(y)$ | . conditional output distribution | $s(y vert x) = t(y vert x)$ | $?$ | . conditional input Distribution | $?$ | ${s(x vert y) = t(x vert y)}$ | . Examples . ToDo . Simluated Dataset {ReDo with examples} . The problem can be simulated in image based calssification dataset like MNIST and CIFAR. . Tweak-One shift . refers to the case where we set a class to have probability $ p &gt; 0.1$, while the distribution over the rest of the classes is uniform. . Minority-Class Shiftis . A more general version of Tweak-One shift, where a fixed number of classes to have probability $p &lt; 0.1$, while the distribution over the rest of the classes isuniform. . Dirichlet shift . we draw a probability vector $p$ from the Dirichlet distribution with concentration parameter set to $ alpha$ for all classes, before including sample points which correspond to the multinomial label variable according top. .",
            "url": "https://deebuls.github.io/devblog/dataset/2020/05/21/Explanation-Learning-DatasetShift.html",
            "relUrl": "/dataset/2020/05/21/Explanation-Learning-DatasetShift.html",
            "date": " • May 21, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Plotting Normal Inverse Gamma Distirbution",
            "content": ". Scipy stats doesnt have Normal Inverse Gamma distirbution. . We would like to incorporate Normal Inverse Gamma distirbution in &quot;scipy.stats&quot; package. . Learning about Normal Inverse Gamma(NIG) distribution will lead you to a plot like this from wikipedia. . . It was intruiging enough to find out how to plot this graph in python and was sure that there will be some already plots available. But to my suprise there is no blogs or docs to plot NIG in python. The closest I found was in R langugage in [1] by Frank Portman. . So I spent some time to plot NIG in python below is the snippet for it. Special thanks to Jake Vadendeplas[2] for his wonderful blogs about visualization in python. . Normal Inverse Gamma Distribution . Let the input $x$ on which its modelled be : $$ x = [ mu, sigma^2] $$ . Probability density function (PDF) . $$ f(x | delta, alpha, beta, lambda ) = sqrt{ left( frac{ lambda}{2 pi x[ sigma^2]} right)} frac{ beta^ alpha}{ Gamma( alpha)} left( frac{1}{x[ sigma^2]} right)^{( alpha + 1)} exp{ left( - frac{2 beta + lambda left(x[ mu] - delta right)^2 }{ 2 x[ sigma]^2} right)} $$ . from scipy.stats import rv_continuous from scipy.stats import norm from scipy.stats import gengamma from scipy.special import gamma from scipy.stats import expon import numpy as np %matplotlib inline import matplotlib.pyplot as plt plt.style.use(&#39;seaborn-white&#39;) class norminvgamma(): r&quot;&quot;&quot;A normal inverse gamma random variable. The mu (``mu``) keyword specifies the parmaeter mu. %(before_notes)s Notes -- The probability density function for `norminvgamma` is: .. math:: x = [ mu, sigma^2] f(x | delta, alpha, beta, lamda) = sqrt( frac{ lamda}{2 pi x[ sigma^2}]) frac{ beta^ alpha}{ gamma( alpha)} frac{1}{x[ sigma^2]}^( alpha + 1) exp(- frac{2 beta + lamda(x[ mu] - delta)^2}{2 x[ sigma^2] }) for a real number :math:`x` and for positive number :math: ` sigma^2` &gt; 0 %(after_notes)s %(example)s &quot;&quot;&quot; def __init__(self, delta, alpha, beta, lamda): self.argcheck(delta, alpha, beta, lamda) self.delta = delta self.alpha = alpha self.beta = beta self.lamda = lamda def argcheck(self, delta, alpha, beta, lamda): return (alpha &gt; 0) def rvs(self, size=1): sigma_2 = gengamma.rvs(self.alpha, self.beta, size=size) sigma_2 = np.array(sigma_2) return [[norm.rvs(self.delta, s/self.lamda), s] for s in sigma_2] def pdf(self, xmu, xsigma2): t1 = ((self.lamda)**0.5) * ((self.beta)**self.alpha) t2 = (xsigma2 * (2 * 3.15)**0.5) * gamma(self.alpha) t3 = (1 / xsigma2**2)**(self.alpha + 1) t4 = expon.pdf((2*self.beta + self.lamda*(self.delta-xmu)**2)/(2*xsigma2**2)) #print (t1, t2, t3, t4) return (t1/t2)*t3*t4 def stats(self): #ToDo return def plot(self,zoom=0.9, axs=None): steps = 50 max_sig_sq = gengamma.ppf(zoom, self.alpha, self.beta) * self.lamda #print(max_sig_sq) mu_range = np.linspace(self.delta - 1 * max_sig_sq, self.delta + 1 * max_sig_sq, num=steps) #print (mu_range[0], mu_range[-1]) sigma_range = np.linspace(0.01, max_sig_sq, num=steps) mu_grid, sigma_grid = np.meshgrid(mu_range, sigma_range) pdf_mesh = self.pdf(mu_grid, sigma_grid) if axs: contours = axs.contour(mu_grid, sigma_grid, pdf_mesh, 20, cmap=&#39;RdGy&#39;); plt.clabel(contours, inline=True, fontsize=8) #extent=[mu_range[0], mu_range[-1], sigma_range[0], sigma_range[-1]] axs.imshow(pdf_mesh, extent=[mu_range[0], mu_range[-1], sigma_range[0], sigma_range[-1]], origin=&#39;lower&#39;, cmap=&#39;Blues&#39;, alpha=0.5) axs.axis(&#39;equal&#39;) axs.set_title(&quot;(&quot;+str(self.delta)+&quot;,&quot;+str(self.alpha)+&quot;,&quot; +str(self.beta)+&quot;,&quot;+str(self.lamda)+&quot;)&quot;) #plt.colorbar(); else: assert True, &quot;Pass the axes to plot from matplotlib&quot; . Varying different range of $ alpha$ . #norminvgamma = norminvgamma_gen() fig, axs = plt.subplots(1, 3, sharey=True, figsize=(15,5)) fig.suptitle(&#39;Vertically alpha&#39;) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=1) samples = nig.rvs(size=10) nig.plot(axs=axs[0]) nig = norminvgamma(delta=0,alpha=2,beta=1, lamda=1) nig.plot(zoom=0.7, axs=axs[1]) nig = norminvgamma(delta=0,alpha=4,beta=1, lamda=1) nig.plot(zoom=0.2, axs=axs[2]) . Varying different range of $ beta$ . fig, axs = plt.subplots(1, 3, sharey=True, figsize=(15,5)) fig.suptitle(&#39;Varying beta&#39;) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=1) samples = nig.rvs(size=10) nig.plot(axs=axs[0]) nig = norminvgamma(delta=0,alpha=1,beta=2, lamda=1) nig.plot( axs=axs[1]) nig = norminvgamma(delta=0,alpha=1,beta=3, lamda=1) nig.plot(axs=axs[2]) . Varying different range of $ lambda$ . fig, axs = plt.subplots(1, 3, sharey=True, figsize=(15,5)) fig.suptitle(&#39;Verying lamda &#39;) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=1) samples = nig.rvs(size=10) nig.plot(axs=axs[0]) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=2) nig.plot( axs=axs[1]) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=4) nig.plot(axs=axs[2]) . References . https://frankportman.github.io/bayesAB/reference/plotNormalInvGamma.html | https://jakevdp.github.io/PythonDataScienceHandbook/04.04-density-and-contour-plots.html |",
            "url": "https://deebuls.github.io/devblog/probability/python/plotting/matplotlib/2020/05/19/probability-normalinversegamma.html",
            "relUrl": "/probability/python/plotting/matplotlib/2020/05/19/probability-normalinversegamma.html",
            "date": " • May 19, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Regression Uncertainty Robustness",
            "content": "Robustness . Robustness . Robustness during training vs testing . fig, ax = plt.subplots(figsize=(5,5)) ax.scatter(x.data.numpy(),y.data.numpy()) ax.axis(&#39;equal&#39;) ax.set_xlabel(&#39;$x$&#39;) ax.set_ylabel(&#39;$y$&#39;) . Text(0, 0.5, &#39;$y$&#39;) . Mean Square Loss robustness analysis . loss_func = torch.nn.MSELoss() # this is for regression mean squared loss # Fit a linear regression using mean squared error. regression = Net(n_feature=1, n_hidden=1, n_output=1) # RegressionModel() params = regression.parameters() optimizer = torch.optim.Adam(params, lr = 0.001) . Analysis . Uncertainty Loss Robustness . Smaller comparable network . The learning is general . Larger network 100 nodes . Plotting Loss Surface . sigma_2 = np.logspace(0.1, 1, num=70, base=10 ) print (sigma_2.max(), sigma_2.min()) diff = np.linspace(-3, 5, num=70) def gauss_logL(xbar, sigma_2, mu): return -0.5*np.log(2*np.pi)-0.5*np.log(sigma_2)-0.5*(xbar -mu)**2/sigma_2 xbar = 1 logL = gauss_logL(xbar, sigma_2[:, np.newaxis], diff) logL -= logL.max() x_grid, sigma_grid = np.meshgrid(diff, sigma_2) logL = gauss_logL(xbar, sigma_grid, x_grid) logL = logL*-1 . 10.0 1.2589254117941673 . fig, ax = plt.subplots(figsize=(10,8),constrained_layout=True) ax = plt.axes(projection=&#39;3d&#39;) ax.plot_surface(x_grid, sigma_grid, logL, rstride=1, cstride=1, cmap=&#39;viridis&#39;, edgecolor=&#39;none&#39;) ax.set_title(&#39;NLL loss surface&#39;); . from itertools import cycle cycol = cycle(&#39;bgrcmk&#39;) fig, ax = plt.subplots(figsize=(10,7)) #for s_2 in [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 2]: for s_2 in [1e-3, 1e-2, 2e-3]: x = np.linspace(-3, 3, num=100) logL = (-1*gauss_logL(0, s_2, x))/1000 ax.plot(x, 0.5*x**2, color=&quot;cyan&quot;, lw=3, alpha=0.5) ax.plot(x,logL, c=next(cycol)) .",
            "url": "https://deebuls.github.io/devblog/probability/regression/uncertainty/robustness/2020/05/05/uncertainty-regression-robustness.html",
            "relUrl": "/probability/regression/uncertainty/robustness/2020/05/05/uncertainty-regression-robustness.html",
            "date": " • May 5, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Likelihood",
            "content": "What is Likelihood? . Likelihood and probablity seems to be same word in the layman domain, but in the stats domain they are different. . In the stats domain the likelihood or likelihood function is a measurement. It measures the distance between a statistical model and the input data. . What is a statistical model? . The diferent probability distributions available. For example, Gausian, gama, beta distribution, exponential for continuous data while Bernoulli, Dirichlet, multinomila distributions for discrete data. . How are statistical models represented? . By their parameters. For example for gaussian distribution the parameters are $ mu$ and $ sigma$ . . How do we select the statictical model? . Depends on many factors. This is the main decision to be made while designing a statistical model based learning. The different factors include: what is the data type: Continuous or discrete? Is it symmetrical or asymetrical? . Domain of the data, binary, real, etc | Does it decay or increase? | . . . etc | . A complete knowledge about the type data and the type of distribution is required to make the appropriate decision. . Common Probability distribution . Data Type Domain Distribution Python (numpy.random) Parameters . univariate, discrete, binary | $$ x in {0,1 } $$ | Bernoulli | binomial(1, p) | $$ p in[0,1]$$ | . univariate, discrete, multivalued | $$ x in { 1,2, dots, K }$$ | multinomial | multinomial(n, pvals) | $$pvals = [p_1, dots , p_k] $$ $$ sum_{i=1}^{K} p_i = 1 $$ | . univariate, continuous, unbounded | $$ x in mathbb{R} $$ | normal | normal(mu, sigma) | $$ mu in mathbb{R} $$ $$ sigma in mathbb{R}$$ | . #Lets make some distributions and find the likelihood to some data import numpy as np import matplotlib.pyplot as plt number_of_samples = 20; #parameters ; sample data from distribution (continuous data) mu, sigma = 12, 0.1 ; univariate_gaussian_samples = np.random.normal(mu, sigma, number_of_samples) mean = [0, 0]; cov = [[1, 0], [0, 100]]; multivariate_gaussian_samples = np.random.multivariate_normal(mean, cov, number_of_samples) #parameters ; sample data from distribution (discreta data) p = 0.8 ; bernoulli_samples = np.random.binomial(1, p, number_of_samples) pvals = [0.2, 0.6, 0.2] ; multinomial_samples = np.random.multinomial(number_of_samples, pvals) alpha, beta = 10, 20 ; beta_samples = np.random.beta(alpha, beta, number_of_samples) alpha = [10,20,10,90] ; dirchilet_samples = np.random.dirichlet(alpha, number_of_samples) . Goal of Likelihood . The goal of likelihood would be given the samples as shown above (beta_samples, dirichlet_samples etc) find the parameters of the corresponding distribution ((alpha, beta), alphas respectively) . Lets look into this process in the comming post . #hide .",
            "url": "https://deebuls.github.io/devblog/probability/python/2020/03/20/probability-likelihood.html",
            "relUrl": "/probability/python/2020/03/20/probability-likelihood.html",
            "date": " • Mar 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I’m a researcher at the Bonn-Aachen International Center for Information Technology (b-it) at the Autonomous Systems group in Bonn, Germany. . My research focus is on developing strategies for safe and reliable incorporation of deep learning methods into robotics. . I am also the lead develper with the b-it-bots team which won the RoboCup WorldCup and GermanOpen in the work league. . Publications . b-it-bots: Our Approach for Autonomous Robotics in Industrial Environments . Deebul Nair, Santosh Thoduka, Iman Awaad, Sven Schneider, Paul G. Plöger, Gerhard K. Kraetzschmar and students . Robot World Cup. Springer, Cham, 2019. . This paper presents the approach of our team, b-it-bots, in the RoboCup@Work competition which resulted in us winning the World Championship in Sydney in 2019. We describe our current hardware, including modifications made to the KUKA youBot, the underlying software framework and components developed for navigation, manipulation, perception and task planning for scenarios in industrial environments. Our combined 2D and 3D approach for object recognition has improved robustness and performance compared to previous years, and our task planning framework has moved us away from large state machines for high-level control. Future work includes closing the perception-manipulation loop for more robust grasping. Our open-source repository is available at https://github.com/b-it-bots/mas_industrial_robotics. . Performance Evaluation of Low-Cost Machine Vision Cameras forImage-Based Grasp Verification . Performance Evaluation of Low-Cost Machine Vision Cameras forImage-Based Grasp Verification . Under review in IROS 2020 . Grasp verification is advantageous for au-tonomous manipulation robots as they provide the feedbackrequired for higher level planning components about successfultask completion. However, a major obstacle in doing graspverification is sensor selection. In this paper, we propose a visionbased grasp verification system using machine vision cameras,with the verification problem formulated as an image classifi-cation task. Machine vision cameras consist of a camera anda processing unit capable of on-board deep learning inference.The inference in these low-power hardware are done near thedata source, reducing the robots dependence on a centralizedserver, leading to reduced latency, and improved reliability.Machine vision cameras provide the deep learning inferencecapabilities using different neural accelerators. Although, it isnot clear from the documentation of these cameras what is theeffect of these neural accelerators on performance metrics suchas latency and throughput. To systematically benchmark thesemachine vision cameras, we propose a parameterized modelgenerator that generates end to end models of ConvolutionalNeural Networks(CNN). Using these generated models webenchmark latency and throughput of two machine visioncameras, JeVois A33 and Sipeed Maix Bit. Our experimentsdemonstrate that the selected machine vision camera and thedeep learning models can robustly verify grasp with 97% perframe accuracy. . Open Source Contributions . BayesPy – Bayesian Python . BayesPy provides tools for Bayesian inference with Python. The user constructs a model as a Bayesian network, observes data and runs posterior inference. The goal is to provide a tool which is efficient, flexible and extendable enough for expert use but also accessible for more casual users. . b-it-bots . ROS software packages of b-it-bots for different robots. . . This website is powered by fastpages. Logo and favicon was designed by Darius Dan .",
          "url": "https://deebuls.github.io/devblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://deebuls.github.io/devblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}