{
  
    
        "post0": {
            "title": "Quantization of Pytorch Models",
            "content": ". Quantization of Pytorch Models . What is quantization . Quantization describes methods for carrying out calculations and storing tensors at smaller bit width than floating point precision. The default size of floating point numbers are 32 bits. | For instance, quantizing the deep learning model means, converting the 32-bit floating point numbers (of weights &amp; activation outputs) to 8-bit integers. | . Types of quantization . Post Training Quantization (PTQ) Static | Dynamic/Weight only | | Quantization Aware Training (QAT) Static | | . Pros Cons . Model gets smaller | Potential for little degradation in accuracy | . Reduced memory usage during inferencing | | . Improves hardware accelerator latency | | . Reduces inference latency | | . Deployment on Edge AI devices with limited memory | | . PyTorch Quantisation approach . G PyTorch nQuantisation Aware Training nTorch model PyTorch Quantisation Aware Training Torch model ONNX model ONNX model PyTorch nQuantisation Aware Training nTorch model-&gt;ONNX model Torch API PyTorch nPost Training Quantisation n(Dynamic/Weight only) nTorch model PyTorch Post Training Quantisation (Dynamic/Weight only) Torch model PyTorch nPost Training Quantisation n(Dynamic/Weight only) nTorch model-&gt;ONNX model Torch API PyTorch nPost Training Quantisation n(Static) nTorch model PyTorch Post Training Quantisation (Static) Torch model PyTorch nPost Training Quantisation n(Static) nTorch model-&gt;ONNX model Torch API Blob model Blob model ONNX model-&gt;Blob model Blob converter API TF model TF model ONNX model-&gt;TF model ONNX API TFLite model TFLite model TF model-&gt;TFLite model TensorFlow API EdgeTPU TFLite model EdgeTPU TFLite model TFLite model-&gt;EdgeTPU TFLite model TensorFlow API ONNX Quantisation approach . G PyTorch nStandard Training nTorch model PyTorch Standard Training Torch model ONNX model ONNX model PyTorch nStandard Training nTorch model-&gt;ONNX model Torch API Quantised nONNX model Quantised ONNX model ONNX model-&gt;Quantised nONNX model ONNX API Blob model Blob model Quantised nONNX model-&gt;Blob model Blob converter API TF model TF model Quantised nONNX model-&gt;TF model ONNX API TFLite model TFLite model TF model-&gt;TFLite model TensorFlow API EdgeTPU TFLite model EdgeTPU TFLite model TFLite model-&gt;EdgeTPU TFLite model TensorFlow API API . PyTorch Quantization (QAT) https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html | import torch | from torchvision.models import MobileNetV2 model = MobileNetV2() | . | Fuse relu &amp; Conv2d | Insert Stubs to model model = nn.Sequential(torch.quantization.QuantStub(), model, torch.quantization.DeQuantStub()) | . | Prepare model m.train() | backend = &quot;fbgemm&quot; | model.qconfig = torch.quantization.get_default_qconfig(backend) | torch.quantization.prepare_qat(model, inplace=True) | . | Run standard training loop | Convert m.eval() | model_quantized = torch.quantization.convert(model, inplace=True) | torch.save(model_quantized, model_file_path) | . | . | PyTorch -&gt; ONNX https://pytorch.org/docs/stable/onnx.html | import torch | torch.onnx.export(model, sample_input, onnx_model_path, opset_version=12, input_names=[&#39;input&#39;], output_names=[&#39;output&#39;]) | . | ONNX Qunatization (Dynamic) https://onnxruntime.ai/docs/performance/quantization.html | import onnx | from onnxruntime.quantization import quantize_dynamic, QuantType | quantized_model = quantize_dynamic(model_path, quantised_model_path) | . | ONNX -&gt; Blob https://docs.luxonis.com/en/latest/pages/tutorials/creating-custom-nn-models/ | import blobconverter | onnx_model = onnx.load(&quot;./results/networks/test1.onnx&quot;) model_simpified, check = simplify(onnx_model) onnx.save(model_simpified, &quot;./results/networks/test_sim1.onnx&quot;) | blobconverter.from_onnx(model=onnx_model_path, data_type=&quot;FP16&quot;, shaves=6, use_cache=False, output_dir=blob_model_path, optimizer_params=[]) | . | ONNX -&gt; TF https://github.com/onnx/onnx-tensorflow/blob/main/example/onnx_to_tf.py | import onnx | from onnx_tf.backend import prepare | onnx_model = onnx.load(onnx_model_path) | tf_rep = prepare(onnx_model) | tf_rep.export_graph(tf_model_path) | . | TF -&gt; TFLite https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter | import tensorflow as tf | converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path) | tflite_model = converter.convert() | with open(tflite_model_path, &#39;wb&#39;) as f: f.write(tflite_model) | . | TFLite -&gt; EdgeTPU TFLite . https://coral.ai/docs/edgetpu/compiler/ | curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - | echo &quot;deb https://packages.cloud.google.com/apt coral-edgetpu-stable main&quot; | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list | sudo apt-get update | sudo apt-get install edgetpu-compiler | edgetpu_compiler [options] tflite_model_path | . | PyTorch Quantization (PTQ - Dynamic/Weight only) . https://pytorch.org/blog/quantization-in-practice/ | . | PyTorch Quantization (PTQ - Static) https://pytorch.org/blog/quantization-in-practice/ | . | !rm -rf /tmp/MobileNet* !ls -lh /tmp/MobileNet* . ls: cannot access &#39;/tmp/MobileNet*&#39;: No such file or directory . torch_model_path = &quot;/tmp/MobileNetV2.pt&quot; torch_QAT_quant_path = &quot;/tmp/MobileNetV2_TorchQATQuant.pt&quot; onnx_model_path = &quot;/tmp/MobileNetV2.onnx&quot; onnx_quant_model_path = &quot;/tmp/MobileNetV2_OnnxQuant.onnx&quot; onnx_sim_model_path = &quot;/tmp/MobileNetV2_OnnxSim.onnx&quot; blob_model_path = &quot;/tmp/MobileNetV2.blob&quot; tf_model_path = &quot;/tmp/MobileNetV2.tf&quot; tflite_model_path = &quot;/tmp/MobileNetV2.tflite&quot; edgetpu_tflite_model_path = &quot;/tmp/MobileNetV2_edgetpu.tflite&quot; torch_PTQ_Weight_Eager_path = &quot;/tmp/MobileNet_V2_Torch_PTQ_Quant_W_EG.pt&quot; torch_PTQ_Weight_FX_path = &quot;/tmp/MobileNet_V2_Torch_PTQ_Quant_W_FX.pt&quot; torch_PTQ_Static_Eager_path = &quot;/tmp/MobileNet_V2_Torch_PTQ_Quant_S_EG.pt&quot; torch_PTQ_Static_FX_path = &quot;/tmp/MobileNet_V2_Torch_PTQ_Quant_S_FX.pt&quot; . PyTorch Model . import os import torch import torchvision import torchvision.transforms as transforms from torchvision.models import MobileNetV2 . model = MobileNetV2() data_dir = os.path.abspath(&quot;./data&quot;) transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.RandomErasing()]) trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform) train_sub_len = int(len(trainset) * 0.001) train_subset, val_subset = torch.utils.data.random_split(trainset, [train_sub_len, len(trainset) - train_sub_len]) trainloader = torch.utils.data.DataLoader(train_subset, batch_size=4, shuffle=True, num_workers=2) n_epochs = 3 opt = torch.optim.SGD(model.parameters(), lr=0.1) loss_fn = torch.nn.CrossEntropyLoss() for epoch in range(n_epochs): for images, labels in trainloader: opt.zero_grad() out = model(images) loss = loss_fn(out, labels) loss.backward() opt.step() . Files already downloaded and verified . torch.save(model, torch_model_path) !ls -lh /tmp/MobileNet* . -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2.pt . 1. PyTorch Quantization (QAT) . import os import torch import torchvision import torchvision.transforms as transforms from torchvision.models import MobileNetV2 data_dir = os.path.abspath(&quot;./data&quot;) transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.RandomErasing()]) trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform) train_sub_len = int(len(trainset) * 0.001) train_subset, val_subset = torch.utils.data.random_split(trainset, [train_sub_len, len(trainset) - train_sub_len]) trainloader = torch.utils.data.DataLoader(train_subset, batch_size=4, shuffle=True, num_workers=2) model = MobileNetV2() &quot;&quot;&quot;Fuse&quot;&quot;&quot; # pair_of_modules_to_fuze = [] # for name, layer in model.named_modules(): # if isinstance(layer, torch.nn.Linear): # pair_of_modules_to_fuze.append([name.split(&#39;.&#39;)[-1]]) # elif isinstance(layer, torch.nn.ReLU) and len(pair_of_modules_to_fuze) &gt; 0: # pair_of_modules_to_fuze[-1].append(name.split(&#39;.&#39;)[-1]) # pair_of_modules_to_fuze = list(filter(lambda x: len(x) == 2, pair_of_modules_to_fuze)) # torch.quantization.fuse_modules(model.modules(), pair_of_modules_to_fuze, inplace=True) &quot;&quot;&quot;Insert stubs&quot;&quot;&quot; model = torch.nn.Sequential(torch.quantization.QuantStub(), model, torch.quantization.DeQuantStub()) &quot;&quot;&quot;Prepare&quot;&quot;&quot; model.train() model.qconfig = torch.quantization.get_default_qconfig(&quot;fbgemm&quot;) torch.quantization.prepare_qat(model, inplace=True) &quot;&quot;&quot;Training Loop&quot;&quot;&quot; n_epochs = 3 opt = torch.optim.SGD(model.parameters(), lr=0.1) loss_fn = torch.nn.CrossEntropyLoss() for epoch in range(n_epochs): for inputs, labels in trainloader: opt.zero_grad() out = model(inputs) loss = loss_fn(out, labels) loss.backward() opt.step() &quot;&quot;&quot;Convert&quot;&quot;&quot; model.eval() model_quantized = torch.quantization.convert(model, inplace=True) torch.save(model_quantized, torch_QAT_quant_path) . Files already downloaded and verified . /usr/local/lib/python3.8/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers. reduce_range will be deprecated in a future release of PyTorch. warnings.warn( . !ls -lh /tmp/MobileNet* . -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2.pt -rw-r--r-- 1 root root 4.1M Jan 20 14:20 /tmp/MobileNetV2_TorchQATQuant.pt . 2. PyTorch to ONNX . model = torch.load(torch_model_path) torch.onnx.export(model, images, onnx_model_path, export_params=True, do_constant_folding=True, input_names = [&#39;input&#39;], output_names = [&#39;output&#39;], dynamic_axes={&#39;input&#39; : {0 : &#39;batch_size&#39;}, &#39;output&#39; : {0 : &#39;batch_size&#39;}}) !ls -lh /tmp/MobileNet* . -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2.onnx -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2.pt -rw-r--r-- 1 root root 4.1M Jan 20 14:20 /tmp/MobileNetV2_TorchQATQuant.pt . 3. ONNX Quantization (Dynamic) . !pip install onnx -q !pip install onnxruntime -q from onnxruntime.quantization import quantize_dynamic, QuantType . ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.5/13.5 MB 108.5 MB/s eta 0:00:00 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 66.6 MB/s eta 0:00:00 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow 2.9.2 requires protobuf&lt;3.20,&gt;=3.9.2, but you have protobuf 3.20.3 which is incompatible. tensorboard 2.9.1 requires protobuf&lt;3.20,&gt;=3.9.2, but you have protobuf 3.20.3 which is incompatible. ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 89.5 MB/s eta 0:00:00 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 KB 4.9 MB/s eta 0:00:00 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 KB 9.5 MB/s eta 0:00:00 . quantized_model = quantize_dynamic(onnx_model_path, onnx_quant_model_path) . 4. ONNX to Blob . !pip install onnxsim -q !pip install Flask==2.1.0 PyYAML==5.4.1 boto3==1.17.39 gunicorn==20.1.0 sentry-sdk -q !pip install blobconverter -q . ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 66.8 MB/s eta 0:00:00 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 238.9/238.9 KB 21.2 MB/s eta 0:00:00 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 KB 9.4 MB/s eta 0:00:00 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.2/95.2 KB 9.6 MB/s eta 0:00:00 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 662.4/662.4 KB 29.8 MB/s eta 0:00:00 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 131.4/131.4 KB 14.9 MB/s eta 0:00:00 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 KB 9.5 MB/s eta 0:00:00 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.4/177.4 KB 15.3 MB/s eta 0:00:00 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.7/232.7 KB 21.5 MB/s eta 0:00:00 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.6/96.6 KB 11.3 MB/s eta 0:00:00 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 KB 15.1 MB/s eta 0:00:00 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 KB 7.3 MB/s eta 0:00:00 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/7.7 MB 77.3 MB/s eta 0:00:00 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 KB 14.0 MB/s eta 0:00:00 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorboard 2.9.1 requires protobuf&lt;3.20,&gt;=3.9.2, but you have protobuf 3.20.3 which is incompatible. notebook 5.7.16 requires jinja2&lt;=3.0.0, but you have jinja2 3.1.2 which is incompatible. . import onnx from onnxsim import simplify import blobconverter . model_simpified, check = simplify(onnx_model_path) onnx.save(model_simpified, onnx_sim_model_path) . # blobconverter.from_onnx( # model=onnx_sim_model_path, # data_type=&quot;FP16&quot;, # shaves=6, # use_cache=False, # output_dir=blob_model_path, # optimizer_params=[]) !ls -lh /tmp/MobileNet* . -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2.onnx -rw-r--r-- 1 root root 3.6M Jan 20 14:20 /tmp/MobileNetV2_OnnxQuant.onnx -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2_OnnxSim.onnx -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2.pt -rw-r--r-- 1 root root 4.1M Jan 20 14:20 /tmp/MobileNetV2_TorchQATQuant.pt . 5. ONNX to TF . !pip install onnx-tf -q . ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.1/226.1 KB 16.3 MB/s eta 0:00:00 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 53.7 MB/s eta 0:00:00 . from onnx_tf.backend import prepare import tensorflow_probability onnx_model = onnx.load(onnx_model_path) tf_rep = prepare(onnx_model) tf_rep.export_graph(tf_model_path) . WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading. . !ls -lh /tmp/MobileNet* . -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2.onnx -rw-r--r-- 1 root root 3.6M Jan 20 14:20 /tmp/MobileNetV2_OnnxQuant.onnx -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2_OnnxSim.onnx -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2.pt -rw-r--r-- 1 root root 4.1M Jan 20 14:20 /tmp/MobileNetV2_TorchQATQuant.pt /tmp/MobileNetV2.tf: total 14M drwxr-xr-x 2 root root 4.0K Jan 20 14:21 assets -rw-r--r-- 1 root root 14M Jan 20 14:21 saved_model.pb drwxr-xr-x 2 root root 4.0K Jan 20 14:21 variables . 6. TF to TFLite . import tensorflow as tf import numpy as np # def fake_dataset_generator(shape, n_iter): # def dataset(): # for _ in range(n_iter): # data = np.random.randn(*shape) # data *= (1 / 255) # batch = np.expand_dims(data, axis=0) # yield [batch.astype(np.float32)] # return dataset # datagen = fake_dataset_generator((192, 192, 3), 10) converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path) # converter.representative_dataset = datagen # converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # converter.inference_input_type = tf.uint8 # converter.inference_output_type = tf.uint8 # converter.optimizations = [tf.lite.Optimize.DEFAULT] tflite_model = converter.convert() with open(tflite_model_path, &#39;wb&#39;) as f: f.write(tflite_model) . !ls -lh /tmp/MobileNet* . -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2.onnx -rw-r--r-- 1 root root 3.6M Jan 20 14:20 /tmp/MobileNetV2_OnnxQuant.onnx -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2_OnnxSim.onnx -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2.pt -rw-r--r-- 1 root root 14M Jan 20 14:22 /tmp/MobileNetV2.tflite -rw-r--r-- 1 root root 4.1M Jan 20 14:20 /tmp/MobileNetV2_TorchQATQuant.pt /tmp/MobileNetV2.tf: total 14M drwxr-xr-x 2 root root 4.0K Jan 20 14:21 assets -rw-r--r-- 1 root root 14M Jan 20 14:21 saved_model.pb drwxr-xr-x 2 root root 4.0K Jan 20 14:21 variables . 7. TFLite to EdgeTPU TFLite . !curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - !echo &quot;deb https://packages.cloud.google.com/apt coral-edgetpu-stable main&quot; | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list !sudo apt-get update !sudo apt-get install edgetpu-compiler . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 1210 100 1210 0 0 60500 0 --:--:-- --:--:-- --:--:-- 60500 OK deb https://packages.cloud.google.com/apt coral-edgetpu-stable main Get:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B] Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease Get:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB] Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB] Get:5 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB] Get:6 https://packages.cloud.google.com/apt coral-edgetpu-stable InRelease [6,332 B] Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64 InRelease Get:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB] Hit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64 InRelease Hit:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64 Release Get:11 https://packages.cloud.google.com/apt coral-edgetpu-stable/main amd64 Packages [2,317 B] Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2,909 kB] Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease Get:17 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [982 kB] Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,374 kB] Get:19 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1,879 kB] Get:20 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,284 kB] Get:21 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,436 kB] Get:22 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,003 kB] Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,125 kB] Fetched 15.4 MB in 2s (7,459 kB/s) Reading package lists... Done Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed: edgetpu-compiler 0 upgraded, 1 newly installed, 0 to remove and 30 not upgraded. Need to get 7,913 kB of archives. After this operation, 31.2 MB of additional disk space will be used. Get:1 https://packages.cloud.google.com/apt coral-edgetpu-stable/main amd64 edgetpu-compiler amd64 16.0 [7,913 kB] Fetched 7,913 kB in 1s (14.9 MB/s) debconf: unable to initialize frontend: Dialog debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, &lt;&gt; line 1.) debconf: falling back to frontend: Readline debconf: unable to initialize frontend: Readline debconf: (This frontend requires a controlling tty.) debconf: falling back to frontend: Teletype dpkg-preconfigure: unable to re-open stdin: Selecting previously unselected package edgetpu-compiler. (Reading database ... 129504 files and directories currently installed.) Preparing to unpack .../edgetpu-compiler_16.0_amd64.deb ... Unpacking edgetpu-compiler (16.0) ... Setting up edgetpu-compiler (16.0) ... Processing triggers for libc-bin (2.31-0ubuntu9.9) ... . #https://github.com/google-coral/edgetpu/issues/453 !edgetpu_compiler &quot;/tmp/MobileNetV2.tflite&quot; . Edge TPU Compiler version 16.0.384591198 Started a compilation timeout timer of 180 seconds. ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors. Compilation failed: Model failed in Tflite interpreter. Please ensure model can be loaded/run in Tflite interpreter. Compilation child process completed within timeout period. Compilation failed! . !ls -lh /tmp/MobileNet* . -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2.onnx -rw-r--r-- 1 root root 3.6M Jan 20 14:20 /tmp/MobileNetV2_OnnxQuant.onnx -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2_OnnxSim.onnx -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2.pt -rw-r--r-- 1 root root 14M Jan 20 14:22 /tmp/MobileNetV2.tflite -rw-r--r-- 1 root root 4.1M Jan 20 14:20 /tmp/MobileNetV2_TorchQATQuant.pt /tmp/MobileNetV2.tf: total 14M drwxr-xr-x 2 root root 4.0K Jan 20 14:21 assets -rw-r--r-- 1 root root 14M Jan 20 14:21 saved_model.pb drwxr-xr-x 2 root root 4.0K Jan 20 14:21 variables . PyTorch Quantization (PTQ - Dynamic/Weight only) . https://pytorch.org/blog/quantization-in-practice/ . import torch from torch import nn model = MobileNetV2() model.eval() ## EAGER MODE from torch.quantization import quantize_dynamic model_quantized = quantize_dynamic(model=model, qconfig_spec={nn.LSTM, nn.Linear}, dtype=torch.qint8, inplace=False) torch.save(model_quantized, torch_PTQ_Weight_Eager_path) ## FX MODE from torch.quantization import quantize_fx qconfig_dict = {&quot;&quot;: torch.quantization.default_dynamic_qconfig} example_inputs = iter(trainloader) img, lab = next(example_inputs) model_prepared = quantize_fx.prepare_fx(model, qconfig_dict, img) model_quantized = quantize_fx.convert_fx(model_prepared) torch.save(model_quantized, torch_PTQ_Weight_FX_path) . /usr/local/lib/python3.8/dist-packages/torch/ao/quantization/fx/prepare.py:1530: UserWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead. warnings.warn( . !ls -lh /tmp/MobileNet* . -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2.onnx -rw-r--r-- 1 root root 3.6M Jan 20 14:20 /tmp/MobileNetV2_OnnxQuant.onnx -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2_OnnxSim.onnx -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2.pt -rw-r--r-- 1 root root 14M Jan 20 14:22 /tmp/MobileNetV2.tflite -rw-r--r-- 1 root root 10M Jan 20 14:22 /tmp/MobileNet_V2_Torch_PTQ_Quant_W_EG.pt -rw-r--r-- 1 root root 9.8M Jan 20 14:22 /tmp/MobileNet_V2_Torch_PTQ_Quant_W_FX.pt -rw-r--r-- 1 root root 4.1M Jan 20 14:20 /tmp/MobileNetV2_TorchQATQuant.pt /tmp/MobileNetV2.tf: total 14M drwxr-xr-x 2 root root 4.0K Jan 20 14:21 assets -rw-r--r-- 1 root root 14M Jan 20 14:21 saved_model.pb drwxr-xr-x 2 root root 4.0K Jan 20 14:21 variables . PyTorch Quantization (PTQ - Static) . https://pytorch.org/blog/quantization-in-practice/ . import torch from torch import nn import copy model = MobileNetV2() ## EAGER MODE m = copy.deepcopy(model) m.eval() # torch.quantization.fuse_modules(m, [&#39;0&#39;,&#39;1&#39;], inplace=True) # torch.quantization.fuse_modules(m, [&#39;2&#39;,&#39;3&#39;], inplace=True) m = nn.Sequential(torch.quantization.QuantStub(), m, torch.quantization.DeQuantStub()) m.qconfig = torch.quantization.get_default_qconfig(&quot;fbgemm&quot;) torch.quantization.prepare(m, inplace=True) example_inputs = iter(trainloader) img, lab = next(example_inputs) with torch.inference_mode(): for _ in range(10): m(img) model_quantized = torch.quantization.convert(m, inplace=True) torch.save(model_quantized, torch_PTQ_Static_Eager_path) ## FX MODE from torch.quantization import quantize_fx m = copy.deepcopy(model) m.eval() qconfig_dict = {&quot;&quot;: torch.quantization.get_default_qconfig(&quot;fbgemm&quot;)} model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, img) with torch.inference_mode(): for _ in range(10): model_prepared(img) model_quantized = quantize_fx.convert_fx(model_prepared) torch.save(model_quantized, torch_PTQ_Static_FX_path) . /usr/local/lib/python3.8/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers. reduce_range will be deprecated in a future release of PyTorch. warnings.warn( . !ls -lh /tmp/MobileNet* . -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2.onnx -rw-r--r-- 1 root root 3.6M Jan 20 14:20 /tmp/MobileNetV2_OnnxQuant.onnx -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2_OnnxSim.onnx -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2.pt -rw-r--r-- 1 root root 14M Jan 20 14:22 /tmp/MobileNetV2.tflite -rw-r--r-- 1 root root 4.1M Jan 20 14:22 /tmp/MobileNet_V2_Torch_PTQ_Quant_S_EG.pt -rw-r--r-- 1 root root 3.8M Jan 20 14:22 /tmp/MobileNet_V2_Torch_PTQ_Quant_S_FX.pt -rw-r--r-- 1 root root 10M Jan 20 14:22 /tmp/MobileNet_V2_Torch_PTQ_Quant_W_EG.pt -rw-r--r-- 1 root root 9.8M Jan 20 14:22 /tmp/MobileNet_V2_Torch_PTQ_Quant_W_FX.pt -rw-r--r-- 1 root root 4.1M Jan 20 14:20 /tmp/MobileNetV2_TorchQATQuant.pt /tmp/MobileNetV2.tf: total 14M drwxr-xr-x 2 root root 4.0K Jan 20 14:21 assets -rw-r--r-- 1 root root 14M Jan 20 14:21 saved_model.pb drwxr-xr-x 2 root root 4.0K Jan 20 14:21 variables . https://pytorch.org/docs/stable/generated/torch.quantization.quantize_fx.prepare_fx.html . Test - QAT IRIS . import torch.nn.functional as F from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from torch.autograd import Variable from torch.quantization import QuantStub, DeQuantStub x, y = load_iris(return_X_y=True) train_X, test_X, train_y, test_y = train_test_split(x, y, test_size=0.8) train_X = Variable(torch.Tensor(train_X).float()) test_X = Variable(torch.Tensor(test_X).float()) train_y = Variable(torch.Tensor(train_y).long()) test_y = Variable(torch.Tensor(test_y).long()) class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(4, 100) self.fc2 = nn.Linear(100, 100) self.fc3 = nn.Linear(100, 3) self.softmax = nn.Softmax(dim=1) self.quant = QuantStub() self.dequant = DeQuantStub() def forward(self, X): X = self.quant(X) X = F.relu(self.fc1(X)) X = self.fc2(X) X = self.fc3(X) X = self.softmax(X) X = self.dequant(X) return X m = Net() m.train() backend = &quot;fbgemm&quot; m.qconfig = torch.quantization.get_default_qconfig(backend) torch.quantization.prepare_qat(m, inplace=True) n_epochs = 10 opt = torch.optim.SGD(m.parameters(), lr=0.1) loss_fn = torch.nn.CrossEntropyLoss() for epoch in range(n_epochs): opt.zero_grad() out = m(train_X) loss = loss_fn(out, train_y) loss.backward() opt.step() m.eval() model_quantized = torch.quantization.convert(m, inplace=True) torch.save(model_quantized, &#39;/tmp/Test_QAT_iris.pt&#39;) . /usr/local/lib/python3.8/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers. reduce_range will be deprecated in a future release of PyTorch. warnings.warn( . Test - Blob Converter . class CatImgs(nn.Module): def forward(self, img1, img2, img3): return torch.cat((img1, img2, img3), 3) X = torch.ones((1, 3, 300, 300), dtype=torch.float32) torch.onnx.export( CatImgs(), (X, X, X), &quot;/tmp/Test_Blob_Onnx.onnx&quot;, opset_version=12, do_constant_folding=True, ) import onnx from onnxsim import simplify onnx_model = onnx.load(&quot;/tmp/Test_Blob_Onnx.onnx&quot;) model_simpified, check = simplify(onnx_model) onnx.save(model_simpified, &quot;/tmp/Test_Blob_OnnxSim.onnx&quot;) import blobconverter blobconverter.from_onnx( model=&quot;/tmp/Test_Blob_OnnxSim.onnx&quot;, output_dir=&quot;/tmp/Test_Blob.blob&quot;, data_type=&quot;FP16&quot;, shaves=6, use_cache=False, optimizer_params=[] ) . Downloading /tmp/Test_Blob.blob/Test_Blob_OnnxSim_openvino_2021.4_6shave.blob... [==================================================] Done . PosixPath(&#39;/tmp/Test_Blob.blob/Test_Blob_OnnxSim_openvino_2021.4_6shave.blob&#39;) . !ls -lh /tmp/MobileNet* &amp; ls -lh /tmp/Test* . -rw-r--r-- 1 root root 283 Jan 20 14:22 /tmp/Test_Blob_Onnx.onnx -rw-r--r-- 1 root root 285 Jan 20 14:22 /tmp/Test_Blob_OnnxSim.onnx -rw-r--r-- 1 root root 21K Jan 20 14:22 /tmp/Test_QAT_iris.pt /tmp/Test_Blob.blob: -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2.onnx -rw-r--r-- 1 root root 3.6M Jan 20 14:20 /tmp/MobileNetV2_OnnxQuant.onnx -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2_OnnxSim.onnx -rw-r--r-- 1 root root 14M Jan 20 14:20 /tmp/MobileNetV2.pt -rw-r--r-- 1 root root 14M Jan 20 14:22 /tmp/MobileNetV2.tflite -rw-r--r-- 1 root root 4.1M Jan 20 14:22 /tmp/MobileNet_V2_Torch_PTQ_Quant_S_EG.pt -rw-r--r-- 1 root root 3.8M Jan 20 14:22 /tmp/MobileNet_V2_Torch_PTQ_Quant_S_FX.pt -rw-r--r-- 1 root root 10M Jan 20 14:22 /tmp/MobileNet_V2_Torch_PTQ_Quant_W_EG.pt -rw-r--r-- 1 root root 9.8M Jan 20 14:22 /tmp/MobileNet_V2_Torch_PTQ_Quant_W_FX.pt -rw-r--r-- 1 root root 4.1M Jan 20 14:20 /tmp/MobileNetV2_TorchQATQuant.pt /tmp/MobileNetV2.tf: total 4.0K -rw-r--r-- 1 root root 1.0K Jan 20 14:22 Test_Blob_OnnxSim_openvino_2021.4_6shave.blob total 14M drwxr-xr-x 2 root root 4.0K Jan 20 14:21 assets -rw-r--r-- 1 root root 14M Jan 20 14:21 saved_model.pb drwxr-xr-x 2 root root 4.0K Jan 20 14:21 variables . TODO . Fix Blob converter for MobileNet | Fix Compile TFLite to EdgeTPU TFLite |",
            "url": "https://deebuls.github.io/devblog/pytorch/dnn/2023/01/17/Quantize-DNN-Model-Pytorch.html",
            "relUrl": "/pytorch/dnn/2023/01/17/Quantize-DNN-Model-Pytorch.html",
            "date": " • Jan 17, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "Exponential Power Distribution: PDF and NLL Visualization ",
            "content": ". Exponential Power Distribution . Generalized Gaussian Distirbution . Plotting the PDF and Negative log likelihood of the distribution. . Analysis on the effect of the parameters on the pdf and the nll. . import sympy.plotting as symplot import sympy as sym sym.init_printing(use_unicode=True) import numpy as np import pandas as pd import seaborn as sns import matplotlib.colors as mcolors import matplotlib.pyplot as plt color = list(mcolors.TABLEAU_COLORS) plt.rcParams[&#39;figure.figsize&#39;] = 25, 10 #Plot​ Size plt.rcParams[&#39;legend.fontsize&#39;]=10 #Legend​ Size from typing import List,Tuple . import pandas as pd import seaborn as sns import numpy as np . Sympy Plotting Function . def plot_exponential_power_distribution(alphas: List, betas: List, sympy_function, plot_ylim: Tuple = (0,6), plot_xlim: Tuple = (-2,2)): xlim = 1e-21 p1 = symplot.plot(sympy_function.subs([(beta, betas[0]), (alpha, alphas[0])]), (diff,-xlim,xlim), show=False, line_color=&#39;darkgreen&#39;) p1[0].label = &#39;&#39; i=0 for a in alphas: for b in betas: p = symplot.plot(sympy_function.subs([(beta,b), (alpha, a)]), (diff,-3,3), show=False, line_color=color[i%len(color)]) p[0].label = &#39;beta %s, alpha %s, zero %s&#39;% (str(b),str(a),str(sympy_function.subs([(beta,b), (diff,0),(alpha, a)]))) p1.append(p[0]) i = i+1 p1.legend = True p1.ylim = plot_ylim p1.xlim = plot_xlim p1.show() . Probability Density function . $$ frac{ beta}{2 alpha Gamma(1/ beta)} e^{- (|x - mu | / alpha)^{ beta}} $$ . Replacing $x - mu$ with $diff$ a single variable. . $$ frac{ beta}{2 alpha Gamma(1/ beta)} e^{- (|diff | / alpha)^{ beta}} $$ . alpha, beta, diff = sym.symbols(&#39;alpha, beta, diff&#39;) . pdf = ( beta / (2 * alpha * sym.gamma(1 / beta)) ) * sym.exp(-((sym.Abs(diff)/alpha)**beta)) pdf . $ displaystyle frac{ beta e^{- left( frac{ left|{diff} right|}{ alpha} right)^{ beta}}}{2 alpha Gamma left( frac{1}{ beta} right)}$ Fixed alpha changing beta . Observtion . The Maximum is at $beta = 2.0$ (Gaussian) at any fixed alpha | $beta = 0.3$ the max pdf is 0.5 which is less than 1 -&gt; can be considered for the minimum beta value | alphas = [0.1] betas = [ 0.3, 0.4, 0.5, 1.0, 1.5, 2.0, 8.0] plot_exponential_power_distribution(alphas, betas, pdf) . Fixed beta changing alpha . Observtion . Alpha is the variance value -&gt; the lower the better the more confident. | | alphas = [ 0.01, 0.1, 0.5, 1.0, 2.0] betas = [ 2.0] #Gussian Distirbution plot_exponential_power_distribution(alphas, betas, pdf) . alphas = [ 0.01, 0.1, 0.5, 1.0, 2.0] betas = [ 1.0] #Laplace distirbution plot_exponential_power_distribution(alphas, betas, pdf) . Negative log likelihood . Here we take the logarithmic of the pdf and the invert it . . The NLL are usually used as loss function in regresion problems. . nll_pdf = -1 * sym.log(pdf) nll_pdf . $ displaystyle - log{ left( frac{ beta e^{- left( frac{ left|{diff} right|}{ alpha} right)^{ beta}}}{2 alpha Gamma left( frac{1}{ beta} right)} right)}$ nll_pdf.subs([(beta,0.5), (diff,0),(alpha, 1.0)]) . $ displaystyle 1.38629436111989$ alphas = [1.5, 2.0] betas = [ 0.1, 0.5, 1.0, 2.0, 8.0] plot_exponential_power_distribution(alphas, betas, nll_pdf, plot_ylim=(0,3)) . Entropy . $$ frac{1}{ beta} - log [ frac{ beta}{2 alpha Gamma(1/ beta)}] $$ . Observations . With reducing value of alpha entropy reduces (since alpha is the variance this fits perfectly) | With increasing beta the entropy reduces This is little counter intutive | Expected that entropy will be low only for beta = 2 (Gaussian) since it had max value in pdf. | But apparently when beta increases the confidence increases that the value is between some range and not a single value . | . | #Modelling in sympy entropy = (1 / beta) - sym.log(beta / (2 * alpha * sym.gamma(1/beta)) ) entropy . $ displaystyle - log{ left( frac{ beta}{2 alpha Gamma left( frac{1}{ beta} right)} right)} + frac{1}{ beta}$ betas = [ 0.1, 0.5, 1.0, 2.0, 8.0] alphas = [ 0.01, 0.1, 0.5, 1.0, 2.0] val = [] for a in alphas: for b in betas: #print (&#39;Entropy : {}, beta: {}, alpha: {}&#39;.format(entropy.subs([(alpha, a), (beta,b)]), b, a)) val.append([float(entropy.subs([(alpha, a), (beta,b)])), b, a]) entropy_data_frame = pd.DataFrame(val, columns=[&#39;entropy&#39;,&#39;beta&#39;, &#39;alpha&#39;]) print (entropy_data_frame.dtypes) entropy_data_frame = entropy_data_frame.pivot(&#39;alpha&#39;, &#39;beta&#39;, &#39;entropy&#39;) print (entropy_data_frame.dtypes) entropy_data_frame . entropy float64 beta float64 alpha float64 dtype: object beta 0.1 float64 0.5 float64 1.0 float64 2.0 float64 8.0 float64 dtype: object . beta 0.1 0.5 1.0 2.0 8.0 . alpha . 0.01 21.192390 | -1.218876 | -2.912023 | -3.532805 | -3.847046 | . 0.10 23.494975 | 1.083709 | -0.609438 | -1.230220 | -1.544461 | . 0.50 25.104413 | 2.693147 | 1.000000 | 0.379218 | 0.064977 | . 1.00 25.797560 | 3.386294 | 1.693147 | 1.072365 | 0.758124 | . 2.00 26.490707 | 4.079442 | 2.386294 | 1.765512 | 1.451271 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; sns.heatmap(entropy_data_frame, annot=True, fmt=&quot;.1f&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f34544d1b80&gt; . Interval . ToDO . Can we calculucalte interval from variance | Can we calculate interval from the quantile | interval is ppf function which is inverse cdf function | We have the CDF function . How to calculate the inverse CDF ? | import scipy.stats as stats . a = 0.1 b = 2.0 p = 0.9 stats.gamma() . &lt;scipy.stats._continuous_distns.gamma_gen at 0x7fbaf9fd26a0&gt; . cdf = 0.5 + (sym.sign(diff)/2 )*(1/sym.gamma(1/beta)) .",
            "url": "https://deebuls.github.io/devblog/visualization/sympy/distribution/statistics/2023/01/17/Exponential-Power-Distribution.html",
            "relUrl": "/visualization/sympy/distribution/statistics/2023/01/17/Exponential-Power-Distribution.html",
            "date": " • Jan 17, 2023"
        }
        
    
  
    
        ,"post2": {
            "title": "Migrating Kelo Tulip to ROS2",
            "content": "Migration of ROS2 . As per the blog the migration steps are (ROS2 migration guide)[https://docs.ros.org/en/foxy/The-ROS2-Project/Contributing/Migration-Guide.html] Package manifests | Metapackages | Message, service, and action definitions | Build system | Update source code | . | Step 1 : Package manifests . | Migration Gazebo https://github.com/ros-simulation/gazebo_ros_pkgs/wiki | . | Messages For messages and services replace the message using the formulation below | #include &lt;sensor_msgs/JointState.hpp&gt; ➡️ #include &lt;sensor_msgs/msg/joint_state.hpp&gt; | #include &lt;nav_msgs/Odometry.hpp&gt; ➡️ #include &lt;nav_msgs/msg/odometry.hpp&gt; | You add the msg and in small letters | . | Main.cpp #include &quot;ros/ros.h&quot; ▶️ #include &quot;rclcpp/rclcpp.hpp&quot; | . | Launching robot in Gazebo : Error model visalization For ROS2 usage, by changing the following in any of the cameras’ urdf file | into . will resolve the issue, as it will evaluate to the full path when xacro generates the URDF. . 8. Adding laser scanner in gazebo . Errors * Used the xacro method from ros1 * xacro method works but plugins have changed * libgazebo_ros_laser.so is now changed to ➡️ libgazebo_ros_ray_sensor.so * Also the paraeter to initialize it is different * Check the github [issue](https://github.com/ros-simulation/gazebo_ros_pkgs/issues/1266#issuecomment-826249858) for proper initialization * 9. Ros2 node with both cpp and python then do the following * I have a mixed C++/Python package. To install a Python executable in this package, I&#39;ve found it sufficient to do this: * Add the Python source file to my_package/scripts * Add this line at the top of ^that file: #!/usr/bin/env python3 * Make it executable with chmod +x * Add an empty __init__.py file in my_package/scripts * Add this to CMakeLists: . install(PROGRAMS scripts/my_python_file.py DESTINATION lib/${PROJECT_NAME} ) . * from https://answers.ros.org/question/299269/ros2-run-python-executable-not-found/ . Errors . Starting gazebo in the launch file gives “camera assertion error “ The solution is to source Gazebo’s setup file, i.e.: . /usr/share/gazebo/setup.sh | https://answers.gazebosim.org//question/28066/is-libgazebo_ros_multicameraso-deprecated/ | echo &quot;source /usr/share/gazebo/setup.bash&quot; &gt;&gt; ~/.bashrc ▶️ for persistence | . |",
            "url": "https://deebuls.github.io/devblog/robotics/ros2/2023/01/13/Migration-ROS-Kelo-Tulip-ROS2.html",
            "relUrl": "/robotics/ros2/2023/01/13/Migration-ROS-Kelo-Tulip-ROS2.html",
            "date": " • Jan 13, 2023"
        }
        
    
  
    
        ,"post3": {
            "title": "Dis-entaglement of Epistemic and Aleatoric uncertainty for Dirichlet Distribution",
            "content": ". How to separate epistemic and aleatoric uncertaity of Dirichlet distirbution . Also studing the implications of it and proposing the applications of the solutions. . Formula for [1] . | theory in [2] | . ToDo : complete the section with info . [1] Separation of Aleatoric and Epistemic Uncertainty in Deterministic Deep Neural Networks Denis Huseljic, Bernhard Sick, Marek Herde, Daniel Kottke . [2] Deep Deterministic Uncertainty: A Simple Baseline Jishnu Mukhoti . import math import numpy as np import torch . prior = 1 n_classes = 5 def predict_epistemic( alpha): &quot;&quot;&quot;Predicts the uncertainty of a sample. (K / alpha_0)&quot;&quot;&quot; return n_classes * prior / alpha.sum(-1, keepdim=True) def predict_aleatoric( alpha): &quot;&quot;&quot;Predicts the uncertainty of a sample. (K / alpha_0)&quot;&quot;&quot; proba_in = (alpha / alpha.sum(-1, keepdim=True)).clamp_(1e-8, 1-1e-8) entropy = - torch.sum((proba_in * proba_in.log()), dim=-1) normalized_entropy = entropy / np.log(n_classes) return normalized_entropy . ones = torch.ones(n_classes) print (predict_epistemic(ones), predict_aleatoric(ones)) . tensor([1.]) tensor(1.) . When alpha of only a single class keeps increasing . Observation : Both uncertainty reduces | Impact : When the model puts all confidence(alpha) on a single class it shows that the model is confident about the class and uncertainty reduces. . | The maximum aleatoric and epistemic uncertitny is both 1.0 . | Epistemic is always lower than Aleatoric | . for i in [1, 10, 50, 1000 ]: x = torch.ones(n_classes) x[0] = i print (x) print (&quot;Epistemic UE : {}, Aleatoric UE : {}&quot;.format(predict_epistemic(x), predict_aleatoric(x))) print (&quot;&quot;,predict_epistemic(x) &gt; predict_aleatoric(x)) . tensor([1., 1., 1., 1., 1.]) Epistemic UE : tensor([1.]), Aleatoric UE : 1.0 tensor([False]) tensor([10., 1., 1., 1., 1.]) Epistemic UE : tensor([0.3571]), Aleatoric UE : 0.6178266406059265 tensor([False]) tensor([50., 1., 1., 1., 1.]) Epistemic UE : tensor([0.0926]), Aleatoric UE : 0.2278686910867691 tensor([False]) tensor([1000., 1., 1., 1., 1.]) Epistemic UE : tensor([0.0050]), Aleatoric UE : 0.019580082967877388 tensor([False]) . When alpha of multiple classes keeps increasing . Observation : Epistemic reduces aleatoric is high | Impact : When the model puts all confidence(alpha) on multiple classes basically suggests that the model is not confident. While since some alpha has increased it suggests that the input is an observed data(not new) and therefore low aleatoric uncertainty | . The maximum aleatoric and epistemic uncertainty is both 1 . for i in [1, 10, 50, 10000 ]: x = torch.ones(n_classes)*i print (x) print (&quot;Epistemic UE : {}, Aleatoric UE : {}&quot;.format(predict_epistemic(x), predict_aleatoric(x))) print (&quot;&quot;,) . tensor([1., 1., 1., 1., 1.]) Epistemic UE : tensor([1.]), Aleatoric UE : 1.0 tensor([10., 10., 10., 10., 10.]) Epistemic UE : tensor([0.1000]), Aleatoric UE : 1.0 tensor([50., 50., 50., 50., 50.]) Epistemic UE : tensor([0.0200]), Aleatoric UE : 1.0 tensor([10000., 10000., 10000., 10000., 10000.]) Epistemic UE : tensor([1.0000e-04]), Aleatoric UE : 1.0 . Impact of prior . prior = 50 . The highest epistmeic uncertainty increases from 1 to the prior value . Conclusions . Dirichlet distirbution can be dis-entagled into aleatoric and epistemic uncertainty. | When all alpha is 1 - both uncertainty are also 1 impling that the network doesnt know anything | If only one output class alpha is higher then both uncertainty is low | The higher the alpha the lower both the uncertainty | If multiple alpha is higher then only aleatoric is high epistemic stays low. Impling that since the some alpha was increased the network has seen the input and its not sure which amongst the outputs is correct. | . Use Case . 1. For identifying OOD data . For the training dataset measure the epistemic uncertainty of the correct predictions. It should be less than 1 and near to zero | During prediction if epistemic uncertainty is higher than the training max then that data should be considered OOD and handled appropriately | 2. For handling in-domain uncertain data . If the epistemic unertainty is is range but if the aleatoric is high we can use these in embodied situation to collect additional data(image) from different view, fuse and make decision. Example if blur image - then differ to predict but dont flag as OOD, maybe in next image the information will be clear. |",
            "url": "https://deebuls.github.io/devblog/statistics/uncertainty/2022/12/19/dirichlet-disentanglement-epistemic-aleatoric-evidential.html",
            "relUrl": "/statistics/uncertainty/2022/12/19/dirichlet-disentanglement-epistemic-aleatoric-evidential.html",
            "date": " • Dec 19, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Comparing Distributions : Normal, Laplace and Cauchy",
            "content": ". Comparing Distributions : Normal, Laplace and Cauchy . We want to represent a fact of a real number $x$ and its uncertainty. Probability distirbution is one of the representation to hold such information. . For example, we want to store a fact that the value of a variable is 10 with uncertaitny of $+- 0.5$ . In other format the value lies between $[9.5, 10.5]$ . This can be represented using different probability distributions. . Normal distirbution - N(10, 0.3) | Laplace distirbution - L(10, 0.2) | Cauchy distribution - C(10, 0.1) | The question we are interested is if a particular fact is represented using these distribution. How to compare the uncertainty ? . For example: the above distirbutions how all of them have peak at 10 but how to compare are these distirbutions having the same spread. . Such situations come in machine learning or deep learning situation where you have the output of the AI model predicting a distribution (laplace or cauchy) and you have the true value and you want to compare which distribution is giving a better result. . Lets visually check how the distirbution looks. . ## hide import numpy as np import scipy.stats as stats import matplotlib.pyplot as plt . normal_dist = stats.norm(loc=10, scale=0.3) laplace_dist = stats.laplace(loc=10, scale=0.2 ) cauchy_dist = stats.laplace(loc=10, scale=0.1) x = np.linspace(8, 12, num=1000) fig, ax = plt.subplots( 1, 1, figsize=(10,8)) ax.plot(x, normal_dist.pdf(x), label=&#39;Normal Distirbution (10, 0.3)&#39;, c=&#39;r&#39;) ax.plot(x, laplace_dist.pdf(x), label=&#39;laplace Distirbution (10, 0.2)&#39;, c=&#39;g&#39;) ax.plot(x, cauchy_dist.pdf(x), label=&#39;cacuhy Distirbution (10, 0.1)&#39;, c=&#39;b&#39;) ax.legend() . &lt;matplotlib.legend.Legend at 0x7f3ea0bd10a0&gt; . Questions . How to compare these 3 distirbution? | More importantly if a particular distirbution is representing an uncertain information, then how can we measure which value is giving the best representation of the uncertainty. | Solution . For comparing uncertainty different methods has been mentioned in literature. The one which we are going to use here is Interval Score, which comes under the field of Proper Scoring Rules [1]. . Proper Scoring rules . Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand [1] . [1] Gneiting, Tilmann, and Adrian E. Raftery. &quot;Strictly proper scoring rules, prediction, and estimation.&quot; Journal of the American statistical Association 102.477 (2007):359-378. . Scoring Rules for Quantile and Interval Forecasts . Interval Score . classical case of $(1 - alpha) times 100 % $ prediction interval | with lower and upper endpoints (predictive quatiles) at level $ alpha/2$ and $ 1 - alpha/2$ | Interval score $$ S_ alpha^{int}(l, u ;x) = (u-l) + frac{2}{ alpha}(l-x) mathbb{1} {x &lt; l } + frac{2}{ alpha} (x - u) mathbb{1} {x &gt; u }$$ | . $ alpha_1= 0.02, alpha_2= 0.05, alpha_3=0.1$ (implying nominal coverages of 98%,95%,90%) . def interval_score(x, lower, upper, alpha=0.05): assert np.all(upper&gt;=lower), &quot;Upper should be greater or equal to lower. Please check are you giving the upper and lower in propoer order &quot; return (upper - lower) + (2/alpha)*(lower-x)*(x&lt;lower) + (2/alpha)*(x-upper)*(x&gt;upper) . x = np.linspace(3.0, 10.0) l = 5.0 u = 8.0 iscore = interval_score(x, l, u) plt.plot(x, iscore, c=&#39;b&#39;) l = 6.0 u = 7.0 iscore = interval_score(x, l, u) plt.plot(x, iscore, c=&#39;r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3ea0b6dbe0&gt;] . The way to interpret the method with respect to state-estimation algorithm can be. Let $x$ be the true_value and the prediction is the range $[5,8]$ then the intervalscore is minimum if the true value was also inbetween $[5,8]$ as shown in the blue plot. . With the red plot you can see if you give a tigher bound then the score is further low. . So if the true value is inside the lower and uper limits the score is minimum also with tighter bound you get better scores . Using Interval Score for Distirbutions . We need to define the intervals . . So how do we calculate the intervals of different distirbutions ?? . sympy to help, it has the . .interval(alpha) function . Lets calcuate the interval function of the above distirbutions . print (&quot; Normal Distirbution 95% interval &quot;, normal_dist.interval(alpha=0.95)) print (&quot; Laplace Distirbution 95% interval&quot;, laplace_dist.interval(alpha=0.95)) print (&quot; Cauchy Distirbution 95% interval&quot;, cauchy_dist.interval(alpha=0.95)) . Normal Distirbution 95% interval (9.412010804637983, 10.587989195362017) Laplace Distirbution 95% interval (9.400853545289202, 10.599146454710798) Cauchy Distirbution 95% interval (9.700426772644601, 10.299573227355399) . Formula . So now we have the lower and upper limit for each distirbution . Lets now calculate the Interval Score. . l, u = normal_dist.interval(alpha=0.95) print (&quot;Normal Distirbution :&quot;) print (&quot;95% interval &quot;, normal_dist.interval(alpha=0.95)) print (&quot;Interval Score &quot;, interval_score(10 , l, u)) print (&quot;############### &quot;) print (&quot;Laplace Distirbution&quot;) l, u = laplace_dist.interval(alpha=0.95) print (&quot;95% interval &quot;, laplace_dist.interval(alpha=0.95)) print (&quot;Interval Score &quot;, interval_score(10 , l, u)) print (&quot;############### &quot;) print (&quot;Cauchy Distirbution 95%&quot;) l, u = cauchy_dist.interval(alpha=0.95) print (&quot;95% interval &quot;, cauchy_dist.interval(alpha=0.95)) print (&quot;Interval Score &quot;,interval_score(10 , l, u)) . Normal Distirbution : 95% interval (9.412010804637983, 10.587989195362017) Interval Score 1.1759783907240333 ############### Laplace Distirbution 95% interval (9.400853545289202, 10.599146454710798) Interval Score 1.1982929094215962 ############### Cauchy Distirbution 95% 95% interval (9.700426772644601, 10.299573227355399) Interval Score 0.5991464547107981 . Here you can see for True value $x = 10$, Cauchy Distirbution (loc=10, scale=0.1) gives the minimum Interval Score of 0.6 . Thus, with Interval Score a Proper scoring rule we can compare different distirbutions. .",
            "url": "https://deebuls.github.io/devblog/statistics/uncertainty/2022/12/15/comparing-Normal-Laplace-Cacuhy-Distirbutions-Interval-score.html",
            "relUrl": "/statistics/uncertainty/2022/12/15/comparing-Normal-Laplace-Cacuhy-Distirbutions-Interval-score.html",
            "date": " • Dec 15, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Running ros2 humble in ubuntu lxd container",
            "content": "ROS2 in Lxd container . Its always dificult to get the appropriate ubuntu version for a running a particular software. For example, currently I have ubuntu 20.04 and I want to test a software in ros2 humble. Humble requires ubuntu version 22.04. I wanted to try if we can use lxd containers for running the appropriate ubuntu version and installing ros in those. . LXD start easy steps . Install lxd in your ubuntu (Using snap) | Use Lxc to make sure its without sudo | For using gui inside the container we eed to attach profiles to the container. | Create a profile using the following command $ lxc profile create x11 Profile x11 created . | Edit the profile with the following command $ lxc profile edit x11 . | Copy paste below in the editor below config: environment.DISPLAY: :0 environment.PULSE_SERVER: unix:/home/ubuntu/pulse-native nvidia.driver.capabilities: all nvidia.runtime: “true” user.user-data: | #cloud-config runcmd: ‘sed -i “s/; enable-shm = yes/enable-shm = no/g” /etc/pulse/client.conf’ packages: | x11-apps | mesa-utils | pulseaudio description: GUI LXD profile devices: PASocket1: bind: container connect: unix:/run/user/1000/pulse/native listen: unix:/home/ubuntu/pulse-native security.gid: “1000” security.uid: “1000” uid: “1000” gid: “1000” mode: “0777” type: proxy X0: bind: container connect: unix:@/tmp/.X11-unix/X1 listen: unix:@/tmp/.X11-unix/X0 security.gid: “1000” security.uid: “1000” type: proxy mygpu: type: gpu name: x11 used_by: [] | . | In the above file check the line connect: unix:@/tmp/.X11-unix/X1 This depends on how your local machine DISPLAY is set. If your local machine DISPLAY is at X1 replace there with X0. | To check your local machine displayecho $DISPLAY. | Now lets create a container with the profile lxc launch ubuntu:22.04 --profile default --profile x11 mycontainer . | To get a shell in the container, run the following. $ lxc exec mycontainer -- sudo --user ubuntu --login mycontainer $ sudo apt install xclock mycontainer $ export DISPLAY=:0 #Add this in bashrc mycontainer $ xclock . | The above command will open the xclock in GUI . If the GUI doesnt come then check your local DISPLAY and the DISPLAY inside the container. | Complete blog with explanation on the process is provided by (Simos)[https://blog.simos.info/running-x11-software-in-lxd-containers/] | LXD and ROS2 installation . Started with blog by ubuntu | It gets you through the installation . I created a ubuntu 22.04 container and installed ros2 inside it . | I create a user called ubuntu | for ros installation I followed ros2 documentation | . | For using ros started ros2 tutorials | First problem gui not working Blog by Nick D Greg introduced the topic of using profiles | Following the blog I created a lxc profile named gui | The blog assumes you start from begining but since we already had the container running we used the below command | lxc profile assign ros-humble default,gui . | inside the container ‘export DISPLAY=:0’ in the bash (also added in bashrc) | now gui is running | $&gt; ros2 run turtlesim turtlesim_node | . | Comman Commands to get started with lxc development . Boot your local ubuntu and first you want to check the active containers the command is lxc list $&gt; lxc list +++-+--+--+--+ | NAME | STATE | IPV4 | IPV6 | TYPE | SNAPSHOTS | +++-+--+--+--+ | ros-humble | RUNNING | 10.171.226.72 (eth0) | fd42:13f7:78ed:7795:216:3eff:fe9c:bde9 (eth0) | CONTAINER | 0 | +++-+--+--+--+ . | Getting bash access $ lxc exec ros-humble -- su --login ubuntu ubuntu@ubuntu-container:~$ . | Appendix . History of comands used insde the container, after logging . 2 apt-cache policy | grep universe 3 sudo apt install software-properties-common 4 sudo add-apt-repository universe 5 sudo apt update &amp;&amp; sudo apt install curl gnupg lsb-release 6 sudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg 7 echo &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(source /etc/os-release &amp;&amp; echo $UBUNTU_CODENAME) main&quot; | sudo tee /etc/apt/sources.list.d/ros2.list &gt; /dev/null 8 sudo apt update &amp;&amp; sudo apt install -y build-essential cmake git python3-colcon-common-extensions python3-flake8 python3-flake8-blind-except python3-flake8-builtins python3-flake8-class-newline python3-flake8-comprehensions python3-flake8-deprecated python3-flake8-docstrings python3-flake8-import-order python3-flake8-quotes python3-pip python3-pytest python3-pytest-cov python3-pytest-repeat python3-pytest-rerunfailures python3-rosdep python3-setuptools python3-vcstool wget 9 sudo apt update 10 sudo apt install ros-humble-desktop .",
            "url": "https://deebuls.github.io/devblog/robotics/2022/10/20/ROS2-LXD-Container.html",
            "relUrl": "/robotics/2022/10/20/ROS2-LXD-Container.html",
            "date": " • Oct 20, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Compile Latex file using Github Action",
            "content": "Compiling Latex documents using Github Action . GitHub Actions provides full access to the runner at your disposal, and one thing you may want to do is make commits in a workflow run and push it back up to GitHub automatically. I’m going to show a simple example where we run the date unix command, save the contents to a file, and push it back to the master branch. Example workflow . Using Makefile . Use make file to automate all the actions which needs to be done after checking out the repo. | Here is a Makefile for compiling latex file Makefile filename=main | . pdf: mkdir -p build pdflatex –output-directory build ${filename} bibtex build/${filename}||true pdflatex –output-directory build ${filename} pdflatex –output-directory build ${filename} mv build/${filename}.pdf . . read: evince build/${filename}.pdf &amp; . clean: rm -f build/${filename}.{ps,pdf,log,aux,out,dvi,bbl,blg} . ## Installing latex commands in Github Action Inorder to use **pdflatex** in the ubuntu machine generted for compilation first you need to install the necessary packages in the ubuntu machine . ## Using git commit in GitHub Actions The following is a workflow which on push will do the following: checkout the repo install latex dependency run make command and compile setup git config commit the changed file and push it back to master yml name: Makefile CI on: push: branches: [ &quot;master&quot; ] pull_request: branches: [ &quot;master&quot; ] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Install dependencies run: sudo apt-get install texlive-latex-base texlive-fonts-recommended texlive-fonts-extra texlive-latex-extra - name: Compile run: make - name: setup git config run: | # setup the username and email. I tend to use &#39;GitHub Actions Bot&#39; with no email by default git config user.name &quot;GitHub Actions Bot&quot; git config user.email &quot;&lt;&gt;&quot; - name: commit run: | # Stage the file, commit and push git add main.pdf git commit -m &quot;new main pdf commit&quot; git push origin master . [1] https://lannonbr.com/blog/2019-12-09-git-commit-in-actions/ [2] Example github page .",
            "url": "https://deebuls.github.io/devblog/latex/2022/06/25/Github-Action-Latex-Compilation.html",
            "relUrl": "/latex/2022/06/25/Github-Action-Latex-Compilation.html",
            "date": " • Jun 25, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Proper Scoring Rules: Interval Score and CRPS",
            "content": ". Note we also compare Gaussian distribution and Laplace distirbution to empiracally find the $95%$ region with respect to their scale. . We found out 95% Confidence interval is $Gaussian ( 2 sigma) == Laplace(3b)$ . Scoring Rules for Quantile and Interval Forecasts . Interval Score . classical case of $(1 - alpha) times 100 % $ prediction interval | with lower and upper endpoints (predictive quatiles) at level $ alpha/2$ and $ 1 - alpha/2$ | Interval score $$ S_ alpha^{int}(l, u ;x) = (u-l) + frac{2}{ alpha}(l-x) mathbb{1} {x &lt; l } + frac{2}{ alpha} (x - u) mathbb{1} {x &gt; u }$$ | . $ alpha_1= 0.02, alpha_2= 0.05, alpha_3=0.1$ (implying nominal coverages of 98%,95%,90%) . import numpy as np . def interval_score(x, lower, upper, alpha=0.05): assert np.all(upper&gt;=lower), &quot;Upper should be greater or equal to lower. Please check are you giving the upper and lower in propoer order &quot; return (upper - lower) + (2/alpha)*(lower-x)*(x&lt;lower) + (2/alpha)*(x-upper)*(x&gt;upper) . x = np.linspace(1.0, 12.0) l = 5.0 u = 8.0 iscore = interval_score(x, l, u) . import matplotlib.pyplot as plt . plt.plot(iscore) . [&lt;matplotlib.lines.Line2D at 0x7f3ecba1b450&gt;] . x = np.linspace(1.0, 12.0) l = 5.0*np.ones_like(x) u = 8.0*np.ones_like(x) iscore = interval_score(x, l, u) plt.plot(iscore) . [&lt;matplotlib.lines.Line2D at 0x7f3ecb504910&gt;] . CRPS . import scipy.stats as stats def crps_gaussian(y_pred, y_std, y_true, scaled=True): &quot;&quot;&quot; Return the negatively oriented continuous ranked probability score for held out data (y_true) given predictive uncertainty with mean (y_pred) and standard-deviation (y_std). Each test point is given equal weight in the overall score over the test set. Negatively oriented means a smaller value is more desirable. &quot;&quot;&quot; # Flatten num_pts = y_true.shape[0] y_pred = y_pred.reshape( num_pts, ) y_std = y_std.reshape( num_pts, ) y_true = y_true.reshape( num_pts, ) # Compute crps y_standardized = (y_true - y_pred) / y_std term_1 = 1 / np.sqrt(np.pi) term_2 = 2 * stats.norm.pdf(y_standardized, loc=0, scale=1) term_3 = y_standardized * (2 * stats.norm.cdf(y_standardized, loc=0, scale=1) - 1) crps_list = -1 * y_std * (term_1 - term_2 - term_3) crps = np.sum(crps_list) # Potentially scale so that sum becomes mean if scaled: crps = crps / len(crps_list) return crps . data = np.random.normal(loc=1.0, scale=1.0, size=1000) mus = np.ones_like(data) sigmas = np.ones_like(data) crps_gaussian(data, sigmas, mus, scaled=False) . 587.2681356774692 . PLot Gaussian and Laplace with same interval . For the laplace distribution . $f(med) = frac{1}{ sigma sqrt{2}} $ and therefore $D(x_{0.5}) = frac{ sigma^2}{2n}$ confidence interval of the median . $$ x - frac{ u_{1 - alpha/2} (n-1)0.707 s}{ sqrt{n}} leq median leq x + frac{u_{1 - alpha/2} (n-1)0.707 s}{ sqrt{n}}$$ . Example : . laplace disitrbution L(0, 2) : variance $s^2 = 2b^2 = 2.246$ Substitutint in above equation median = 0.0119 estimated b = 1.0596 . mu = 0.0119 b = 1.0596 n = 50 var = 2*b**2; print (&quot;Variance : &quot;, var) confidence_95 = (stats.norm.ppf(1 - 0.05/2) * 0.707 * var**0.5) print (mu-confidence_95 , mu+confidence_95) . Variance : 2.24550432 -2.0645642208852193 2.088364220885219 . stats.norm.ppf(1 - 0.05/2) * 0.707 * var . 3.111583069190677 . for i in np.linspace(0,0.99,num=10): print (i,stats.norm.ppf(i), stats.laplace.ppf(i)) . 0.0 -inf -inf 0.11 -1.2265281200366098 -1.5141277326297755 0.22 -0.7721932141886848 -0.8209805520698302 0.33 -0.4399131656732338 -0.4155154439616658 0.44 -0.15096921549677725 -0.12783337150988489 0.55 0.12566134685507416 0.1053605156578264 0.66 0.41246312944140495 0.3856624808119848 0.77 0.7388468491852137 0.7765287894989964 0.88 1.1749867920660904 1.4271163556401458 0.99 2.3263478740408408 3.912023005428145 . (0.29 * n**0.5)/(0.707* var**0.5 * 49) . 0.039501224521614836 . stats.norm.ppf(1 - 0.05/2) . 1.959963984540054 . import matplotlib.pyplot as plt import numpy as np import scipy.stats as stats import math mu = 0 variance = 1 sigma = math.sqrt(variance) x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100) plt.plot(x, stats.norm.pdf(x, mu, sigma)) plt.plot(x, stats.laplace.pdf(x, mu,sigma), c=&#39;r&#39;) plt.show() . Gaussian vs Laplace . 95% Confidence interval is $Gaussian ( 2 sigma) == Laplace(3b)$ . r = stats.norm.rvs(mu, sigma, size=1000) v = ((mu - 2*sigma) &lt; r ) &amp; (r&lt; (mu + 2*sigma)) np.sum(v)/1000 . 0.953 . r = stats.laplace.rvs(mu, sigma, size=1000) v = ((mu - 3*sigma) &lt; r ) &amp; (r&lt; (mu + 3*sigma)) np.sum(v)/1000 . 0.952 . for mu in np.linspace(0, 255, num=10): for sigma in np.linspace(0, 5, num=10): r = stats.laplace.rvs(mu, sigma, size=1000) v = ((mu - 3*sigma) &lt; r ) &amp; (r&lt; (mu + 3*sigma)) print (np.sum(v)/1000) . 0.0 0.943 0.962 0.959 0.962 0.947 0.941 0.953 0.949 0.941 0.0 0.962 0.956 0.955 0.95 0.959 0.952 0.943 0.95 0.939 0.0 0.947 0.951 0.943 0.954 0.941 0.961 0.951 0.949 0.963 0.0 0.961 0.955 0.956 0.951 0.939 0.941 0.95 0.949 0.956 0.0 0.943 0.941 0.956 0.948 0.949 0.955 0.942 0.953 0.948 0.0 0.945 0.949 0.957 0.951 0.953 0.954 0.949 0.955 0.954 0.0 0.957 0.945 0.945 0.935 0.944 0.945 0.961 0.95 0.95 0.0 0.951 0.942 0.954 0.953 0.953 0.954 0.941 0.947 0.955 0.0 0.942 0.95 0.956 0.939 0.939 0.952 0.951 0.95 0.951 0.0 0.95 0.94 0.968 0.95 0.949 0.958 0.939 0.947 0.945 . stats.laplace.ppf(1 - 0.05/2) . 2.99573227355399 . x = np.array([True, False]) y = np.array([True , True]) x &amp; y . array([ True, False]) . mu = 0 variance = 1 sigma = math.sqrt(variance) b = math.sqrt(var/2) . mu + 2*sigma . 2.0 . mu + 3*0.67 . 2.0100000000000002 . b = 0.67 laplace_variance = 2*b**2 print (laplace_variance) . 0.8978000000000002 . mu = 0 variance = 1 sigma = math.sqrt(variance) x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100) plt.plot(x, stats.norm.pdf(x, mu, sigma)) plt.plot(x, stats.laplace.pdf(x, mu,b), c=&#39;r&#39;) plt.show() . stats.laplace.ppf(0.975) . 2.99573227355399 . stats.laplace.ppf(0.025) . -2.995732273553991 .",
            "url": "https://deebuls.github.io/devblog/statistics/uncertainty/2022/02/05/proper-scoring-interval-score-crps.html",
            "relUrl": "/statistics/uncertainty/2022/02/05/proper-scoring-interval-score-crps.html",
            "date": " • Feb 5, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Intel Realsense Camera + Kinova Gen3 + Gazebo",
            "content": "Dataset shift . Packages installed . * * . realsense description for the urdf and the xacro files | sudo apt install ros-noetic-realsense2-camera -&gt; maybe not required | sudo apt install ros-noetic-realsense2-description -&gt; this is required | . | . Links . [1] https://answers.gazebosim.org//question/24989/adding-intel-camera-to-robot-end-effector/ [2] https://github.com/IntelRealSense/realsense-ros [3] https://github.com/pal-robotics-forks/realsense/blob/upstream/realsense2_description/urdf/_d435.urdf.xacro . https://answers.ros.org/question/348331/realsense-d435-gazebo-plugin/ . https://github.com/pal-robotics/realsense_gazebo_plugin/issues/7#issuecomment-609040406 .",
            "url": "https://deebuls.github.io/devblog/robotics/2022/01/19/Add-Realsense-Camera-Kinova-Gen3-Gazebo.html",
            "relUrl": "/robotics/2022/01/19/Add-Realsense-Camera-Kinova-Gen3-Gazebo.html",
            "date": " • Jan 19, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Embedding output for Multi-class classification ",
            "content": ". Pytorch Embedding . The pytorch embedding is a simple lookup table that stores embeddings of a fixed dictionary and size. This module is used to store embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding embeddings. . A good usage of this can be used as a replacement to one-hot-encoding and replace the loss function with regression losses. . Here we would like to visualize the different embeddings by plotting converting them to a 2 dimensional manifold. . Reference . [1] https://arxiv.org/pdf/1710.10393.pdf | [2] https://github.com/lancopku/label-embedding-network/blob/master/ComputerVision/resnet8.py | . import torch import seaborn as sns import pandas as pd import matplotlib.pyplot as plt import numpy as np . embedding = torch.nn.Embedding(10, 2, padding_idx=5) input = torch.LongTensor([0,1,2,3, 4, 5, 6, 7, 8, 9]) embed_output = embedding(input) embed_output = pd.DataFrame(embed_output.tolist()) embed_output . 0 1 . 0 -0.080791 | -1.381228 | . 1 -0.146998 | 0.281716 | . 2 0.746007 | -0.701610 | . 3 1.822672 | -1.249770 | . 4 0.450180 | -0.028375 | . 5 0.000000 | 0.000000 | . 6 -0.433445 | -0.090980 | . 7 -0.479569 | -0.597076 | . 8 0.313272 | -0.827177 | . 9 -0.703487 | -1.317037 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; embedding.weight . Parameter containing: tensor([[-0.0808, -1.3812], [-0.1470, 0.2817], [ 0.7460, -0.7016], [ 1.8227, -1.2498], [ 0.4502, -0.0284], [ 0.0000, 0.0000], [-0.4334, -0.0910], [-0.4796, -0.5971], [ 0.3133, -0.8272], [-0.7035, -1.3170]], requires_grad=True) . sns.scatterplot(data=embed_output, x=0, y=1) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4436ab1d30&gt; . sns.scatterplot(data=embed_output, x=0, y=1) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4436a10d60&gt; . embedding.weight . Parameter containing: tensor([[-0.0808, -1.3812], [-0.1470, 0.2817], [ 0.7460, -0.7016], [ 1.8227, -1.2498], [ 0.4502, -0.0284], [ 0.0000, 0.0000], [-0.4334, -0.0910], [-0.4796, -0.5971], [ 0.3133, -0.8272], [-0.7035, -1.3170]], requires_grad=True) . Pytorch IRIS . import pandas as pd from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, precision_score, recall_score import torch import torch.nn as nn import torch.nn.functional as F from torch.autograd import Variable . from sklearn.datasets import load_iris . class Net(nn.Module): # define nn def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(4, 100) self.fc2 = nn.Linear(100, 100) self.fc3 = nn.Linear(100, 3) self.softmax = nn.Softmax(dim=1) def forward(self, X): X = F.relu(self.fc1(X)) X = self.fc2(X) X = self.fc3(X) X = self.softmax(X) return X # load IRIS dataset X, y = load_iris(return_X_y=True) train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.8) print (train_y, test_y) . [1 2 1 0 2 2 1 1 1 1 2 1 1 2 1 0 0 0 2 2 0 2 0 2 2 2 0 1 0 2] [2 1 0 2 0 0 1 2 2 0 0 1 0 0 2 1 1 0 0 1 0 0 0 2 2 2 2 1 2 2 0 2 1 0 2 0 2 1 0 1 0 0 1 1 2 2 0 2 1 0 2 2 2 1 2 1 0 0 1 2 1 1 2 0 0 1 2 1 2 1 2 1 1 0 1 1 2 1 1 0 0 0 2 0 2 2 2 0 1 2 0 2 1 2 2 0 0 1 1 1 0 1 0 0 0 0 0 1 2 2 2 1 0 1 1 1 0 1 0 1] . # wrap up with Variable in pytorch train_X = Variable(torch.Tensor(train_X).float()) test_X = Variable(torch.Tensor(test_X).float()) train_y = Variable(torch.Tensor(train_y).long()) test_y = Variable(torch.Tensor(test_y).long()) net = Net() criterion = nn.CrossEntropyLoss()# cross entropy loss optimizer = torch.optim.SGD(net.parameters(), lr=0.01) for epoch in range(1000): optimizer.zero_grad() out = net(train_X) loss = criterion(out, train_y) loss.backward() optimizer.step() if epoch % 100 == 0: print (&#39;number of epoch {} loss {} &#39;.format(epoch, loss)) predict_out = net(test_X) _, predict_y = torch.max(predict_out, 1) print (&#39;prediction accuracy&#39;, accuracy_score(test_y.data, predict_y.data)) print (&#39;macro precision&#39;, precision_score(test_y.data, predict_y.data, average=&#39;macro&#39;)) print (&#39;micro precision&#39;, precision_score(test_y.data, predict_y.data, average=&#39;micro&#39;)) print (&#39;macro recall&#39;, recall_score(test_y.data, predict_y.data, average=&#39;macro&#39;)) print (&#39;micro recall&#39;, recall_score(test_y.data, predict_y.data, average=&#39;micro&#39;)) . number of epoch 0 loss 1.10525643825531 number of epoch 100 loss 0.906145453453064 number of epoch 200 loss 0.8332260251045227 number of epoch 300 loss 0.7851205468177795 number of epoch 400 loss 0.7443270087242126 number of epoch 500 loss 0.7093335390090942 number of epoch 600 loss 0.6817362308502197 number of epoch 700 loss 0.661094605922699 number of epoch 800 loss 0.6458678841590881 number of epoch 900 loss 0.6344795823097229 prediction accuracy 0.9833333333333333 macro precision 0.9833333333333334 micro precision 0.9833333333333333 macro recall 0.9833333333333334 micro recall 0.9833333333333333 . print (&#39;a {} {} &#39;.format(&#39;b&#39;, &#39;c&#39;)) . a b c . Embedding and Regression . class Net(nn.Module): # define nn def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(4, 100) self.fc2 = nn.Linear(100, 100) self.fc3 = nn.Linear(100, 2) #self.softmax = nn.Softmax(dim=1) def forward(self, X): X = F.relu(self.fc1(X)) X = self.fc2(X) X = self.fc3(X) return X . embed = torch.nn.Embedding(3,2) embed_train_y = Variable(embed(train_y)) embed_test_y = Variable(embed(test_y)) true_embed = embed(torch.LongTensor([0,1,2])) true_embed = pd.DataFrame(true_embed.tolist()) true_embed[&#39;y&#39;] = [0, 1, 2] sns.scatterplot(data=true_embed, x=0, y=1, hue=&#39;y&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f44369e5940&gt; . net = Net() criterion = nn.MSELoss()# cross entropy loss optimizer = torch.optim.SGD(net.parameters(), lr=0.01) for epoch in range(1000): optimizer.zero_grad() out = net(train_X) loss = criterion(out, embed_train_y) loss.backward() optimizer.step() if epoch % 100 == 0: print (&#39;number of epoch {} loss {} &#39;.format(epoch, loss)) predict_out = net(test_X) print (&#39; Test MSE &#39;, torch.abs(predict_out - embed_test_y).mean()) . number of epoch 0 loss 0.3358815610408783 number of epoch 100 loss 0.042973946779966354 number of epoch 200 loss 0.036721814423799515 number of epoch 300 loss 0.03191041201353073 number of epoch 400 loss 0.027886144816875458 number of epoch 500 loss 0.024619022384285927 number of epoch 600 loss 0.021988747641444206 number of epoch 700 loss 0.01982944831252098 number of epoch 800 loss 0.018047431483864784 number of epoch 900 loss 0.02423388510942459 Test MSE tensor(0.0877, grad_fn=&lt;MeanBackward0&gt;) . embed_output = pd.DataFrame(predict_out.tolist()) embed_output[&#39;y&#39;] = test_y.tolist() sns.scatterplot(data=embed_output, x=0, y=1, hue=&#39;y&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4436964520&gt; . Gaussian Regression . class GaussianNet(nn.Module): # define nn def __init__(self): super(GaussianNet, self).__init__() self.fc1 = nn.Linear(4, 100) self.fc2 = nn.Linear(100, 100) self.fc3 = nn.Linear(100, 2) self.variance = nn.Linear(100, 2) #self.softmax = nn.Softmax(dim=1) def forward(self, X): X = F.relu(self.fc1(X)) X = self.fc2(X) out = self.fc3(X) var = F.softplus(self.variance(X)) return out, var . net = GaussianNet() criterion = nn.GaussianNLLLoss()# Gaussian NLL loss #criterion = nn.MSELoss()# Gaussian NLL loss #optimizer = torch.optim.SGD(net.parameters(), lr=0.01) optimizer = torch.optim.AdamW(net.parameters(), lr=0.001) for epoch in range(1000): optimizer.zero_grad() out, var = net(train_X) loss = criterion(out, embed_train_y, var) loss.backward() optimizer.step() if epoch % 100 == 0: print (&quot;out &quot; , out[0]) print (&quot;var &quot; , var[0]) print (&#39;number of epoch {} loss {} &#39;.format(epoch, loss)) predict_out, predict_var = net(test_X) print (&#39; Test MSE &#39;, torch.abs(predict_out - embed_test_y).mean()) . out tensor([ 1.2781, -0.3668], grad_fn=&lt;SelectBackward0&gt;) var tensor([0.5012, 0.6097], grad_fn=&lt;SelectBackward0&gt;) number of epoch 0 loss 0.41761282086372375 out tensor([ 6.1669e-01, -5.4980e-04], grad_fn=&lt;SelectBackward0&gt;) var tensor([0.0310, 0.0035], grad_fn=&lt;SelectBackward0&gt;) number of epoch 100 loss -1.5372980833053589 out tensor([ 0.5606, -0.0783], grad_fn=&lt;SelectBackward0&gt;) var tensor([0.0197, 0.0022], grad_fn=&lt;SelectBackward0&gt;) number of epoch 200 loss -2.0017826557159424 out tensor([ 0.5585, -0.0794], grad_fn=&lt;SelectBackward0&gt;) var tensor([0.0341, 0.0144], grad_fn=&lt;SelectBackward0&gt;) number of epoch 300 loss -1.6041406393051147 out tensor([ 0.6390, -0.1252], grad_fn=&lt;SelectBackward0&gt;) var tensor([0.0173, 0.0026], grad_fn=&lt;SelectBackward0&gt;) number of epoch 400 loss -2.2373907566070557 out tensor([ 0.6222, -0.0581], grad_fn=&lt;SelectBackward0&gt;) var tensor([0.0159, 0.0107], grad_fn=&lt;SelectBackward0&gt;) number of epoch 500 loss -2.0754570960998535 out tensor([ 0.6291, -0.0862], grad_fn=&lt;SelectBackward0&gt;) var tensor([0.0069, 0.0005], grad_fn=&lt;SelectBackward0&gt;) number of epoch 600 loss -2.6287074089050293 out tensor([ 0.6051, -0.0870], grad_fn=&lt;SelectBackward0&gt;) var tensor([0.0062, 0.0011], grad_fn=&lt;SelectBackward0&gt;) number of epoch 700 loss -2.4698598384857178 out tensor([ 0.6261, -0.0507], grad_fn=&lt;SelectBackward0&gt;) var tensor([0.0120, 0.0603], grad_fn=&lt;SelectBackward0&gt;) number of epoch 800 loss -1.6372172832489014 out tensor([ 0.6267, -0.0832], grad_fn=&lt;SelectBackward0&gt;) var tensor([0.0051, 0.0049], grad_fn=&lt;SelectBackward0&gt;) number of epoch 900 loss -2.361809015274048 Test MSE tensor(0.0631, grad_fn=&lt;MeanBackward0&gt;) . embed_output = pd.DataFrame(predict_out.tolist()) embed_output[&#39;y&#39;] = test_y.tolist() embed_output[[&#39;var_0&#39;, &#39;var_1&#39;]] = predict_var.tolist() embed_output[&#39;combined_var&#39;] = embed_output[&#39;var_0&#39;] + embed_output[&#39;var_1&#39;] sns.scatterplot(data=embed_output, x=0, y=1, hue=&#39;y&#39;, size=&#39;combined_var&#39;, sizes=(4, 400), alpha=.5, palette=&quot;muted&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f44368c7640&gt; . sns.scatterplot(data=embed_output, x=&#39;var_0&#39;, y=&#39;var_1&#39;, hue=&#39;y&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f44369e5280&gt; . embed_output.describe() . 0 1 y var_0 var_1 combined_var . count 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | . mean 0.327190 | 0.036141 | 0.966667 | 0.010574 | 0.000746 | 0.011319 | . std 0.291685 | 0.104080 | 0.819237 | 0.007504 | 0.000511 | 0.007713 | . min -0.689349 | -0.107067 | 0.000000 | 0.000642 | 0.000074 | 0.000841 | . 25% 0.145816 | -0.073233 | 0.000000 | 0.003095 | 0.000380 | 0.004076 | . 50% 0.418697 | 0.028199 | 1.000000 | 0.011524 | 0.000624 | 0.012242 | . 75% 0.534689 | 0.135289 | 2.000000 | 0.015900 | 0.000967 | 0.016656 | . max 0.673590 | 0.237829 | 2.000000 | 0.035810 | 0.002695 | 0.037705 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Learning Embedding while training . Million dollar question will the network converge while training . class Net(nn.Module): # define nn def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(4, 100) self.fc2 = nn.Linear(100, 100) self.fc3 = nn.Linear(100, 2) #self.softmax = nn.Softmax(dim=1) self.embed = nn.Embedding(3,2,max_norm=True) self.embed.weight = nn.Parameter(torch.normal(0, 5, size=(3, 2))) def embed_label(self, Y): return self.embed(Y) def forward(self, X, Y): X = F.relu(self.fc1(X)) X = self.fc2(X) return self.fc3(X), self.embed(Y) . #embed = torch.nn.Embedding(3,2) net = Net() embed_train_y = Variable(net.embed_label(train_y)) embed_test_y = Variable(net.embed_label(test_y)) true_embed = net.embed_label(torch.LongTensor([0,1,2])) true_embed = pd.DataFrame(true_embed.tolist()) true_embed[&#39;y&#39;] = [0, 1, 2] sns.scatterplot(data=true_embed, x=0, y=1, hue=&#39;y&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f44359ea490&gt; . for name, param in net.named_parameters(): if param.requires_grad: print (name, param.shape) . fc1.weight torch.Size([100, 4]) fc1.bias torch.Size([100]) fc2.weight torch.Size([100, 100]) fc2.bias torch.Size([100]) fc3.weight torch.Size([2, 100]) fc3.bias torch.Size([2]) embed.weight torch.Size([3, 2]) . criterion = nn.MSELoss()# cross entropy loss optimizer = torch.optim.SGD(net.parameters(), lr=0.01) #embed_optimizer= torch.optim.SGD(embed.parameters(), lr=1e-1) before = torch.zeros_like(net.embed.weight) before_fc3 = torch.zeros_like(net.fc3.weight) for epoch in range(1000): net.zero_grad() #embed_optimizer.zero_grad() out, embed_train_y = net(train_X, train_y) embed_train_y = Variable(embed_train_y) loss = criterion(out, embed_train_y) loss.backward() optimizer.step() #embed_optimizer.step() if epoch % 100 == 0: print (&#39;number of epoch {} loss {} &#39;.format(epoch, loss)) after = net.embed.weight after_fc3 = net.fc3.weight # calculate the diff between the weights before an update and after the update** print (&#39;After update : embed equal&#39;, torch.equal(before.data, after.data)) before = net.embed.weight.clone() print (&quot;fc3 equal &quot;, torch.equal(before_fc3.data, after_fc3.data)) before_fc3 = net.fc3.weight.clone() predict_out,embed_test_y = net(test_X, test_y) print (&#39; Test MSE &#39;, torch.abs(predict_out - embed_test_y).mean()) . number of epoch 0 loss 0.7351046800613403 After update : embed equal False fc3 equal False number of epoch 100 loss 0.2303016483783722 After update : embed equal True fc3 equal False number of epoch 200 loss 0.17618589103221893 After update : embed equal True fc3 equal False number of epoch 300 loss 0.1351977288722992 After update : embed equal True fc3 equal False number of epoch 400 loss 0.11154220253229141 After update : embed equal True fc3 equal False number of epoch 500 loss 0.09350577741861343 After update : embed equal True fc3 equal False number of epoch 600 loss 0.0817851573228836 After update : embed equal True fc3 equal False number of epoch 700 loss 0.08192727714776993 After update : embed equal True fc3 equal False number of epoch 800 loss 0.14396952092647552 After update : embed equal True fc3 equal False number of epoch 900 loss 0.1144104152917862 After update : embed equal True fc3 equal False Test MSE tensor(0.2455, grad_fn=&lt;MeanBackward0&gt;) . true_embed = net.embed_label(torch.LongTensor([0,1,2])) true_embed = pd.DataFrame(true_embed.tolist()) true_embed[&#39;y&#39;] = [0, 1, 2] sns.scatterplot(data=true_embed, x=0, y=1, hue=&#39;y&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4435975310&gt; . f, ax = plt.subplots(1,1, figsize=(5,5)) embed_output = pd.DataFrame(predict_out.tolist()) embed_output[&#39;y&#39;] = test_y.tolist() sns.scatterplot(data=embed_output, x=0, y=1, hue=&#39;y&#39;, alpha=.5, palette=&quot;muted&quot;, ax=ax) sns.scatterplot(data=true_embed, x=0, y=1, style=&#39;y&#39;, ax=ax, color=&#39;r&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f44357dedc0&gt; .",
            "url": "https://deebuls.github.io/devblog/deep%20learning/pytorch/2022/01/11/pytroch-embedding-2D-visualization.html",
            "relUrl": "/deep%20learning/pytorch/2022/01/11/pytroch-embedding-2D-visualization.html",
            "date": " • Jan 11, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Flexible Distributions as an Approach to Robustness",
            "content": "Understanding paper &quot;Flexible Distributions as an Approach to Robustness : The Skew-t Case&quot; by Adelchi Azzalini . Introduction . When a continuous variable of interest spans the whole real line, an interesting distribution is the one with density function . $$ c_v exp(- frac{|x|^{v}}{v}), qquad x in mathbb{R} $$ . where $ V &gt; 0 $ and . $$ C_v = frac{1}{ 2 v^{1/v} Gamma(1 + frac{1}{v})}$$ . Here the parameter $v$ manoeuvres the tail weight in the sense that . $v$ = 2 corresponds to the normal distribution, | $0 &lt; v &lt; 2$ produces tails heavier than the normal ones, | $ v &gt; 2 $ produces lighter tails. | import sympy as sym print (sym.__version__) . 1.8 . v = sym.Symbol(&#39;v&#39;) x = sym.Symbol(&#39;x&#39;) gamma_v = sym.gamma(v + 1/v) c_v = 1 / (2 * v**(1/v) * gamma_v) skew_t = c_v * sym.exp(-sym.Abs(x)**v/ v) . p1 = sym.plot(skew_t.subs(v, 2), label=&quot;Gaussian&quot;, line_color=&#39;blue&#39;, show=False, legend=True) p2 = sym.plot(skew_t.subs(v, 1), label=&quot;Heavy Tail&quot;, line_color=&#39;red&#39;, show=False) p3 = sym.plot(skew_t.subs(v, 3), label=&quot;Thin Tail&quot;, line_color=&#39;cyan&#39;, show=False) p4 = sym.plot(skew_t.subs(v, 0.5), label=&quot;Heavy Tail&quot;, line_color=&#39;black&#39;, show=False) p1.append(p2[0]) p1.append(p3[0]) p1.append(p4[0]) p1.show() . p1 = sym.plot(-sym.log(skew_t.subs(v, 2)), (x, -1, 1), label=&quot;Gaussian&quot;, line_color=&#39;blue&#39;, show=False, legend=True) p2 = sym.plot(-sym.log(skew_t.subs(v, 1)), (x, -1, 1), label=&quot;Heavy Tail&quot;, line_color=&#39;red&#39;, show=False, legend=True) p3 = sym.plot(-sym.log(skew_t.subs(v, 3)), (x, -1, 1), label=&quot;Thin Tail&quot;, line_color=&#39;cyan&#39;, show=False, legend=True) p4 = sym.plot(-sym.log(skew_t.subs(v, 0.6)), (x, -1, 1), label=&quot;Heavy Tail&quot;, line_color=&#39;black&#39;, show=False) p1.append(p2[0]) p1.append(p3[0]) p1.append(p4[0]) p1.show() . Notes . [Analyzing distribution with Sympy] (https://brianzhang01.github.io/2018/04/distributions-with-sympy/) . sym.init_printing() x, t = sym.symbols(&#39;x t&#39;, real=True) def area(dist): return sym.simplify(sym.integrate(dist, (x, -sym.oo, sym.oo))) def mean(dist): return area(dist*x) def EX2(dist): return area(dist*x**2) def variance(dist): return sym.simplify(EX2(dist) - mean(dist)**2) def mgf(dist): return sym.simplify(area(dist*sym.exp(x*t))) def latex(result): return &quot;$&quot; + sym.latex(result) + &quot;$ n&quot; def summarize(dist): #print (&quot;Distribution: &quot; + latex(dist)) (dist) print (&quot;Area: &quot; + latex(area(dist))) print (&quot;Mean: &quot; + latex(mean(dist))) print (&quot;Variance: &quot; + latex(variance(dist))) print (&quot;MGF: &quot; + latex(mgf(dist))) summarise = summarize # alias . # Define other symbols that show up mu = sym.symbols(&#39;mu&#39;, real=True) sigma, a, b, lamb, nu = sym.symbols(&#39;sigma a b lambda nu&#39;, positive=True) . # Normal Distribution normal = (2*sym.pi*sigma**2) ** sym.Rational(-1, 2) * sym.exp(-(x-mu)**2/(2*sigma**2)) summarize(normal) sym.pprint(normal) . Area: $1$ Mean: $ mu$ Variance: $ sigma^{2}$ MGF: $e^{ frac{t left(2 mu + sigma^{2} t right)}{2}}$ 2 -(-μ + x) ─────────── 2 2⋅σ √2⋅ℯ ─────────────── 2⋅√π⋅σ . sym.pprint (sym.latex(mu)) . mu . sym.pprint(skew_t.subs(v, 2)) . 2 -│x│ ────── 2 √2⋅ℯ ────────── 3⋅√π . from sympy.stats import * z = sym.Symbol(&#39;z&#39;) v = Normal(&#39;v&#39;, 30, 1) pdf = density(v) sym.plot(pdf(z), (z, 27, 33)) . &lt;sympy.plotting.plot.Plot at 0x7f119269c9d0&gt; . sym.pprint(pdf(z)) . 2 -(z - 30) ─────────── 2 √2⋅ℯ ─────────────── 2⋅√π . gamma_v = sym.gamma(v + 1/v) print (gamma_v) . gamma(v + 1/v) . gamma_v.subs(v,2) . $ displaystyle frac{3 sqrt{ pi}}{4}$",
            "url": "https://deebuls.github.io/devblog/robust%20statistics/maximum%20likelihood/2021/09/21/Robust-Statistics.html",
            "relUrl": "/robust%20statistics/maximum%20likelihood/2021/09/21/Robust-Statistics.html",
            "date": " • Sep 21, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Neurips Competition Uncertainty",
            "content": "Notes on Neurips Competition . I am participating in the Neurips Bayesian Deep Learning Competition, I will like to journal my notes here . Idea . Broad idea is to use Evidential loss function, Dropout, TTA combination. . Journal . 8th August Working in superconvergence | . | 13th August Roll back to pytorch-cifar and modifications | . | 14th August Training on evidential loss reaches only 83% accuracy in 300 epochs. | Why is evidental loss reducing the accuracy | . | 15th August Training with CE, AdamW, OnecylceLR? Can we improve training speed. | . | 17th August Dirichlet loss function. | . | 18th August Dirichlet + Mixup : best results, touched 90% | . | . Reference . Super Convergence cifar10, pytorchdata, cifar training, wideresenet, 92 accuracy | . | Pytorch Cifar SOA pytorch cifar10, all models, | . | Mixup pytorch mixup data combining while training | . | . Pytorch cifar . Model data criterion optim scheduler epochs accuracy link Notes . Resnet18 | pytorch | cross-entropy | SGD | annealing-200 | 200 | 94 | 1 |   | . Resnet20 | pytorch | cross-entropy | SGD | annealing-200 | 200 | 89 | 1 |   | . Resnet20 | tf | cross-entropy | SGD | annealing-200 | 200 | 90 | 1 |   | . Resnet20 | tf | Evidential | SGD | annealing-200 | 600 | 73/??/83 | 1 | Added randmErasing | . Resnet20 | tf | Label smooting | SGD | annealing-200 | 200 |   | ?? | ?? | . Resent20 | tf | cross-entropy | AdamW | 1 cycle | 30 | 83 | 1 |   | . Resnet20 | tf | cross-entropy | AdamW | 1 cycle | 100 | 88 | 1 | max_lr = 0.01 | . Resnet20 | tf | cross-entropy | AdamW | 1 cycle | 30 | 50 | 1 | max_lr=0.1 | . Resnet20 | tf | cross-entropy | AdamW | 1 cycle | 30 | 80 | 1 | max_lr=0.05 | . Resnet20 | tf | Evidential | AdamW | 1 cycle | 30 | 69 | 1 | max_lr=0.05 | . Resnet20 | tf | Evidential | AdamW | annealing-200 | 200 | 75 | 1 | max_lr=0.01 | . Resnet20 | tf | cross-entropy | AdamW | 1 cycle | 200 | 89 | 1 | max_lr = 0.05 | . Resnet20 | tf | cross-entropy | AdamW | 1 cycle | 200 | 89 | 1 | max_lr = 0.05, randomErase | .",
            "url": "https://deebuls.github.io/devblog/uncertainty/metrics/2021/08/08/Neurips-Competition-Uncertainty.html",
            "relUrl": "/uncertainty/metrics/2021/08/08/Neurips-Competition-Uncertainty.html",
            "date": " • Aug 8, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Using uncertainties in DNN application",
            "content": "Application of Uncertainty Estimation in DNN . Objective . We would like to look into two papers which have developed methodologies to use the uncertatiny estimated by the deep neural network(DNN) in their system. The goal is to find the pros and cons of these methodologies . Paper 1: Benchmarking uncertatinty estimation methods in Deep learning with Safety-Related Metrics . In this paper they propose two new Safety-Related Metrics: Remaining Error Rate(RER) and Remaining Accuracy Rate(Rate). Here is definition as per the paper: . A system which relies on these estimates is expected to be in functional mode if the predictions are certain and in fall-back/mitigation mode if the prediction is uncertain. However, the most critical result regarding safety are the predictions where the model is certain about its prediction but incorrect (CI). We call the ratio of the number of certain but incorrect samples to all samples the Remaining Error Rate (RER). For minimizing the overall risk, it needs to be as low as possible. Nonetheless, if a model would always give a low confidence as output, the system would constantly remain in fall-back mode and will be unable to provide the intended functionality. Therefore, the ratio of the number of certain and correct samples to all samples - we call it the Remaining Accuracy Rate (RAR) - needs to be as high as possible to stay in performance mode for most of the time. . Interpretation . The predictions from the DNN are classified into 4 sections as below (Table 1 from paper) .   Certain Uncertain . Correct | CC | UC | . Incorrect | CI | UI | . The definition of the metrics are : . $RER = frac{CI}{CC+CI+UC+UI}$ $RAR = frac{CC}{CC+CI+UC+UI}$ . Pros . Its a very simple metric. | Simplicity of metric is a very important thing for usability of metrics. | . Cons . A minor issue will be on the threshold which seprates Certain vs Uncertain. Is it 99% or 90% etc. All will yield different results | A major problem which we consider is the assumption in which the uncertatiny is being planned to be used in the system. A system which relies on these estimates is expected to be in functional mode if the predictions are certain and in fall-back/mitigation mode if the prediction is uncertain. . | This means that the system has 2 modes A functional mode | A fall-back/mitigation mode | . | Is this a safe assumption with regards to deployment of DNN? | Can an application deploying DNN have 2 modes ? | What should an autonomous car in fall-back mode do ? | . :bangbang: | The assumption on how a system uses uncertatiny is that the system has 2 modes functional and fall-back | :-: | :- | . Paper 2 : Fail-Safe Execution of Deep learning based Systems through Uncertatiny Monitoring . In this paper, as the title suggests they create a separate model called the Supervisor Model which will monitor the uncertainty of Deep learning and avoid any faults in the system | What is a supervisor model : Network supervision can be viewed as a binary classification task: malicious samples, i.e., inputs which lead to a misclassification (for classification problems) or to severe imprecision (in regression problems) are positive samples that have to be rejected. Other samples, also called benign samples, are negative samples in the binary classification task. An uncertainty based supervisor accepts an input i as a benign sample if its uncertainty u(i) is lower than some threshold t. The choice of t is a crucial setting, as a high t will fail to reject many malicious samples (false negatives) and a low t will cause too many false alerts (false positives). . | Thus the supervisor is a binary classification task to avoid beningn samples. They also define a metric S-Score which combined measures the performance of both the model and the supervisor model | There is lot of similarity with respect to the above paper here also | . Pros . They have made a library out of it such that any model can be used. | The threshold on which to make the decission is now being learned by the data. | . Cons . Again, these method is based on the assumption that the system which uses DNN has 2 modes of operation( normal mode and fall-back mode) | . :bangbang: | The same assumption on how a system uses uncertatiny, that the system has 2 modes functional and fall-back | :—: | :— | . Conclusion . All methods are based on the assumption that the system has 2 modes of operation | The uncertatiny estimation is used to determine whethere the DNN output should be trusted or should be avoided | . This is not enough . The methods which use DNN dont have a fall back mode. If there was an non DNN based method then by “First rule of Machine/Deep Learning” that will be used for solving the problem | . | There can be argument to say that there are redundant DNN systems and this method can be used to kick-off redundant system Even this argument is not valid as if you have redundant system, you should use all of them and make a decision | . | . Solutions . The one solution which I have been workin is about not binarizing the probability but the propagating it through the system | The best example is of the filters which have been developed over years to handle uncertain sensors. | . References . [1]M. Weiss and P. Tonella, “Fail-Safe Execution of Deep Learning based Systems through Uncertainty Monitoring,” arXiv:2102.00902 [cs], Feb. 2021, Accessed: Apr. 13, 2021. [Online]. Available: http://arxiv.org/abs/2102.00902. . | [2]M. Henne, A. Schwaiger, K. Roscher, and G. Weiss, “Benchmarking Uncertainty Estimation Methods for Deep Learning With Safety-Related Metrics,” p. 8, 2020. . | .",
            "url": "https://deebuls.github.io/devblog/uncertainty/2021/04/13/how-to-use-uncertainty-estimate-from-dnn.html",
            "relUrl": "/uncertainty/2021/04/13/how-to-use-uncertainty-estimate-from-dnn.html",
            "date": " • Apr 13, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "One-vs-All Classifier",
            "content": ". The paper . Padhy, S. et al. (2020) ‘Revisiting One-vs-All Classifiers for Predictive Uncertainty and Out-of-Distribution Detection in Neural Networks’, arXiv:2007.05134 [cs, stat]. Available at: http://arxiv.org/abs/2007.05134 (Accessed: 26 January 2021). . Abstract . Problem . Out-of-Distribution - finding on which samples the classifier should not predict | OOD using uncertainty estimate | uncertainty estimate - correctly estimate the confidence (uncertainty) in the prediction | . How . Using oves-vs-all classifier | distance-based logit representation to encode uncertainty | . Introduction . Capturing epistemic uncertainty is more important, as it captures model&#39;s lack of knowledge about data . | Three different paradigms to measure uncertainty . in-distribution calibration of models measured on an i.i.d. test set, | robustness under dataset shift, and | anomaly/out-of-distribution (OOD) detection, which measures the ability of models to assign low confidence predictions on inputs far away from the training data. | | . Unique selling points . we first study the contribution of the loss function used during training to the quality of predictive uncertainty of models. | Specifically, we show why the parametrization of the probabilities underlying softmax cross-entropy loss are ill-suited for uncertainty estimation. | We then propose two simple replacements to the parametrization of the probability distribution: a one-vs-all normalization scheme that does not force all points in the input space to map to one of the classes, thereby naturally capturing the notion of “none of the above”, and | a distance-based logit representation to encode uncertainty as a function of distance to the training manifold. | | . Experiments . Under Dataset Shift . CIFAR 10 Corrupted different intensity | CIFAR 100 Corrupted | . | OOD . CIFAR 100 vs CIFAR10 | CIFAR100 vs SVHN | . | Comparison of learned class centers . | Reliability Diagrams . | Imagenet . | CLINC Intent Classification Dataset . |",
            "url": "https://deebuls.github.io/devblog/probability/regression/uncertainty/robustness/2021/01/01/one-vs-all-classifier-revisiting.html",
            "relUrl": "/probability/regression/uncertainty/robustness/2021/01/01/one-vs-all-classifier-revisiting.html",
            "date": " • Jan 1, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "DNN Wiki",
            "content": "What to learn? . I wanted to develop glossary/Wiki of DNN related topics and my explanation of them so that I can be sure that I know the topics. But for that I needed a list of relevant topics. DNN is an exponentially exploding field and with low signa to noise ratio. So it becomes really difficult in fooling up with the new work without having a firm understanding of what is firm knowledge. This selection of topics should help any new commer to be sure that if I know what these topics are then you can claim that you know a little about deep learning. . Deep Learning vs Other Machine Learning Approaches | The Essential Math of Artificial Neurons | The Essential Math of Neural Networks | Activation Functions | Cost/Loss Functions | Stochastic Gradient Descent | Backpropagation | Mini-Batches | Learning Rate | Optimizers (e.g., Adam, Nadam) | Glorot/He Weight Initialization | Dense Layers | Softmax Layers | Dropout | Data Augmentation | . References . https://aiplus.odsc.com/courses/deep-learning-with-tensorflow-2-and-pytorch-1 | |",
            "url": "https://deebuls.github.io/devblog/learning/2020/07/22/Learning-DNN.html",
            "relUrl": "/learning/2020/07/22/Learning-DNN.html",
            "date": " • Jul 22, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Different types of uncertainty in DNN systems?",
            "content": "What are the uncertainties in DNN? . This question is easily answered if you have read a little bit of DNN literature. The most popular answer is . Aleatoric uncertainty | Epistemic uncertainty | . An additional statement is always added to these different types of uncertainty which will claim that . Aleatoric uncertatinty is model uncertainty | Epistemic uncertatinty is data uncertainty | . But what actually are these values and what do they explain ? Lets do a brief overview of different literature to answer these questions. . Alternative definitions . “Aleatory uncertainty is also referred to in the literature as variability, irreducible uncertainty, inherent uncertainty, and stochastic uncertainty. Epistemic uncertainty is also termed reducible uncertainty, subjective uncertainty, and state-of-knowledge uncertainty.” [2] . “Aleatory uncertainties are described as arising from inherent variabilities or randomness in systems, whereas epistemic uncertainties are due to imperfect knowledge.” [1] . Important points . Epistemic sources of uncertatinty is reducible but aleatory uncertainty is not reducible[1] | References . [1] Probability is Perfect, but we Can’t Elicitit Perfectly Anthony O’Hagan &amp; Jeremy E. Oakley http://www.yaroslavvb.com/papers/epistemic.pdf . [2] Challenge problems: uncertainty in system response given uncertain parameters Author links open overlay panelWilliam L.Oberkam https://www.sciencedirect.com/science/article/pii/S0951832004000493 .",
            "url": "https://deebuls.github.io/devblog/uncertainty/2020/05/29/Explanation-DNN-Uncertainty.html",
            "relUrl": "/uncertainty/2020/05/29/Explanation-DNN-Uncertainty.html",
            "date": " • May 29, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Regression Uncertainty",
            "content": "Regression . In machine learning problems we take input data $x$ and use it to predict the output data $y$. If the predicted data is continuous or discrete. When the output data $y$ is continuous then the problem is called regression while if its is discrete then the problem is called classification . . Deep neural networks have prooved to be a good tool for solving regression tasks. Currently there are new improvements in making the regression task robust by not only predicting the output data $y$ but also the corresponding uncertainty. . In this blog we will look into 2 methods of representing uncertatinty from literature. . Epistemic uncertatinty (Data uncertatinty) [1] | Aleotary uncertainty (Model uncertatinty) [2] | . Hierarchical Modelling . In regression task can be modelled in 3 different methods based on which all uncertainty needs to be accounted for during architecture of the deep network. The different layers and their hierarchichal structre is represented in the image above. . Name loss function uncertatinty output distribution . Deterministic Regression | mean square error | not modeled | $ hat{y}$ | - | . Likelihood Regression | Maximum likelihood estimation | data | $ hat{y}$, $ sigma^2$ | Normal | . Evidential Regression | Maximum likelihood estimation | data + model | $ hat{y}$,$ alpha$, $ beta$, $ lambda$ | Normal inverse gamma | . Dataset . We will start by creating a one dimension synthetic regression dataset. The synthetic dataset has added priori noise. The dataset is generated from $$ y = frac{ sin(3x)}{3x} + epsilon $$ where $ epsilon sim mathcal{N}(0, 0.02)$ . The training dataset consist of $x$ values between $-3 leq x leq 3$; while the test dataset consist of values outside of training $-4 leq x leq 4$ . Model . We will be using a common model for all the different learning formulation. . # this is one way to define a network class Net(torch.nn.Module): def __init__(self, n_feature, n_hidden, n_output): super(Net, self).__init__() self.hidden = torch.nn.Linear(n_feature, n_hidden) # hidden layer self.hidden_2 = torch.nn.Linear(n_hidden, n_hidden) self.predict = torch.nn.Linear(n_hidden, n_output) # output layer def forward(self, x): x = self.hidden(x) x = F.relu(self.hidden_2(x)) # activation function for hidden layer x = self.predict(x) # linear output return x . Deterministic Regression . Loss function . Mean square error $$ mathcal{L} = frac{1}{2} vert vert y_i - hat{y} vert vert ^2$$ . Output . The model will have single output $ hat{y}$ . msenet = Net(n_feature=1, n_hidden=10, n_output=1) # define the network print(msenet) # net architecture #optimizer = torch.optim.SGD(net.parameters(), lr=0.01) optimizer = torch.optim.Adam(msenet.parameters(), lr=0.001) loss_func = torch.nn.MSELoss() # this is for regression mean squared loss # train the network for t in range(10000): prediction = msenet(x) # input x and predict based on x loss = loss_func(prediction, y) # must be (1. nn output, 2. target) optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if t%1000 == 0: print(loss) prediction = msenet(test_x) plot_prediction(test_x, test_y, prediction) . Net( (hidden): Linear(in_features=1, out_features=10, bias=True) (hidden_2): Linear(in_features=10, out_features=10, bias=True) (predict): Linear(in_features=10, out_features=1, bias=True) ) tensor(0.5153, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0478, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0023, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) . Likelihood Regression . Loss function . In here we will use the maximum likelihood estimation error, which is also called as the Negative log likelihood. . $$ mathcal{L} = - left(-0.5 log(2 pi) - 0.5 sum log( sigma^2) - 0.5 sum frac{(y - hat{y})^2}{ sigma^2} right) $$ . Output . We will create a deep net with 2 output variables. . predicted output $ hat{y}$ | the variance of the normal distribution $ sigma^2$ | . Note . We aer also adding a regularizer $ frac{ | y - hat{y} | }{ sigma^2}$. This helps to regluarize the learning. It is inversely related to $ sigma^2$ and it scales based on distance from the actual value. . The regularizer ensures that values predicted far from true values have higher variance.So for values near to the predicted it gives less loss but values far away from prediction gives a large loss. . # Loss Function class MLELoss(torch.nn.Module): def __init__(self, weight=None, size_average=True): super(MLELoss, self).__init__() def forward(self, inputs, targets, smooth=1): targets = targets.view(-1) #converting variable to single dimension mu = inputs[:,0].view(-1) #extracting mu and sigma_2 logsigma_2 = inputs[:,1].view(-1) #logsigma and exp drives the variable to positive values always sigma_2 = torch.exp(logsigma_2) kl_divergence = (targets - mu)**2/sigma_2 #Regularizer mse = -0.5 * torch.sum(((targets - mu)**2)/sigma_2) sigma_trace = -0.5 * torch.sum(sigma_2) log2pi = -0.5 * np.log(2 * np.pi) J = mse + sigma_trace + log2pi loss = -J + kl_divergence.sum() return loss mlenet = Net(n_feature=1, n_hidden=10, n_output=2) # define the network print(mlenet) # net architecture optimizer = torch.optim.Adam(mlenet.parameters(), lr=0.001) loss_func = MLELoss() # this is for regression mean squared loss # train the network for t in range(10000): prediction = mlenet(x) # input x and predict based on x loss = loss_func(prediction, y) # must be (1. nn output, 2. target) optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if t%1000 == 0: print(loss) prediction = mlenet(test_x) mu = prediction[:,0] sigma2 = torch.exp(prediction[:,1]) sigma = torch.sqrt(sigma2) plot_prediction(test_x, test_y, mu, sigma) . Net( (hidden): Linear(in_features=1, out_features=10, bias=True) (hidden_2): Linear(in_features=10, out_features=10, bias=True) (predict): Linear(in_features=10, out_features=2, bias=True) ) tensor(68.1900, grad_fn=&lt;AddBackward0&gt;) tensor(6.4617, grad_fn=&lt;AddBackward0&gt;) tensor(6.1683, grad_fn=&lt;AddBackward0&gt;) tensor(6.1068, grad_fn=&lt;AddBackward0&gt;) tensor(6.0325, grad_fn=&lt;AddBackward0&gt;) tensor(5.9853, grad_fn=&lt;AddBackward0&gt;) tensor(5.9657, grad_fn=&lt;AddBackward0&gt;) tensor(5.9485, grad_fn=&lt;AddBackward0&gt;) tensor(5.9929, grad_fn=&lt;AddBackward0&gt;) tensor(5.9366, grad_fn=&lt;AddBackward0&gt;) . Evidential Regression . Evidential regression is based on paper [2] (Amini &amp; e.t.al, 2019), which is based on the ideas of [3, 4] that if we represent the output of the model with a higher order data distribution its possible to model the data and model uncertainties. . Loss . $$ mathcal{L} = left( frac{ Gamma( alpha - 0.5)}{4 Gamma( alpha) lambda sqrt beta} right) left( 2 beta(1 + lambda) + (2 alpha -1) lambda(y_i - hat{y})^2 right)$$ . output . The model has 4 outputs . $ hat{y}$ | $ alpha$ | $ beta$ | $ lambda$ | . Regularizer . Regularizer is required to penalize the loss function for OOD predictions. . $$ | y - hat{y} | ^2 (2 alpha + lambda)$$ . class EvidentialLoss(torch.nn.Module): def __init__(self, weight=None, size_average=True): super(EvidentialLoss, self).__init__() def forward(self, inputs, targets, smooth=1): targets = targets.view(-1) y = inputs[:,0].view(-1) #first column is mu,delta, predicted value loga = inputs[:,1].view(-1) #alpha logb = inputs[:,2].view(-1) #beta logl = inputs[:,3].view(-1) #lamda a = torch.exp(loga) b = torch.exp(logb) l = torch.exp(logl) term1 = (torch.exp(torch.lgamma(a - 0.5)))/(4 * torch.exp(torch.lgamma(a)) * l * torch.sqrt(b)) #print(&quot;term1 :&quot;, term1) term2 = 2 * b *(1 + l) + (2*a - 1)*l*(y - targets)**2 #print(&quot;term2 :&quot;, term2) J = term1 * term2 #print(&quot;J :&quot;, J) Kl_divergence = torch.abs(y - targets) * (2*a + l) #Kl_divergence = ((y - targets)**2) * (2*a + l) #print (&quot;KL &quot;,Kl_divergence.data.numpy()) loss = J + Kl_divergence #print (&quot;loss :&quot;, loss) return loss.mean() evnet = Net(n_feature=1, n_hidden=10, n_output=4) # define the network print(evnet) # net architecture optimizer = torch.optim.Adam(evnet.parameters(), lr=0.001) loss_func = EvidentialLoss() # train the network for t in range(20000): prediction = evnet(x) # input x and predict based on x loss = loss_func(prediction, y) # must be (1. nn output, 2. target) optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if t%10000 == 0: print(loss) prediction = evnet(test_x) mu = prediction[:,0].view(-1) #first column is mu,delta, predicted value a = prediction[:,1].view(-1) #alpha b = prediction[:,2].view(-1) #beta l = prediction[:,3].view(-1) #lamda a = torch.exp(a); b = torch.exp(b); l = torch.exp(l) var = b / ((a -1)*l) #epistemic/ model/prediciton uncertaitnty e = b / (a - 1) # aleatoric uncertainty/ data uncertainty plot_prediction(test_x, test_y, mu, var, e) . Net( (hidden): Linear(in_features=1, out_features=10, bias=True) (hidden_2): Linear(in_features=10, out_features=10, bias=True) (predict): Linear(in_features=10, out_features=4, bias=True) ) tensor(3.1019, grad_fn=&lt;MeanBackward0&gt;) tensor(0.1492, grad_fn=&lt;MeanBackward0&gt;) . Conclusions . Loss functions of maximum likelihood and evidence have been implemented from formulas from the paper. . Important: Leasons Learned - how to treat loss function when one of the output variable is always positive. (log and exponential to rescue) -The difference between optimize and loss function and do you sum and add constant ones or do you add constant in all equation and sum. | The trianing is working on some runs not all. | The model learns the function easily but learning the uncertainty in unkonw region takes longer number of iterations. Needs to be further investigated. . Note: ToDo : Test functions for the loss function written . Note: ToDo : What is stopping condition? when to stop learning? . Note: ToDo : Replace the regularizer in regularizer in likelihood loss with actual regularizer. . Warning: Why is uncertatiny different for both the models. Evidential model seems to be very confident in the known region, which seems to be fishy . Warning: The loss function doesnt give same results all the time, so needs to be further investigated | . References . [1] Kendall, Alex, and Yarin Gal. “What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?” ArXiv:1703.04977 [Cs], October 5, 2017. http://arxiv.org/abs/1703.04977. . [2] Amini, Alexander, Wilko Schwarting, Ava Soleimany, and Daniela Rus. “Deep Evidential Regression.” ArXiv:1910.02600 [Cs, Stat], October 7, 2019. http://arxiv.org/abs/1910.02600. . [3] Sensoy, Murat, Lance Kaplan, and Melih Kandemir. “Evidential Deep Learning to Quantify Classification Uncertainty,” n.d., 11. . [4] Malinin, Andrey, and Mark Gales. &quot;Predictive uncertainty estimation via prior networks.&quot; Advances in Neural Information Processing Systems. 2018. .",
            "url": "https://deebuls.github.io/devblog/probability/regression/uncertainty/2020/05/27/uncertainty-regression.html",
            "relUrl": "/probability/regression/uncertainty/2020/05/27/uncertainty-regression.html",
            "date": " • May 27, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Dataset shift",
            "content": "Dataset shift . Dataset shift is still an unsolved problems when it comes to deploying learning models “in the wild”. . There are 2 different categories of dataset shift . Co-variate shift | label shift | . Let $ mathbb{S} $ be the source data distribution and $ mathbb{T} $ be the target data distribution. If we denote the input variables as x and output variables as y, then . Covariate shift . s(x)≠t(x)s(x) neq t(x)s(x)=t(x) input distribution of both source and target are different . but . s(y∣x)=t(y∣x)s(y|x) = t(y|x)s(y∣x)=t(y∣x) conditional output distirbution is invariant to dataset shift. . Label shift . s(y)≠t(y)s(y) neq t(y)s(y)=t(y) output distribution of both source and target are different . but . s(x∣y)=t(x∣y)s(x|y) = t(x|y)s(x∣y)=t(x∣y) conditional input distirbution is invariant to dataset shift .   Covariate Shift Label Shift . input distribution | $s(x) neq t(x)$ | $?$ | . output distribution | $?$ | $s(y) neq t(y)$ | . conditional output distribution | $s(y vert x) = t(y vert x)$ | $?$ | . conditional input Distribution | $?$ | ${s(x vert y) = t(x vert y)}$ | . Examples . ToDo . Simluated Dataset {ReDo with examples} . The problem can be simulated in image based calssification dataset like MNIST and CIFAR. . Tweak-One shift . refers to the case where we set a class to have probability $ p &gt; 0.1$, while the distribution over the rest of the classes is uniform. . Minority-Class Shiftis . A more general version of Tweak-One shift, where a fixed number of classes to have probability $p &lt; 0.1$, while the distribution over the rest of the classes isuniform. . Dirichlet shift . we draw a probability vector $p$ from the Dirichlet distribution with concentration parameter set to $ alpha$ for all classes, before including sample points which correspond to the multinomial label variable according top. .",
            "url": "https://deebuls.github.io/devblog/dataset/2020/05/21/Explanation-Learning-DatasetShift.html",
            "relUrl": "/dataset/2020/05/21/Explanation-Learning-DatasetShift.html",
            "date": " • May 21, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Plotting Normal Inverse Gamma Distirbution",
            "content": ". Scipy stats doesnt have Normal Inverse Gamma distirbution. . We would like to incorporate Normal Inverse Gamma distirbution in &quot;scipy.stats&quot; package. . Learning about Normal Inverse Gamma(NIG) distribution will lead you to a plot like this from wikipedia. . . It was intruiging enough to find out how to plot this graph in python and was sure that there will be some already plots available. But to my suprise there is no blogs or docs to plot NIG in python. The closest I found was in R langugage in [1] by Frank Portman. . So I spent some time to plot NIG in python below is the snippet for it. Special thanks to Jake Vadendeplas[2] for his wonderful blogs about visualization in python. . Normal Inverse Gamma Distribution . Let the input $x$ on which its modelled be : $$ x = [ mu, sigma^2] $$ . Probability density function (PDF) . $$ f(x | delta, alpha, beta, lambda ) = sqrt{ left( frac{ lambda}{2 pi x[ sigma^2]} right)} frac{ beta^ alpha}{ Gamma( alpha)} left( frac{1}{x[ sigma^2]} right)^{( alpha + 1)} exp{ left( - frac{2 beta + lambda left(x[ mu] - delta right)^2 }{ 2 x[ sigma]^2} right)} $$ . from scipy.stats import rv_continuous from scipy.stats import norm from scipy.stats import gengamma from scipy.special import gamma from scipy.stats import expon import numpy as np %matplotlib inline import matplotlib.pyplot as plt plt.style.use(&#39;seaborn-white&#39;) class norminvgamma(): r&quot;&quot;&quot;A normal inverse gamma random variable. The mu (``mu``) keyword specifies the parmaeter mu. %(before_notes)s Notes -- The probability density function for `norminvgamma` is: .. math:: x = [ mu, sigma^2] f(x | delta, alpha, beta, lamda) = sqrt( frac{ lamda}{2 pi x[ sigma^2}]) frac{ beta^ alpha}{ gamma( alpha)} frac{1}{x[ sigma^2]}^( alpha + 1) exp(- frac{2 beta + lamda(x[ mu] - delta)^2}{2 x[ sigma^2] }) for a real number :math:`x` and for positive number :math: ` sigma^2` &gt; 0 %(after_notes)s %(example)s &quot;&quot;&quot; def __init__(self, delta, alpha, beta, lamda): self.argcheck(delta, alpha, beta, lamda) self.delta = delta self.alpha = alpha self.beta = beta self.lamda = lamda def argcheck(self, delta, alpha, beta, lamda): return (alpha &gt; 0) def rvs(self, size=1): sigma_2 = gengamma.rvs(self.alpha, self.beta, size=size) sigma_2 = np.array(sigma_2) return [[norm.rvs(self.delta, s/self.lamda), s] for s in sigma_2] def pdf(self, xmu, xsigma2): t1 = ((self.lamda)**0.5) * ((self.beta)**self.alpha) t2 = (xsigma2 * (2 * 3.15)**0.5) * gamma(self.alpha) t3 = (1 / xsigma2**2)**(self.alpha + 1) t4 = expon.pdf((2*self.beta + self.lamda*(self.delta-xmu)**2)/(2*xsigma2**2)) #print (t1, t2, t3, t4) return (t1/t2)*t3*t4 def stats(self): #ToDo return def plot(self,zoom=0.9, axs=None): steps = 50 max_sig_sq = gengamma.ppf(zoom, self.alpha, self.beta) * self.lamda #print(max_sig_sq) mu_range = np.linspace(self.delta - 1 * max_sig_sq, self.delta + 1 * max_sig_sq, num=steps) #print (mu_range[0], mu_range[-1]) sigma_range = np.linspace(0.01, max_sig_sq, num=steps) mu_grid, sigma_grid = np.meshgrid(mu_range, sigma_range) pdf_mesh = self.pdf(mu_grid, sigma_grid) if axs: contours = axs.contour(mu_grid, sigma_grid, pdf_mesh, 20, cmap=&#39;RdGy&#39;); plt.clabel(contours, inline=True, fontsize=8) #extent=[mu_range[0], mu_range[-1], sigma_range[0], sigma_range[-1]] axs.imshow(pdf_mesh, extent=[mu_range[0], mu_range[-1], sigma_range[0], sigma_range[-1]], origin=&#39;lower&#39;, cmap=&#39;Blues&#39;, alpha=0.5) axs.axis(&#39;equal&#39;) axs.set_title(&quot;(&quot;+str(self.delta)+&quot;,&quot;+str(self.alpha)+&quot;,&quot; +str(self.beta)+&quot;,&quot;+str(self.lamda)+&quot;)&quot;) #plt.colorbar(); else: assert True, &quot;Pass the axes to plot from matplotlib&quot; . Varying different range of $ alpha$ . #norminvgamma = norminvgamma_gen() fig, axs = plt.subplots(1, 3, sharey=True, figsize=(15,5)) fig.suptitle(&#39;Vertically alpha&#39;) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=1) samples = nig.rvs(size=10) nig.plot(axs=axs[0]) nig = norminvgamma(delta=0,alpha=2,beta=1, lamda=1) nig.plot(zoom=0.7, axs=axs[1]) nig = norminvgamma(delta=0,alpha=4,beta=1, lamda=1) nig.plot(zoom=0.2, axs=axs[2]) . Varying different range of $ beta$ . fig, axs = plt.subplots(1, 3, sharey=True, figsize=(15,5)) fig.suptitle(&#39;Varying beta&#39;) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=1) samples = nig.rvs(size=10) nig.plot(axs=axs[0]) nig = norminvgamma(delta=0,alpha=1,beta=2, lamda=1) nig.plot( axs=axs[1]) nig = norminvgamma(delta=0,alpha=1,beta=3, lamda=1) nig.plot(axs=axs[2]) . Varying different range of $ lambda$ . fig, axs = plt.subplots(1, 3, sharey=True, figsize=(15,5)) fig.suptitle(&#39;Verying lamda &#39;) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=1) samples = nig.rvs(size=10) nig.plot(axs=axs[0]) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=2) nig.plot( axs=axs[1]) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=4) nig.plot(axs=axs[2]) . References . https://frankportman.github.io/bayesAB/reference/plotNormalInvGamma.html | https://jakevdp.github.io/PythonDataScienceHandbook/04.04-density-and-contour-plots.html |",
            "url": "https://deebuls.github.io/devblog/probability/python/plotting/matplotlib/2020/05/19/probability-normalinversegamma.html",
            "relUrl": "/probability/python/plotting/matplotlib/2020/05/19/probability-normalinversegamma.html",
            "date": " • May 19, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Regression Uncertainty Robustness",
            "content": "Robustness . Robustness in learning is the capacity of the network to handle corrupted data (training data or test data). . Robustness during training vs testing . Training time robustness is capability of the model to overcome corrupted training data(input or labels). . Testing time robustness is the capability of the model to corrupted data during testing eg. Adversarial test. . Objective . The objective of this blog is to understand the robustness capability of different loss fucntions with respect to training time dataset corruption. . The loss functions in considerations are: . Gaussian negative log likelihood | Laplace negative log likelihood | Cauchy negative log likelihood | Data . Lets start with plotting our toy dataset. . As you can see the dataset has some corrupted labels which should not be considered while learning. . fig, ax = plt.subplots(figsize=(5,5)) ax.scatter(x.data.numpy(),y.data.numpy()) ax.axis(&#39;equal&#39;) ax.set_xlabel(&#39;$x$&#39;) ax.set_ylabel(&#39;$y$&#39;) ax.axis(&quot;equal&quot;) ax.annotate(&#39;noisy labels&#39;, xy=(0.25, 0.8), xytext=(0.0, 0.6), arrowprops=dict(facecolor=&#39;red&#39;, shrink=0.05)) ax.annotate(&#39;noisy labels&#39;, xy=(0.85, 0.1), xytext=(0.7, 0.3), arrowprops=dict(facecolor=&#39;red&#39;, shrink=0.05)) . Text(0.7, 0.3, &#39;noisy labels&#39;) . Model . We will train a simple linear model . # this is one way to define a network class Net(torch.nn.Module): def __init__(self, n_feature, n_hidden, n_output): super(Net, self).__init__() self.hidden = torch.nn.Linear(n_feature, n_hidden) # hidden layer self.predict = torch.nn.Linear(n_hidden, n_output) # output layer def forward(self, x): x = F.relu(self.hidden(x)) # activation function for hidden layer x = self.predict(x) # linear output return x . Mean Square Loss robustness analysis . loss_func = torch.nn.MSELoss() # this is for regression mean squared loss # Fit a linear regression using mean squared error. regression = Net(n_feature=1, n_hidden=1, n_output=1) # RegressionModel() params = regression.parameters() optimizer = torch.optim.Adam(params, lr = 0.001) . Analysis MSE Regression . Gaussian Loss Robustness analysis . For gaussian loss the model in addition to the prediction also predicts the confidence in the prediction in the form of the gaussian variance. . # this is one way to define a network class GaussianNet(torch.nn.Module): def __init__(self, n_feature, n_hidden, n_output): super(GaussianNet, self).__init__() self.hidden = torch.nn.Linear(n_feature, n_hidden) # hidden layer self.predict = torch.nn.Linear(n_hidden, n_output) # output layer self.variance = torch.nn.Linear(n_hidden, 1) # variance layer #torch.nn.init.xavier_uniform_(self.variance.weight) #torch.nn.init.normal_(self.variance.weight, mean=1.0) #torch.nn.init.normal_(self.variance.bias, mean=0.0) def forward(self, x): x = F.relu(self.hidden(x)) # activation function for hidden layer out = self.predict(x) # linear output var = F.softplus(self.variance(x)) return out, var . loss_func = torch.nn.GaussianNLLLoss( reduction=&#39;none&#39;) # Fit a linear regression using mean squared error. regression = GaussianNet(n_feature=1, n_hidden=2, n_output=1) # RegressionModel() params = regression.parameters() optimizer = torch.optim.Adam(params, lr = 0.001) . Smaller comparable network . Laplace loss function . def LaplaceNLLLoss(input, target, scale, eps=1e-06, reduction=&#39;mean&#39;): loss = torch.log(2*scale) + torch.abs(input - target)/scale # Inputs and targets much have same shape input = input.view(input.size(0), -1) target = target.view(target.size(0), -1) if input.size() != target.size(): raise ValueError(&quot;input and target must have same size&quot;) # Second dim of scale must match that of input or be equal to 1 scale = scale.view(input.size(0), -1) if scale.size(1) != input.size(1) and scale.size(1) != 1: raise ValueError(&quot;scale is of incorrect size&quot;) # Check validity of reduction mode if reduction != &#39;none&#39; and reduction != &#39;mean&#39; and reduction != &#39;sum&#39;: raise ValueError(reduction + &quot; is not valid&quot;) # Entries of var must be non-negative if torch.any(scale &lt; 0): raise ValueError(&quot;scale has negative entry/entries&quot;) # Clamp for stability scale = scale.clone() with torch.no_grad(): scale.clamp_(min=eps) # Calculate loss (without constant) loss = (torch.log(2*scale) + torch.abs(input - target) / scale).view(input.size(0), -1).sum(dim=1) # Apply reduction if reduction == &#39;mean&#39;: return loss.mean() elif reduction == &#39;sum&#39;: return loss.sum() else: return loss . loss_func = LaplaceNLLLoss # Fit a linear regression using mean squared error. regression = GaussianNet(n_feature=1, n_hidden=2, n_output=1) # RegressionModel() params = regression.parameters() optimizer = torch.optim.Adam(params, lr = 0.001) ##################### # Training #################### my_images = [] fig, (ax1, ax2) = plt.subplots(figsize=(20,7), nrows=1, ncols=2) # train the network for epoch in range(4000): prediction, scales = regression(x) loss_all = loss_func(prediction, y, scales, reduction=&#39;none&#39;) # must be (1. nn output, 2. target) loss = torch.mean(loss_all) #if t%10 == 0: print (loss) optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if np.mod(epoch, 100) == 0: sort_x, _ = torch.sort(x, dim=0) sort_prediction, sort_scales = regression(sort_x) print (loss) # plot and show learning process plt.cla() ax1.cla() ax1.set_title(&#39;Regression Analysis&#39;, fontsize=35) ax1.set_xlabel(&#39;Independent variable&#39;, fontsize=24) ax1.set_ylabel(&#39;Dependent variable&#39;, fontsize=24) ax1.set_xlim(-0.05, 1.0) ax1.set_ylim(-0.1, 1.0) ax1.scatter(x.data.numpy(), y.data.numpy(), color = &quot;orange&quot;) ax1.plot(sort_x.data.numpy(), sort_prediction.data.numpy(), &#39;g-&#39;, lw=3) dyfit = 2 * sort_scales.data.numpy() # 2*sigma ~ 95% confidence region ax1.fill_between( np.squeeze(sort_x.data.numpy()), np.squeeze(sort_prediction.data.numpy() - dyfit), np.squeeze(sort_prediction.data.numpy() + dyfit), color=&#39;gray&#39;, alpha=0.2) #l2_loss_plot_x = np.linspace(0,1,num=100) #y_plot_true = l2_loss_plot_x * scale_true + shift_true #ax1.plot(l2_loss_plot_x, y_plot_true, &#39;k&#39;) ax1.text(1.0, 0.1, &#39;Step = %d&#39; % epoch, fontdict={&#39;size&#39;: 24, &#39;color&#39;: &#39;red&#39;}) ax1.text(1.0, 0, &#39;Loss = %.4f&#39; % loss.data.numpy(), fontdict={&#39;size&#39;: 24, &#39;color&#39;: &#39;red&#39;}) diff = prediction.data.numpy() - y.data.numpy() ax2.cla() l2_loss_plot_x = np.linspace(-1,1,num=100) ax2.plot(l2_loss_plot_x, 0.5*l2_loss_plot_x**2, color=&quot;green&quot;, lw=3, alpha=0.5) ax2.scatter(diff, loss_all.data.numpy()) ax2.set_title(&#39;Loss &#39;, fontsize=35) ax2.set_xlabel(&#39;y - y_pred&#39;) ax2.set_ylim(-3.1, 3) ax2.set_xlim(-1, 1) # Used to return the plot as an image array # (https://ndres.me/post/matplotlib-animated-gifs-easily/) fig.canvas.draw() # draw the canvas, cache the renderer image = np.frombuffer(fig.canvas.tostring_rgb(), dtype=&#39;uint8&#39;) image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,)) my_images.append(image) . tensor(1.4615, grad_fn=&lt;MeanBackward0&gt;) tensor(0.8189, grad_fn=&lt;MeanBackward0&gt;) tensor(0.1342, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.2028, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.6679, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8267, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8590, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8677, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8730, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8773, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8809, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8837, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8857, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8873, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8879, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8891, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8894, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8898, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8901, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8902, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8901, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8901, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8903, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8901, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8902, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8901, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8903, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8904, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8902, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8900, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8903, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8903, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8902, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8905, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8903, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8903, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8905, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8900, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8904, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8905, grad_fn=&lt;MeanBackward0&gt;) . Analysis Laplace NLL Regression . The line is better fit to the data by avoiding to the noisy data. | . Cauchy NLL Loss function Robustness analysis . def CauchyNLLLoss(input, target, scale, eps=1e-06, reduction=&#39;mean&#39;): # Inputs and targets much have same shape input = input.view(input.size(0), -1) target = target.view(target.size(0), -1) if input.size() != target.size(): raise ValueError(&quot;input and target must have same size&quot;) # Second dim of scale must match that of input or be equal to 1 scale = scale.view(input.size(0), -1) if scale.size(1) != input.size(1) and scale.size(1) != 1: raise ValueError(&quot;scale is of incorrect size&quot;) # Check validity of reduction mode if reduction != &#39;none&#39; and reduction != &#39;mean&#39; and reduction != &#39;sum&#39;: raise ValueError(reduction + &quot; is not valid&quot;) # Entries of var must be non-negative if torch.any(scale &lt; 0): raise ValueError(&quot;scale has negative entry/entries&quot;) # Clamp for stability scale = scale.clone() with torch.no_grad(): scale.clamp_(min=eps) # Calculate loss (without constant) loss = (torch.log(3.14*scale) + torch.log(1 + ((input - target)**2)/scale**2)) .view(input.size(0), -1).sum(dim=1) # Apply reduction if reduction == &#39;mean&#39;: return loss.mean() elif reduction == &#39;sum&#39;: return loss.sum() else: return loss . loss_func = CauchyNLLLoss # Fit a linear regression using mean squared error. regression = GaussianNet(n_feature=1, n_hidden=2, n_output=1) # RegressionModel() params = regression.parameters() optimizer = torch.optim.Adam(params, lr = 0.001) ##################### # Training #################### my_images = [] fig, (ax1, ax2) = plt.subplots(figsize=(20,7), nrows=1, ncols=2) # train the network for epoch in range(4000): prediction, sigmas = regression(x) # input x and predict based on x loss_all = loss_func(prediction, y, sigmas, reduction=&#39;none&#39;) # must be (1. nn output, 2. target) loss = torch.mean(loss_all) #if t%10 == 0: print (loss) optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if np.mod(epoch, 100) == 0: #print (loss) sort_x, _ = torch.sort(x, dim=0) sort_prediction, sort_sigmas = regression(sort_x) # input x and predict based on x # plot and show learning process plt.cla() ax1.cla() ax1.set_title(&#39;Regression Analysis&#39;, fontsize=35) ax1.set_xlabel(&#39;Independent variable&#39;, fontsize=24) ax1.set_ylabel(&#39;Dependent variable&#39;, fontsize=24) ax1.set_xlim(-0.05, 1.0) ax1.set_ylim(-0.1, 1.0) ax1.scatter(x.data.numpy(), y.data.numpy(), color = &quot;orange&quot;) ax1.plot(sort_x.data.numpy(), sort_prediction.data.numpy(), &#39;g-&#39;, lw=3) dyfit = 2 * np.sqrt(sort_sigmas.data.numpy()) # 2*sigma ~ 95% confidence region ax1.fill_between( np.squeeze(sort_x.data.numpy()), np.squeeze(sort_prediction.data.numpy() - dyfit), np.squeeze(sort_prediction.data.numpy() + dyfit), color=&#39;gray&#39;, alpha=0.2) #l2_loss_plot_x = np.linspace(0,1,num=100) #y_plot_true = l2_loss_plot_x * scale_true + shift_true #ax1.plot(l2_loss_plot_x, y_plot_true, &#39;k&#39;) ax1.text(1.0, 0.1, &#39;Step = %d&#39; % epoch, fontdict={&#39;size&#39;: 24, &#39;color&#39;: &#39;red&#39;}) ax1.text(1.0, 0, &#39;Loss = %.4f&#39; % loss.data.numpy(), fontdict={&#39;size&#39;: 24, &#39;color&#39;: &#39;red&#39;}) diff = prediction.data.numpy() - y.data.numpy() ax2.cla() l2_loss_plot_x = np.linspace(-1,1,num=100) ax2.plot(l2_loss_plot_x, 0.5*l2_loss_plot_x**2, color=&quot;green&quot;, lw=3, alpha=0.5) ax2.scatter(diff, loss_all.data.numpy()) ax2.set_title(&#39;Loss &#39;, fontsize=35) ax2.set_xlabel(&#39;y - y_pred&#39;) ax2.set_ylim(-3.1, 3) ax2.set_xlim(-1, 1) # Used to return the plot as an image array # (https://ndres.me/post/matplotlib-animated-gifs-easily/) fig.canvas.draw() # draw the canvas, cache the renderer image = np.frombuffer(fig.canvas.tostring_rgb(), dtype=&#39;uint8&#39;) image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,)) my_images.append(image) . Analysis Cauchy NLL Regression . The line is better fit to the data than the gaussian by avoiding to the noisy data. | . Plotting Loss Surface . sigma_2 = np.logspace(0.1, 1, num=70, base=10 ) print (sigma_2.max(), sigma_2.min()) diff = np.linspace(-3, 5, num=70) def gauss_logL(xbar, sigma_2, mu): return -0.5*np.log(2*np.pi)-0.5*np.log(sigma_2)-0.5*(xbar -mu)**2/sigma_2 xbar = 1 logL = gauss_logL(xbar, sigma_2[:, np.newaxis], diff) logL -= logL.max() x_grid, sigma_grid = np.meshgrid(diff, sigma_2) logL = gauss_logL(xbar, sigma_grid, x_grid) logL = logL*-1 . 10.0 1.2589254117941673 . fig, ax = plt.subplots(figsize=(10,8),constrained_layout=True) ax = plt.axes(projection=&#39;3d&#39;) ax.plot_surface(x_grid, sigma_grid, logL, rstride=1, cstride=1, cmap=&#39;viridis&#39;, edgecolor=&#39;none&#39;) ax.set_title(&#39;NLL loss surface&#39;); . from itertools import cycle cycol = cycle(&#39;bgrcmk&#39;) fig, ax = plt.subplots(figsize=(10,7)) #for s_2 in [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 2]: for s_2 in [1e-3, 1e-2, 2e-3]: x = np.linspace(-3, 3, num=100) logL = (-1*gauss_logL(0, s_2, x))/1000 ax.plot(x, 0.5*x**2, color=&quot;cyan&quot;, lw=3, alpha=0.5) ax.plot(x,logL, c=next(cycol)) .",
            "url": "https://deebuls.github.io/devblog/probability/regression/uncertainty/robustness/2020/05/05/uncertainty-regression-robustness.html",
            "relUrl": "/probability/regression/uncertainty/robustness/2020/05/05/uncertainty-regression-robustness.html",
            "date": " • May 5, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Likelihood",
            "content": "What is Likelihood? . Likelihood and probablity seems to be same word in the layman domain, but in the stats domain they are different. . In the stats domain the likelihood or likelihood function is a measurement. It measures the distance between a statistical model and the input data. . What is a statistical model? . The diferent probability distributions available. For example, Gausian, gama, beta distribution, exponential for continuous data while Bernoulli, Dirichlet, multinomila distributions for discrete data. . How are statistical models represented? . By their parameters. For example for gaussian distribution the parameters are $ mu$ and $ sigma$ . . How do we select the statictical model? . Depends on many factors. This is the main decision to be made while designing a statistical model based learning. The different factors include: what is the data type: Continuous or discrete? Is it symmetrical or asymetrical? . Domain of the data, binary, real, etc | Does it decay or increase? | . . . etc | . A complete knowledge about the type data and the type of distribution is required to make the appropriate decision. . Good blog on likelihood with scipy.stats . https://www.kaggle.com/code/tentotheminus9/so-you-have-a-diagnostic-test-result/notebook | . Common Probability distribution . Data Type Domain Distribution Python (numpy.random) Parameters . univariate, discrete, binary | $$ x in {0,1 } $$ | Bernoulli | binomial(1, p) | $$ p in[0,1]$$ | . univariate, discrete, multivalued | $$ x in { 1,2, dots, K }$$ | multinomial | multinomial(n, pvals) | $$pvals = [p_1, dots , p_k] $$ $$ sum_{i=1}^{K} p_i = 1 $$ | . univariate, continuous, unbounded | $$ x in mathbb{R} $$ | normal | normal(mu, sigma) | $$ mu in mathbb{R} $$ $$ sigma in mathbb{R}$$ | . #Lets make some distributions and find the likelihood to some data import numpy as np import matplotlib.pyplot as plt number_of_samples = 20; #parameters ; sample data from distribution (continuous data) mu, sigma = 12, 0.1 ; univariate_gaussian_samples = np.random.normal(mu, sigma, number_of_samples) mean = [0, 0]; cov = [[1, 0], [0, 100]]; multivariate_gaussian_samples = np.random.multivariate_normal(mean, cov, number_of_samples) #parameters ; sample data from distribution (discreta data) p = 0.8 ; bernoulli_samples = np.random.binomial(1, p, number_of_samples) pvals = [0.2, 0.6, 0.2] ; multinomial_samples = np.random.multinomial(number_of_samples, pvals) alpha, beta = 10, 20 ; beta_samples = np.random.beta(alpha, beta, number_of_samples) alpha = [10,20,10,90] ; dirchilet_samples = np.random.dirichlet(alpha, number_of_samples) . Goal of Likelihood . The goal of likelihood would be given the samples as shown above (beta_samples, dirichlet_samples etc) find the parameters of the corresponding distribution ((alpha, beta), alphas respectively) . Lets look into this process in the comming post . #hide .",
            "url": "https://deebuls.github.io/devblog/probability/python/2020/03/20/probability-likelihood.html",
            "relUrl": "/probability/python/2020/03/20/probability-likelihood.html",
            "date": " • Mar 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I’m a researcher at the Bonn-Aachen International Center for Information Technology (b-it) at the Autonomous Systems group in Bonn, Germany. . My research focus is on developing strategies for safe and reliable incorporation of deep learning methods into robotics. . I am also the lead develper with the b-it-bots team which won the RoboCup WorldCup and GermanOpen in the work league. . Publications . b-it-bots: Our Approach for Autonomous Robotics in Industrial Environments . Deebul Nair, Santosh Thoduka, Iman Awaad, Sven Schneider, Paul G. Plöger, Gerhard K. Kraetzschmar and students . Robot World Cup. Springer, Cham, 2019. . This paper presents the approach of our team, b-it-bots, in the RoboCup@Work competition which resulted in us winning the World Championship in Sydney in 2019. We describe our current hardware, including modifications made to the KUKA youBot, the underlying software framework and components developed for navigation, manipulation, perception and task planning for scenarios in industrial environments. Our combined 2D and 3D approach for object recognition has improved robustness and performance compared to previous years, and our task planning framework has moved us away from large state machines for high-level control. Future work includes closing the perception-manipulation loop for more robust grasping. Our open-source repository is available at https://github.com/b-it-bots/mas_industrial_robotics. . Performance Evaluation of Low-Cost Machine Vision Cameras forImage-Based Grasp Verification . Performance Evaluation of Low-Cost Machine Vision Cameras forImage-Based Grasp Verification . Under review in IROS 2020 . Grasp verification is advantageous for au-tonomous manipulation robots as they provide the feedbackrequired for higher level planning components about successfultask completion. However, a major obstacle in doing graspverification is sensor selection. In this paper, we propose a visionbased grasp verification system using machine vision cameras,with the verification problem formulated as an image classifi-cation task. Machine vision cameras consist of a camera anda processing unit capable of on-board deep learning inference.The inference in these low-power hardware are done near thedata source, reducing the robots dependence on a centralizedserver, leading to reduced latency, and improved reliability.Machine vision cameras provide the deep learning inferencecapabilities using different neural accelerators. Although, it isnot clear from the documentation of these cameras what is theeffect of these neural accelerators on performance metrics suchas latency and throughput. To systematically benchmark thesemachine vision cameras, we propose a parameterized modelgenerator that generates end to end models of ConvolutionalNeural Networks(CNN). Using these generated models webenchmark latency and throughput of two machine visioncameras, JeVois A33 and Sipeed Maix Bit. Our experimentsdemonstrate that the selected machine vision camera and thedeep learning models can robustly verify grasp with 97% perframe accuracy. . Open Source Contributions . BayesPy – Bayesian Python . BayesPy provides tools for Bayesian inference with Python. The user constructs a model as a Bayesian network, observes data and runs posterior inference. The goal is to provide a tool which is efficient, flexible and extendable enough for expert use but also accessible for more casual users. . b-it-bots . ROS software packages of b-it-bots for different robots. . . This website is powered by fastpages. Logo and favicon was designed by Darius Dan .",
          "url": "https://deebuls.github.io/devblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://deebuls.github.io/devblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}