{
  
    
        "post0": {
            "title": "Dis-entaglement of Epistemic and Aleatoric uncertainty for Dirichlet Distribution",
            "content": ". How to separate epistemic and aleatoric uncertaity of Dirichlet distirbution . Also studing the implications of it and proposing the applications of the solutions. . Formula for [1] . | theory in [2] | . ToDo : complete the section with info . [1] Separation of Aleatoric and Epistemic Uncertainty in Deterministic Deep Neural Networks Denis Huseljic, Bernhard Sick, Marek Herde, Daniel Kottke . [2] Deep Deterministic Uncertainty: A Simple Baseline Jishnu Mukhoti . import math import numpy as np import torch . prior = 1 n_classes = 5 def predict_epistemic( alpha): &quot;&quot;&quot;Predicts the uncertainty of a sample. (K / alpha_0)&quot;&quot;&quot; return n_classes * prior / alpha.sum(-1, keepdim=True) def predict_aleatoric( alpha): &quot;&quot;&quot;Predicts the uncertainty of a sample. (K / alpha_0)&quot;&quot;&quot; proba_in = (alpha / alpha.sum(-1, keepdim=True)).clamp_(1e-8, 1-1e-8) entropy = - torch.sum((proba_in * proba_in.log()), dim=-1) normalized_entropy = entropy / np.log(n_classes) return normalized_entropy . ones = torch.ones(n_classes) print (predict_epistemic(ones), predict_aleatoric(ones)) . tensor([1.]) tensor(1.) . When alpha of only a single class keeps increasing . Observation : Both uncertainty reduces | Impact : When the model puts all confidence(alpha) on a single class it shows that the model is confident about the class and uncertainty reduces. . | The maximum aleatoric and epistemic uncertitny is both 1.0 . | Epistemic is always lower than Aleatoric | . for i in [1, 10, 50, 1000 ]: x = torch.ones(n_classes) x[0] = i print (x) print (&quot;Epistemic UE : {}, Aleatoric UE : {}&quot;.format(predict_epistemic(x), predict_aleatoric(x))) print (&quot;&quot;,predict_epistemic(x) &gt; predict_aleatoric(x)) . tensor([1., 1., 1., 1., 1.]) Epistemic UE : tensor([1.]), Aleatoric UE : 1.0 tensor([False]) tensor([10., 1., 1., 1., 1.]) Epistemic UE : tensor([0.3571]), Aleatoric UE : 0.6178266406059265 tensor([False]) tensor([50., 1., 1., 1., 1.]) Epistemic UE : tensor([0.0926]), Aleatoric UE : 0.2278686910867691 tensor([False]) tensor([1000., 1., 1., 1., 1.]) Epistemic UE : tensor([0.0050]), Aleatoric UE : 0.019580082967877388 tensor([False]) . When alpha of multiple classes keeps increasing . Observation : Epistemic reduces aleatoric is high | Impact : When the model puts all confidence(alpha) on multiple classes basically suggests that the model is not confident. While since some alpha has increased it suggests that the input is an observed data(not new) and therefore low aleatoric uncertainty | . The maximum aleatoric and epistemic uncertainty is both 1 . for i in [1, 10, 50, 10000 ]: x = torch.ones(n_classes)*i print (x) print (&quot;Epistemic UE : {}, Aleatoric UE : {}&quot;.format(predict_epistemic(x), predict_aleatoric(x))) print (&quot;&quot;,) . tensor([1., 1., 1., 1., 1.]) Epistemic UE : tensor([1.]), Aleatoric UE : 1.0 tensor([10., 10., 10., 10., 10.]) Epistemic UE : tensor([0.1000]), Aleatoric UE : 1.0 tensor([50., 50., 50., 50., 50.]) Epistemic UE : tensor([0.0200]), Aleatoric UE : 1.0 tensor([10000., 10000., 10000., 10000., 10000.]) Epistemic UE : tensor([1.0000e-04]), Aleatoric UE : 1.0 . Impact of prior . prior = 50 . The highest epistmeic uncertainty increases from 1 to the prior value . Conclusions . Dirichlet distirbution can be dis-entagled into aleatoric and epistemic uncertainty. | When all alpha is 1 - both uncertainty are also 1 impling that the network doesnt know anything | If only one output class alpha is higher then both uncertainty is low | The higher the alpha the lower both the uncertainty | If multiple alpha is higher then only aleatoric is high epistemic stays low. Impling that since the some alpha was increased the network has seen the input and its not sure which amongst the outputs is correct. | . Use Case . 1. For identifying OOD data . For the training dataset measure the epistemic uncertainty of the correct predictions. It should be less than 1 and near to zero | During prediction if epistemic uncertainty is higher than the training max then that data should be considered OOD and handled appropriately | 2. For handling in-domain uncertain data . If the epistemic unertainty is is range but if the aleatoric is high we can use these in embodied situation to collect additional data(image) from different view, fuse and make decision. Example if blur image - then differ to predict but dont flag as OOD, maybe in next image the information will be clear. |",
            "url": "https://deebuls.github.io/devblog/statistics/uncertainty/2022/12/19/dirichlet-disentanglement-epistemic-aleatoric-evidential.html",
            "relUrl": "/statistics/uncertainty/2022/12/19/dirichlet-disentanglement-epistemic-aleatoric-evidential.html",
            "date": " • Dec 19, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Comparing Distributions : Normal, Laplace and Cauchy",
            "content": ". Comparing Distributions : Normal, Laplace and Cauchy . We want to represent a fact of a real number $x$ and its uncertainty. Probability distirbution is one of the representation to hold such information. . For example, we want to store a fact that the value of a variable is 10 with uncertaitny of $+- 0.5$ . In other format the value lies between $[9.5, 10.5]$ . This can be represented using different probability distributions. . Normal distirbution - N(10, 0.3) | Laplace distirbution - L(10, 0.2) | Cauchy distribution - C(10, 0.1) | The question we are interested is if a particular fact is represented using these distribution. How to compare the uncertainty ? . For example: the above distirbutions how all of them have peak at 10 but how to compare are these distirbutions having the same spread. . Such situations come in machine learning or deep learning situation where you have the output of the AI model predicting a distribution (laplace or cauchy) and you have the true value and you want to compare which distribution is giving a better result. . Lets visually check how the distirbution looks. . ## hide import numpy as np import scipy.stats as stats import matplotlib.pyplot as plt . normal_dist = stats.norm(loc=10, scale=0.3) laplace_dist = stats.laplace(loc=10, scale=0.2 ) cauchy_dist = stats.laplace(loc=10, scale=0.1) x = np.linspace(8, 12, num=1000) fig, ax = plt.subplots( 1, 1, figsize=(10,8)) ax.plot(x, normal_dist.pdf(x), label=&#39;Normal Distirbution (10, 0.3)&#39;, c=&#39;r&#39;) ax.plot(x, laplace_dist.pdf(x), label=&#39;laplace Distirbution (10, 0.2)&#39;, c=&#39;g&#39;) ax.plot(x, cauchy_dist.pdf(x), label=&#39;cacuhy Distirbution (10, 0.1)&#39;, c=&#39;b&#39;) ax.legend() . &lt;matplotlib.legend.Legend at 0x7f3ea0bd10a0&gt; . Questions . How to compare these 3 distirbution? | More importantly if a particular distirbution is representing an uncertain information, then how can we measure which value is giving the best representation of the uncertainty. | Solution . For comparing uncertainty different methods has been mentioned in literature. The one which we are going to use here is Interval Score, which comes under the field of Proper Scoring Rules [1]. . Proper Scoring rules . Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand [1] . [1] Gneiting, Tilmann, and Adrian E. Raftery. &quot;Strictly proper scoring rules, prediction, and estimation.&quot; Journal of the American statistical Association 102.477 (2007):359-378. . Scoring Rules for Quantile and Interval Forecasts . Interval Score . classical case of $(1 - alpha) times 100 % $ prediction interval | with lower and upper endpoints (predictive quatiles) at level $ alpha/2$ and $ 1 - alpha/2$ | Interval score $$ S_ alpha^{int}(l, u ;x) = (u-l) + frac{2}{ alpha}(l-x) mathbb{1} {x &lt; l } + frac{2}{ alpha} (x - u) mathbb{1} {x &gt; u }$$ | . $ alpha_1= 0.02, alpha_2= 0.05, alpha_3=0.1$ (implying nominal coverages of 98%,95%,90%) . def interval_score(x, lower, upper, alpha=0.05): assert np.all(upper&gt;=lower), &quot;Upper should be greater or equal to lower. Please check are you giving the upper and lower in propoer order &quot; return (upper - lower) + (2/alpha)*(lower-x)*(x&lt;lower) + (2/alpha)*(x-upper)*(x&gt;upper) . x = np.linspace(3.0, 10.0) l = 5.0 u = 8.0 iscore = interval_score(x, l, u) plt.plot(x, iscore, c=&#39;b&#39;) l = 6.0 u = 7.0 iscore = interval_score(x, l, u) plt.plot(x, iscore, c=&#39;r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3ea0b6dbe0&gt;] . The way to interpret the method with respect to state-estimation algorithm can be. Let $x$ be the true_value and the prediction is the range $[5,8]$ then the intervalscore is minimum if the true value was also inbetween $[5,8]$ as shown in the blue plot. . With the red plot you can see if you give a tigher bound then the score is further low. . So if the true value is inside the lower and uper limits the score is minimum also with tighter bound you get better scores . Using Interval Score for Distirbutions . We need to define the intervals . . So how do we calculate the intervals of different distirbutions ?? . sympy to help, it has the . .interval(alpha) function . Lets calcuate the interval function of the above distirbutions . print (&quot; Normal Distirbution 95% interval &quot;, normal_dist.interval(alpha=0.95)) print (&quot; Laplace Distirbution 95% interval&quot;, laplace_dist.interval(alpha=0.95)) print (&quot; Cauchy Distirbution 95% interval&quot;, cauchy_dist.interval(alpha=0.95)) . Normal Distirbution 95% interval (9.412010804637983, 10.587989195362017) Laplace Distirbution 95% interval (9.400853545289202, 10.599146454710798) Cauchy Distirbution 95% interval (9.700426772644601, 10.299573227355399) . Formula . So now we have the lower and upper limit for each distirbution . Lets now calculate the Interval Score. . l, u = normal_dist.interval(alpha=0.95) print (&quot;Normal Distirbution :&quot;) print (&quot;95% interval &quot;, normal_dist.interval(alpha=0.95)) print (&quot;Interval Score &quot;, interval_score(10 , l, u)) print (&quot;############### &quot;) print (&quot;Laplace Distirbution&quot;) l, u = laplace_dist.interval(alpha=0.95) print (&quot;95% interval &quot;, laplace_dist.interval(alpha=0.95)) print (&quot;Interval Score &quot;, interval_score(10 , l, u)) print (&quot;############### &quot;) print (&quot;Cauchy Distirbution 95%&quot;) l, u = cauchy_dist.interval(alpha=0.95) print (&quot;95% interval &quot;, cauchy_dist.interval(alpha=0.95)) print (&quot;Interval Score &quot;,interval_score(10 , l, u)) . Normal Distirbution : 95% interval (9.412010804637983, 10.587989195362017) Interval Score 1.1759783907240333 ############### Laplace Distirbution 95% interval (9.400853545289202, 10.599146454710798) Interval Score 1.1982929094215962 ############### Cauchy Distirbution 95% 95% interval (9.700426772644601, 10.299573227355399) Interval Score 0.5991464547107981 . Here you can see for True value $x = 10$, Cauchy Distirbution (loc=10, scale=0.1) gives the minimum Interval Score of 0.6 . Thus, with Interval Score a Proper scoring rule we can compare different distirbutions. .",
            "url": "https://deebuls.github.io/devblog/statistics/uncertainty/2022/12/15/comparing-Normal-Laplace-Cacuhy-Distirbutions-Interval-score.html",
            "relUrl": "/statistics/uncertainty/2022/12/15/comparing-Normal-Laplace-Cacuhy-Distirbutions-Interval-score.html",
            "date": " • Dec 15, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Running ros2 humble in ubuntu lxd container",
            "content": "ROS2 in Lxd container . Its always dificult to get the appropriate ubuntu version for a running a particular software. For example, currently I have ubuntu 20.04 and I want to test a software in ros2 humble. Humble requires ubuntu version 22.04. I wanted to try if we can use lxd containers for running the appropriate ubuntu version and installing ros in those. . LXD and ROS2 installation . Started with blog by ubuntu | It gets you through the installation . I created a ubuntu 22.04 container and installed ros2 inside it . | I create a user called ubuntu | for ros installation I followed ros2 documentation | . | For using ros started ros2 tutorials | First problem gui not working Blog by Nick D Greg introduced the topic of using profiles | Following the blog I created a lxc profile named gui | The blog assumes you start from begining but since we already had the container running we used the below command | lxc profile assign ros-humble default,gui . | inside the container ‘export DISPLAY=:0’ in the bash (also added in bashrc) | now gui is running | $&gt; ros2 run turtlesim turtlesim_node | . | Comman Commands to get started with lxc development . Boot your local ubuntu and first you want to check the active containers the command is lxc list $&gt; lxc list +++-+--+--+--+ | NAME | STATE | IPV4 | IPV6 | TYPE | SNAPSHOTS | +++-+--+--+--+ | ros-humble | RUNNING | 10.171.226.72 (eth0) | fd42:13f7:78ed:7795:216:3eff:fe9c:bde9 (eth0) | CONTAINER | 0 | +++-+--+--+--+ . | Getting bash access $ lxc exec ros-humble -- su --login ubuntu ubuntu@ubuntu-container:~$ . | Appendix . History of comands used insde the container, after logging . 2 apt-cache policy | grep universe 3 sudo apt install software-properties-common 4 sudo add-apt-repository universe 5 sudo apt update &amp;&amp; sudo apt install curl gnupg lsb-release 6 sudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg 7 echo &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(source /etc/os-release &amp;&amp; echo $UBUNTU_CODENAME) main&quot; | sudo tee /etc/apt/sources.list.d/ros2.list &gt; /dev/null 8 sudo apt update &amp;&amp; sudo apt install -y build-essential cmake git python3-colcon-common-extensions python3-flake8 python3-flake8-blind-except python3-flake8-builtins python3-flake8-class-newline python3-flake8-comprehensions python3-flake8-deprecated python3-flake8-docstrings python3-flake8-import-order python3-flake8-quotes python3-pip python3-pytest python3-pytest-cov python3-pytest-repeat python3-pytest-rerunfailures python3-rosdep python3-setuptools python3-vcstool wget 9 sudo apt update 10 sudo apt install ros-humble-desktop .",
            "url": "https://deebuls.github.io/devblog/robotics/2022/10/20/ROS2-LXD-Container.html",
            "relUrl": "/robotics/2022/10/20/ROS2-LXD-Container.html",
            "date": " • Oct 20, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Compile Latex file using Github Action",
            "content": "Compiling Latex documents using Github Action . GitHub Actions provides full access to the runner at your disposal, and one thing you may want to do is make commits in a workflow run and push it back up to GitHub automatically. I’m going to show a simple example where we run the date unix command, save the contents to a file, and push it back to the master branch. Example workflow . Using Makefile . Use make file to automate all the actions which needs to be done after checking out the repo. | Here is a Makefile for compiling latex file Makefile filename=main | . pdf: mkdir -p build pdflatex –output-directory build ${filename} bibtex build/${filename}||true pdflatex –output-directory build ${filename} pdflatex –output-directory build ${filename} mv build/${filename}.pdf . . read: evince build/${filename}.pdf &amp; . clean: rm -f build/${filename}.{ps,pdf,log,aux,out,dvi,bbl,blg} . ## Installing latex commands in Github Action Inorder to use **pdflatex** in the ubuntu machine generted for compilation first you need to install the necessary packages in the ubuntu machine . ## Using git commit in GitHub Actions The following is a workflow which on push will do the following: checkout the repo install latex dependency run make command and compile setup git config commit the changed file and push it back to master yml name: Makefile CI on: push: branches: [ &quot;master&quot; ] pull_request: branches: [ &quot;master&quot; ] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Install dependencies run: sudo apt-get install texlive-latex-base texlive-fonts-recommended texlive-fonts-extra texlive-latex-extra - name: Compile run: make - name: setup git config run: | # setup the username and email. I tend to use &#39;GitHub Actions Bot&#39; with no email by default git config user.name &quot;GitHub Actions Bot&quot; git config user.email &quot;&lt;&gt;&quot; - name: commit run: | # Stage the file, commit and push git add main.pdf git commit -m &quot;new main pdf commit&quot; git push origin master . [1] https://lannonbr.com/blog/2019-12-09-git-commit-in-actions/ [2] Example github page .",
            "url": "https://deebuls.github.io/devblog/latex/2022/06/25/Github-Action-Latex-Compilation.html",
            "relUrl": "/latex/2022/06/25/Github-Action-Latex-Compilation.html",
            "date": " • Jun 25, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Proper Scoring Rules: Interval Score and CRPS",
            "content": ". Note we also compare Gaussian distribution and Laplace distirbution to empiracally find the $95%$ region with respect to their scale. . We found out 95% Confidence interval is $Gaussian ( 2 sigma) == Laplace(3b)$ . Scoring Rules for Quantile and Interval Forecasts . Interval Score . classical case of $(1 - alpha) times 100 % $ prediction interval | with lower and upper endpoints (predictive quatiles) at level $ alpha/2$ and $ 1 - alpha/2$ | Interval score $$ S_ alpha^{int}(l, u ;x) = (u-l) + frac{2}{ alpha}(l-x) mathbb{1} {x &lt; l } + frac{2}{ alpha} (x - u) mathbb{1} {x &gt; u }$$ | . $ alpha_1= 0.02, alpha_2= 0.05, alpha_3=0.1$ (implying nominal coverages of 98%,95%,90%) . import numpy as np . def interval_score(x, lower, upper, alpha=0.05): assert np.all(upper&gt;=lower), &quot;Upper should be greater or equal to lower. Please check are you giving the upper and lower in propoer order &quot; return (upper - lower) + (2/alpha)*(lower-x)*(x&lt;lower) + (2/alpha)*(x-upper)*(x&gt;upper) . x = np.linspace(1.0, 12.0) l = 5.0 u = 8.0 iscore = interval_score(x, l, u) . import matplotlib.pyplot as plt . plt.plot(iscore) . [&lt;matplotlib.lines.Line2D at 0x7f3ecba1b450&gt;] . x = np.linspace(1.0, 12.0) l = 5.0*np.ones_like(x) u = 8.0*np.ones_like(x) iscore = interval_score(x, l, u) plt.plot(iscore) . [&lt;matplotlib.lines.Line2D at 0x7f3ecb504910&gt;] . CRPS . import scipy.stats as stats def crps_gaussian(y_pred, y_std, y_true, scaled=True): &quot;&quot;&quot; Return the negatively oriented continuous ranked probability score for held out data (y_true) given predictive uncertainty with mean (y_pred) and standard-deviation (y_std). Each test point is given equal weight in the overall score over the test set. Negatively oriented means a smaller value is more desirable. &quot;&quot;&quot; # Flatten num_pts = y_true.shape[0] y_pred = y_pred.reshape( num_pts, ) y_std = y_std.reshape( num_pts, ) y_true = y_true.reshape( num_pts, ) # Compute crps y_standardized = (y_true - y_pred) / y_std term_1 = 1 / np.sqrt(np.pi) term_2 = 2 * stats.norm.pdf(y_standardized, loc=0, scale=1) term_3 = y_standardized * (2 * stats.norm.cdf(y_standardized, loc=0, scale=1) - 1) crps_list = -1 * y_std * (term_1 - term_2 - term_3) crps = np.sum(crps_list) # Potentially scale so that sum becomes mean if scaled: crps = crps / len(crps_list) return crps . data = np.random.normal(loc=1.0, scale=1.0, size=1000) mus = np.ones_like(data) sigmas = np.ones_like(data) crps_gaussian(data, sigmas, mus, scaled=False) . 587.2681356774692 . PLot Gaussian and Laplace with same interval . For the laplace distribution . $f(med) = frac{1}{ sigma sqrt{2}} $ and therefore $D(x_{0.5}) = frac{ sigma^2}{2n}$ confidence interval of the median . $$ x - frac{ u_{1 - alpha/2} (n-1)0.707 s}{ sqrt{n}} leq median leq x + frac{u_{1 - alpha/2} (n-1)0.707 s}{ sqrt{n}}$$ . Example : . laplace disitrbution L(0, 2) : variance $s^2 = 2b^2 = 2.246$ Substitutint in above equation median = 0.0119 estimated b = 1.0596 . mu = 0.0119 b = 1.0596 n = 50 var = 2*b**2; print (&quot;Variance : &quot;, var) confidence_95 = (stats.norm.ppf(1 - 0.05/2) * 0.707 * var**0.5) print (mu-confidence_95 , mu+confidence_95) . Variance : 2.24550432 -2.0645642208852193 2.088364220885219 . stats.norm.ppf(1 - 0.05/2) * 0.707 * var . 3.111583069190677 . for i in np.linspace(0,0.99,num=10): print (i,stats.norm.ppf(i), stats.laplace.ppf(i)) . 0.0 -inf -inf 0.11 -1.2265281200366098 -1.5141277326297755 0.22 -0.7721932141886848 -0.8209805520698302 0.33 -0.4399131656732338 -0.4155154439616658 0.44 -0.15096921549677725 -0.12783337150988489 0.55 0.12566134685507416 0.1053605156578264 0.66 0.41246312944140495 0.3856624808119848 0.77 0.7388468491852137 0.7765287894989964 0.88 1.1749867920660904 1.4271163556401458 0.99 2.3263478740408408 3.912023005428145 . (0.29 * n**0.5)/(0.707* var**0.5 * 49) . 0.039501224521614836 . stats.norm.ppf(1 - 0.05/2) . 1.959963984540054 . import matplotlib.pyplot as plt import numpy as np import scipy.stats as stats import math mu = 0 variance = 1 sigma = math.sqrt(variance) x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100) plt.plot(x, stats.norm.pdf(x, mu, sigma)) plt.plot(x, stats.laplace.pdf(x, mu,sigma), c=&#39;r&#39;) plt.show() . Gaussian vs Laplace . 95% Confidence interval is $Gaussian ( 2 sigma) == Laplace(3b)$ . r = stats.norm.rvs(mu, sigma, size=1000) v = ((mu - 2*sigma) &lt; r ) &amp; (r&lt; (mu + 2*sigma)) np.sum(v)/1000 . 0.953 . r = stats.laplace.rvs(mu, sigma, size=1000) v = ((mu - 3*sigma) &lt; r ) &amp; (r&lt; (mu + 3*sigma)) np.sum(v)/1000 . 0.952 . for mu in np.linspace(0, 255, num=10): for sigma in np.linspace(0, 5, num=10): r = stats.laplace.rvs(mu, sigma, size=1000) v = ((mu - 3*sigma) &lt; r ) &amp; (r&lt; (mu + 3*sigma)) print (np.sum(v)/1000) . 0.0 0.943 0.962 0.959 0.962 0.947 0.941 0.953 0.949 0.941 0.0 0.962 0.956 0.955 0.95 0.959 0.952 0.943 0.95 0.939 0.0 0.947 0.951 0.943 0.954 0.941 0.961 0.951 0.949 0.963 0.0 0.961 0.955 0.956 0.951 0.939 0.941 0.95 0.949 0.956 0.0 0.943 0.941 0.956 0.948 0.949 0.955 0.942 0.953 0.948 0.0 0.945 0.949 0.957 0.951 0.953 0.954 0.949 0.955 0.954 0.0 0.957 0.945 0.945 0.935 0.944 0.945 0.961 0.95 0.95 0.0 0.951 0.942 0.954 0.953 0.953 0.954 0.941 0.947 0.955 0.0 0.942 0.95 0.956 0.939 0.939 0.952 0.951 0.95 0.951 0.0 0.95 0.94 0.968 0.95 0.949 0.958 0.939 0.947 0.945 . stats.laplace.ppf(1 - 0.05/2) . 2.99573227355399 . x = np.array([True, False]) y = np.array([True , True]) x &amp; y . array([ True, False]) . mu = 0 variance = 1 sigma = math.sqrt(variance) b = math.sqrt(var/2) . mu + 2*sigma . 2.0 . mu + 3*0.67 . 2.0100000000000002 . b = 0.67 laplace_variance = 2*b**2 print (laplace_variance) . 0.8978000000000002 . mu = 0 variance = 1 sigma = math.sqrt(variance) x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100) plt.plot(x, stats.norm.pdf(x, mu, sigma)) plt.plot(x, stats.laplace.pdf(x, mu,b), c=&#39;r&#39;) plt.show() . stats.laplace.ppf(0.975) . 2.99573227355399 . stats.laplace.ppf(0.025) . -2.995732273553991 .",
            "url": "https://deebuls.github.io/devblog/statistics/uncertainty/2022/02/05/proper-scoring-interval-score-crps.html",
            "relUrl": "/statistics/uncertainty/2022/02/05/proper-scoring-interval-score-crps.html",
            "date": " • Feb 5, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Intel Realsense Camera + Kinova Gen3 + Gazebo",
            "content": "Dataset shift . Packages installed . * * . realsense description for the urdf and the xacro files | sudo apt install ros-noetic-realsense2-camera -&gt; maybe not required | sudo apt install ros-noetic-realsense2-description -&gt; this is required | . | . Links . [1] https://answers.gazebosim.org//question/24989/adding-intel-camera-to-robot-end-effector/ [2] https://github.com/IntelRealSense/realsense-ros [3] https://github.com/pal-robotics-forks/realsense/blob/upstream/realsense2_description/urdf/_d435.urdf.xacro . https://answers.ros.org/question/348331/realsense-d435-gazebo-plugin/ . https://github.com/pal-robotics/realsense_gazebo_plugin/issues/7#issuecomment-609040406 .",
            "url": "https://deebuls.github.io/devblog/robotics/2022/01/19/Add-Realsense-Camera-Kinova-Gen3-Gazebo.html",
            "relUrl": "/robotics/2022/01/19/Add-Realsense-Camera-Kinova-Gen3-Gazebo.html",
            "date": " • Jan 19, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Flexible Distributions as an Approach to Robustness",
            "content": "Understanding paper &quot;Flexible Distributions as an Approach to Robustness : The Skew-t Case&quot; by Adelchi Azzalini . Introduction . When a continuous variable of interest spans the whole real line, an interesting distribution is the one with density function . $$ c_v exp(- frac{|x|^{v}}{v}), qquad x in mathbb{R} $$ . where $ V &gt; 0 $ and . $$ C_v = frac{1}{ 2 v^{1/v} Gamma(1 + frac{1}{v})}$$ . Here the parameter $v$ manoeuvres the tail weight in the sense that . $v$ = 2 corresponds to the normal distribution, | $0 &lt; v &lt; 2$ produces tails heavier than the normal ones, | $ v &gt; 2 $ produces lighter tails. | import sympy as sym print (sym.__version__) . 1.8 . v = sym.Symbol(&#39;v&#39;) x = sym.Symbol(&#39;x&#39;) gamma_v = sym.gamma(v + 1/v) c_v = 1 / (2 * v**(1/v) * gamma_v) skew_t = c_v * sym.exp(-sym.Abs(x)**v/ v) . p1 = sym.plot(skew_t.subs(v, 2), label=&quot;Gaussian&quot;, line_color=&#39;blue&#39;, show=False, legend=True) p2 = sym.plot(skew_t.subs(v, 1), label=&quot;Heavy Tail&quot;, line_color=&#39;red&#39;, show=False) p3 = sym.plot(skew_t.subs(v, 3), label=&quot;Thin Tail&quot;, line_color=&#39;cyan&#39;, show=False) p4 = sym.plot(skew_t.subs(v, 0.5), label=&quot;Heavy Tail&quot;, line_color=&#39;black&#39;, show=False) p1.append(p2[0]) p1.append(p3[0]) p1.append(p4[0]) p1.show() . p1 = sym.plot(-sym.log(skew_t.subs(v, 2)), (x, -1, 1), label=&quot;Gaussian&quot;, line_color=&#39;blue&#39;, show=False, legend=True) p2 = sym.plot(-sym.log(skew_t.subs(v, 1)), (x, -1, 1), label=&quot;Heavy Tail&quot;, line_color=&#39;red&#39;, show=False, legend=True) p3 = sym.plot(-sym.log(skew_t.subs(v, 3)), (x, -1, 1), label=&quot;Thin Tail&quot;, line_color=&#39;cyan&#39;, show=False, legend=True) p4 = sym.plot(-sym.log(skew_t.subs(v, 0.6)), (x, -1, 1), label=&quot;Heavy Tail&quot;, line_color=&#39;black&#39;, show=False) p1.append(p2[0]) p1.append(p3[0]) p1.append(p4[0]) p1.show() . Notes . [Analyzing distribution with Sympy] (https://brianzhang01.github.io/2018/04/distributions-with-sympy/) . sym.init_printing() x, t = sym.symbols(&#39;x t&#39;, real=True) def area(dist): return sym.simplify(sym.integrate(dist, (x, -sym.oo, sym.oo))) def mean(dist): return area(dist*x) def EX2(dist): return area(dist*x**2) def variance(dist): return sym.simplify(EX2(dist) - mean(dist)**2) def mgf(dist): return sym.simplify(area(dist*sym.exp(x*t))) def latex(result): return &quot;$&quot; + sym.latex(result) + &quot;$ n&quot; def summarize(dist): #print (&quot;Distribution: &quot; + latex(dist)) (dist) print (&quot;Area: &quot; + latex(area(dist))) print (&quot;Mean: &quot; + latex(mean(dist))) print (&quot;Variance: &quot; + latex(variance(dist))) print (&quot;MGF: &quot; + latex(mgf(dist))) summarise = summarize # alias . # Define other symbols that show up mu = sym.symbols(&#39;mu&#39;, real=True) sigma, a, b, lamb, nu = sym.symbols(&#39;sigma a b lambda nu&#39;, positive=True) . # Normal Distribution normal = (2*sym.pi*sigma**2) ** sym.Rational(-1, 2) * sym.exp(-(x-mu)**2/(2*sigma**2)) summarize(normal) sym.pprint(normal) . Area: $1$ Mean: $ mu$ Variance: $ sigma^{2}$ MGF: $e^{ frac{t left(2 mu + sigma^{2} t right)}{2}}$ 2 -(-μ + x) ─────────── 2 2⋅σ √2⋅ℯ ─────────────── 2⋅√π⋅σ . sym.pprint (sym.latex(mu)) . mu . sym.pprint(skew_t.subs(v, 2)) . 2 -│x│ ────── 2 √2⋅ℯ ────────── 3⋅√π . from sympy.stats import * z = sym.Symbol(&#39;z&#39;) v = Normal(&#39;v&#39;, 30, 1) pdf = density(v) sym.plot(pdf(z), (z, 27, 33)) . &lt;sympy.plotting.plot.Plot at 0x7f119269c9d0&gt; . sym.pprint(pdf(z)) . 2 -(z - 30) ─────────── 2 √2⋅ℯ ─────────────── 2⋅√π . gamma_v = sym.gamma(v + 1/v) print (gamma_v) . gamma(v + 1/v) . gamma_v.subs(v,2) . $ displaystyle frac{3 sqrt{ pi}}{4}$",
            "url": "https://deebuls.github.io/devblog/robust%20statistics/maximum%20likelihood/2021/09/21/Robust-Statistics.html",
            "relUrl": "/robust%20statistics/maximum%20likelihood/2021/09/21/Robust-Statistics.html",
            "date": " • Sep 21, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Neurips Competition Uncertainty",
            "content": "Notes on Neurips Competition . I am participating in the Neurips Bayesian Deep Learning Competition, I will like to journal my notes here . Idea . Broad idea is to use Evidential loss function, Dropout, TTA combination. . Journal . 8th August Working in superconvergence | . | 13th August Roll back to pytorch-cifar and modifications | . | 14th August Training on evidential loss reaches only 83% accuracy in 300 epochs. | Why is evidental loss reducing the accuracy | . | 15th August Training with CE, AdamW, OnecylceLR? Can we improve training speed. | . | 17th August Dirichlet loss function. | . | 18th August Dirichlet + Mixup : best results, touched 90% | . | . Reference . Super Convergence cifar10, pytorchdata, cifar training, wideresenet, 92 accuracy | . | Pytorch Cifar SOA pytorch cifar10, all models, | . | Mixup pytorch mixup data combining while training | . | . Pytorch cifar . Model data criterion optim scheduler epochs accuracy link Notes . Resnet18 | pytorch | cross-entropy | SGD | annealing-200 | 200 | 94 | 1 |   | . Resnet20 | pytorch | cross-entropy | SGD | annealing-200 | 200 | 89 | 1 |   | . Resnet20 | tf | cross-entropy | SGD | annealing-200 | 200 | 90 | 1 |   | . Resnet20 | tf | Evidential | SGD | annealing-200 | 600 | 73/??/83 | 1 | Added randmErasing | . Resnet20 | tf | Label smooting | SGD | annealing-200 | 200 |   | ?? | ?? | . Resent20 | tf | cross-entropy | AdamW | 1 cycle | 30 | 83 | 1 |   | . Resnet20 | tf | cross-entropy | AdamW | 1 cycle | 100 | 88 | 1 | max_lr = 0.01 | . Resnet20 | tf | cross-entropy | AdamW | 1 cycle | 30 | 50 | 1 | max_lr=0.1 | . Resnet20 | tf | cross-entropy | AdamW | 1 cycle | 30 | 80 | 1 | max_lr=0.05 | . Resnet20 | tf | Evidential | AdamW | 1 cycle | 30 | 69 | 1 | max_lr=0.05 | . Resnet20 | tf | Evidential | AdamW | annealing-200 | 200 | 75 | 1 | max_lr=0.01 | . Resnet20 | tf | cross-entropy | AdamW | 1 cycle | 200 | 89 | 1 | max_lr = 0.05 | . Resnet20 | tf | cross-entropy | AdamW | 1 cycle | 200 | 89 | 1 | max_lr = 0.05, randomErase | .",
            "url": "https://deebuls.github.io/devblog/uncertainty/metrics/2021/08/08/Neurips-Competition-Uncertainty.html",
            "relUrl": "/uncertainty/metrics/2021/08/08/Neurips-Competition-Uncertainty.html",
            "date": " • Aug 8, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Using uncertainties in DNN application",
            "content": "Application of Uncertainty Estimation in DNN . Objective . We would like to look into two papers which have developed methodologies to use the uncertatiny estimated by the deep neural network(DNN) in their system. The goal is to find the pros and cons of these methodologies . Paper 1: Benchmarking uncertatinty estimation methods in Deep learning with Safety-Related Metrics . In this paper they propose two new Safety-Related Metrics: Remaining Error Rate(RER) and Remaining Accuracy Rate(Rate). Here is definition as per the paper: . A system which relies on these estimates is expected to be in functional mode if the predictions are certain and in fall-back/mitigation mode if the prediction is uncertain. However, the most critical result regarding safety are the predictions where the model is certain about its prediction but incorrect (CI). We call the ratio of the number of certain but incorrect samples to all samples the Remaining Error Rate (RER). For minimizing the overall risk, it needs to be as low as possible. Nonetheless, if a model would always give a low confidence as output, the system would constantly remain in fall-back mode and will be unable to provide the intended functionality. Therefore, the ratio of the number of certain and correct samples to all samples - we call it the Remaining Accuracy Rate (RAR) - needs to be as high as possible to stay in performance mode for most of the time. . Interpretation . The predictions from the DNN are classified into 4 sections as below (Table 1 from paper) .   Certain Uncertain . Correct | CC | UC | . Incorrect | CI | UI | . The definition of the metrics are : . $RER = frac{CI}{CC+CI+UC+UI}$ $RAR = frac{CC}{CC+CI+UC+UI}$ . Pros . Its a very simple metric. | Simplicity of metric is a very important thing for usability of metrics. | . Cons . A minor issue will be on the threshold which seprates Certain vs Uncertain. Is it 99% or 90% etc. All will yield different results | A major problem which we consider is the assumption in which the uncertatiny is being planned to be used in the system. A system which relies on these estimates is expected to be in functional mode if the predictions are certain and in fall-back/mitigation mode if the prediction is uncertain. . | This means that the system has 2 modes A functional mode | A fall-back/mitigation mode | . | Is this a safe assumption with regards to deployment of DNN? | Can an application deploying DNN have 2 modes ? | What should an autonomous car in fall-back mode do ? | . :bangbang: | The assumption on how a system uses uncertatiny is that the system has 2 modes functional and fall-back | :-: | :- | . Paper 2 : Fail-Safe Execution of Deep learning based Systems through Uncertatiny Monitoring . In this paper, as the title suggests they create a separate model called the Supervisor Model which will monitor the uncertainty of Deep learning and avoid any faults in the system | What is a supervisor model : Network supervision can be viewed as a binary classification task: malicious samples, i.e., inputs which lead to a misclassification (for classification problems) or to severe imprecision (in regression problems) are positive samples that have to be rejected. Other samples, also called benign samples, are negative samples in the binary classification task. An uncertainty based supervisor accepts an input i as a benign sample if its uncertainty u(i) is lower than some threshold t. The choice of t is a crucial setting, as a high t will fail to reject many malicious samples (false negatives) and a low t will cause too many false alerts (false positives). . | Thus the supervisor is a binary classification task to avoid beningn samples. They also define a metric S-Score which combined measures the performance of both the model and the supervisor model | There is lot of similarity with respect to the above paper here also | . Pros . They have made a library out of it such that any model can be used. | The threshold on which to make the decission is now being learned by the data. | . Cons . Again, these method is based on the assumption that the system which uses DNN has 2 modes of operation( normal mode and fall-back mode) | . :bangbang: | The same assumption on how a system uses uncertatiny, that the system has 2 modes functional and fall-back | :—: | :— | . Conclusion . All methods are based on the assumption that the system has 2 modes of operation | The uncertatiny estimation is used to determine whethere the DNN output should be trusted or should be avoided | . This is not enough . The methods which use DNN dont have a fall back mode. If there was an non DNN based method then by “First rule of Machine/Deep Learning” that will be used for solving the problem | . | There can be argument to say that there are redundant DNN systems and this method can be used to kick-off redundant system Even this argument is not valid as if you have redundant system, you should use all of them and make a decision | . | . Solutions . The one solution which I have been workin is about not binarizing the probability but the propagating it through the system | The best example is of the filters which have been developed over years to handle uncertain sensors. | . References . [1]M. Weiss and P. Tonella, “Fail-Safe Execution of Deep Learning based Systems through Uncertainty Monitoring,” arXiv:2102.00902 [cs], Feb. 2021, Accessed: Apr. 13, 2021. [Online]. Available: http://arxiv.org/abs/2102.00902. . | [2]M. Henne, A. Schwaiger, K. Roscher, and G. Weiss, “Benchmarking Uncertainty Estimation Methods for Deep Learning With Safety-Related Metrics,” p. 8, 2020. . | .",
            "url": "https://deebuls.github.io/devblog/uncertainty/2021/04/13/how-to-use-uncertainty-estimate-from-dnn.html",
            "relUrl": "/uncertainty/2021/04/13/how-to-use-uncertainty-estimate-from-dnn.html",
            "date": " • Apr 13, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "One-vs-All Classifier",
            "content": ". The paper . Padhy, S. et al. (2020) ‘Revisiting One-vs-All Classifiers for Predictive Uncertainty and Out-of-Distribution Detection in Neural Networks’, arXiv:2007.05134 [cs, stat]. Available at: http://arxiv.org/abs/2007.05134 (Accessed: 26 January 2021). . Abstract . Problem . Out-of-Distribution - finding on which samples the classifier should not predict | OOD using uncertainty estimate | uncertainty estimate - correctly estimate the confidence (uncertainty) in the prediction | . How . Using oves-vs-all classifier | distance-based logit representation to encode uncertainty | . Introduction . Capturing epistemic uncertainty is more important, as it captures model&#39;s lack of knowledge about data . | Three different paradigms to measure uncertainty . in-distribution calibration of models measured on an i.i.d. test set, | robustness under dataset shift, and | anomaly/out-of-distribution (OOD) detection, which measures the ability of models to assign low confidence predictions on inputs far away from the training data. | | . Unique selling points . we first study the contribution of the loss function used during training to the quality of predictive uncertainty of models. | Specifically, we show why the parametrization of the probabilities underlying softmax cross-entropy loss are ill-suited for uncertainty estimation. | We then propose two simple replacements to the parametrization of the probability distribution: a one-vs-all normalization scheme that does not force all points in the input space to map to one of the classes, thereby naturally capturing the notion of “none of the above”, and | a distance-based logit representation to encode uncertainty as a function of distance to the training manifold. | | . Experiments . Under Dataset Shift . CIFAR 10 Corrupted different intensity | CIFAR 100 Corrupted | . | OOD . CIFAR 100 vs CIFAR10 | CIFAR100 vs SVHN | . | Comparison of learned class centers . | Reliability Diagrams . | Imagenet . | CLINC Intent Classification Dataset . |",
            "url": "https://deebuls.github.io/devblog/probability/regression/uncertainty/robustness/2021/01/01/one-vs-all-classifier-revisiting.html",
            "relUrl": "/probability/regression/uncertainty/robustness/2021/01/01/one-vs-all-classifier-revisiting.html",
            "date": " • Jan 1, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "DNN Wiki",
            "content": "What to learn? . I wanted to develop glossary/Wiki of DNN related topics and my explanation of them so that I can be sure that I know the topics. But for that I needed a list of relevant topics. DNN is an exponentially exploding field and with low signa to noise ratio. So it becomes really difficult in fooling up with the new work without having a firm understanding of what is firm knowledge. This selection of topics should help any new commer to be sure that if I know what these topics are then you can claim that you know a little about deep learning. . Deep Learning vs Other Machine Learning Approaches | The Essential Math of Artificial Neurons | The Essential Math of Neural Networks | Activation Functions | Cost/Loss Functions | Stochastic Gradient Descent | Backpropagation | Mini-Batches | Learning Rate | Optimizers (e.g., Adam, Nadam) | Glorot/He Weight Initialization | Dense Layers | Softmax Layers | Dropout | Data Augmentation | . References . https://aiplus.odsc.com/courses/deep-learning-with-tensorflow-2-and-pytorch-1 | |",
            "url": "https://deebuls.github.io/devblog/learning/2020/07/22/Learning-DNN.html",
            "relUrl": "/learning/2020/07/22/Learning-DNN.html",
            "date": " • Jul 22, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Different types of uncertainty in DNN systems?",
            "content": "What are the uncertainties in DNN? . This question is easily answered if you have read a little bit of DNN literature. The most popular answer is . Aleatoric uncertainty | Epistemic uncertainty | . An additional statement is always added to these different types of uncertainty which will claim that . Aleatoric uncertatinty is model uncertainty | Epistemic uncertatinty is data uncertainty | . But what actually are these values and what do they explain ? Lets do a brief overview of different literature to answer these questions. . Alternative definitions . “Aleatory uncertainty is also referred to in the literature as variability, irreducible uncertainty, inherent uncertainty, and stochastic uncertainty. Epistemic uncertainty is also termed reducible uncertainty, subjective uncertainty, and state-of-knowledge uncertainty.” [2] . “Aleatory uncertainties are described as arising from inherent variabilities or randomness in systems, whereas epistemic uncertainties are due to imperfect knowledge.” [1] . Important points . Epistemic sources of uncertatinty is reducible but aleatory uncertainty is not reducible[1] | References . [1] Probability is Perfect, but we Can’t Elicitit Perfectly Anthony O’Hagan &amp; Jeremy E. Oakley http://www.yaroslavvb.com/papers/epistemic.pdf . [2] Challenge problems: uncertainty in system response given uncertain parameters Author links open overlay panelWilliam L.Oberkam https://www.sciencedirect.com/science/article/pii/S0951832004000493 .",
            "url": "https://deebuls.github.io/devblog/uncertainty/2020/05/29/Explanation-DNN-Uncertainty.html",
            "relUrl": "/uncertainty/2020/05/29/Explanation-DNN-Uncertainty.html",
            "date": " • May 29, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Regression Uncertainty",
            "content": "Regression . In machine learning problems we take input data $x$ and use it to predict the output data $y$. If the predicted data is continuous or discrete. When the output data $y$ is continuous then the problem is called regression while if its is discrete then the problem is called classification . . Deep neural networks have prooved to be a good tool for solving regression tasks. Currently there are new improvements in making the regression task robust by not only predicting the output data $y$ but also the corresponding uncertainty. . In this blog we will look into 2 methods of representing uncertatinty from literature. . Epistemic uncertatinty (Data uncertatinty) [1] | Aleotary uncertainty (Model uncertatinty) [2] | . Hierarchical Modelling . In regression task can be modelled in 3 different methods based on which all uncertainty needs to be accounted for during architecture of the deep network. The different layers and their hierarchichal structre is represented in the image above. . Name loss function uncertatinty output distribution . Deterministic Regression | mean square error | not modeled | $ hat{y}$ | - | . Likelihood Regression | Maximum likelihood estimation | data | $ hat{y}$, $ sigma^2$ | Normal | . Evidential Regression | Maximum likelihood estimation | data + model | $ hat{y}$,$ alpha$, $ beta$, $ lambda$ | Normal inverse gamma | . Dataset . We will start by creating a one dimension synthetic regression dataset. The synthetic dataset has added priori noise. The dataset is generated from $$ y = frac{ sin(3x)}{3x} + epsilon $$ where $ epsilon sim mathcal{N}(0, 0.02)$ . The training dataset consist of $x$ values between $-3 leq x leq 3$; while the test dataset consist of values outside of training $-4 leq x leq 4$ . Model . We will be using a common model for all the different learning formulation. . # this is one way to define a network class Net(torch.nn.Module): def __init__(self, n_feature, n_hidden, n_output): super(Net, self).__init__() self.hidden = torch.nn.Linear(n_feature, n_hidden) # hidden layer self.hidden_2 = torch.nn.Linear(n_hidden, n_hidden) self.predict = torch.nn.Linear(n_hidden, n_output) # output layer def forward(self, x): x = self.hidden(x) x = F.relu(self.hidden_2(x)) # activation function for hidden layer x = self.predict(x) # linear output return x . Deterministic Regression . Loss function . Mean square error $$ mathcal{L} = frac{1}{2} vert vert y_i - hat{y} vert vert ^2$$ . Output . The model will have single output $ hat{y}$ . msenet = Net(n_feature=1, n_hidden=10, n_output=1) # define the network print(msenet) # net architecture #optimizer = torch.optim.SGD(net.parameters(), lr=0.01) optimizer = torch.optim.Adam(msenet.parameters(), lr=0.001) loss_func = torch.nn.MSELoss() # this is for regression mean squared loss # train the network for t in range(10000): prediction = msenet(x) # input x and predict based on x loss = loss_func(prediction, y) # must be (1. nn output, 2. target) optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if t%1000 == 0: print(loss) prediction = msenet(test_x) plot_prediction(test_x, test_y, prediction) . Net( (hidden): Linear(in_features=1, out_features=10, bias=True) (hidden_2): Linear(in_features=10, out_features=10, bias=True) (predict): Linear(in_features=10, out_features=1, bias=True) ) tensor(0.5153, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0478, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0023, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) . Likelihood Regression . Loss function . In here we will use the maximum likelihood estimation error, which is also called as the Negative log likelihood. . $$ mathcal{L} = - left(-0.5 log(2 pi) - 0.5 sum log( sigma^2) - 0.5 sum frac{(y - hat{y})^2}{ sigma^2} right) $$ . Output . We will create a deep net with 2 output variables. . predicted output $ hat{y}$ | the variance of the normal distribution $ sigma^2$ | . Note . We aer also adding a regularizer $ frac{ | y - hat{y} | }{ sigma^2}$. This helps to regluarize the learning. It is inversely related to $ sigma^2$ and it scales based on distance from the actual value. . The regularizer ensures that values predicted far from true values have higher variance.So for values near to the predicted it gives less loss but values far away from prediction gives a large loss. . # Loss Function class MLELoss(torch.nn.Module): def __init__(self, weight=None, size_average=True): super(MLELoss, self).__init__() def forward(self, inputs, targets, smooth=1): targets = targets.view(-1) #converting variable to single dimension mu = inputs[:,0].view(-1) #extracting mu and sigma_2 logsigma_2 = inputs[:,1].view(-1) #logsigma and exp drives the variable to positive values always sigma_2 = torch.exp(logsigma_2) kl_divergence = (targets - mu)**2/sigma_2 #Regularizer mse = -0.5 * torch.sum(((targets - mu)**2)/sigma_2) sigma_trace = -0.5 * torch.sum(sigma_2) log2pi = -0.5 * np.log(2 * np.pi) J = mse + sigma_trace + log2pi loss = -J + kl_divergence.sum() return loss mlenet = Net(n_feature=1, n_hidden=10, n_output=2) # define the network print(mlenet) # net architecture optimizer = torch.optim.Adam(mlenet.parameters(), lr=0.001) loss_func = MLELoss() # this is for regression mean squared loss # train the network for t in range(10000): prediction = mlenet(x) # input x and predict based on x loss = loss_func(prediction, y) # must be (1. nn output, 2. target) optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if t%1000 == 0: print(loss) prediction = mlenet(test_x) mu = prediction[:,0] sigma2 = torch.exp(prediction[:,1]) sigma = torch.sqrt(sigma2) plot_prediction(test_x, test_y, mu, sigma) . Net( (hidden): Linear(in_features=1, out_features=10, bias=True) (hidden_2): Linear(in_features=10, out_features=10, bias=True) (predict): Linear(in_features=10, out_features=2, bias=True) ) tensor(68.1900, grad_fn=&lt;AddBackward0&gt;) tensor(6.4617, grad_fn=&lt;AddBackward0&gt;) tensor(6.1683, grad_fn=&lt;AddBackward0&gt;) tensor(6.1068, grad_fn=&lt;AddBackward0&gt;) tensor(6.0325, grad_fn=&lt;AddBackward0&gt;) tensor(5.9853, grad_fn=&lt;AddBackward0&gt;) tensor(5.9657, grad_fn=&lt;AddBackward0&gt;) tensor(5.9485, grad_fn=&lt;AddBackward0&gt;) tensor(5.9929, grad_fn=&lt;AddBackward0&gt;) tensor(5.9366, grad_fn=&lt;AddBackward0&gt;) . Evidential Regression . Evidential regression is based on paper [2] (Amini &amp; e.t.al, 2019), which is based on the ideas of [3, 4] that if we represent the output of the model with a higher order data distribution its possible to model the data and model uncertainties. . Loss . $$ mathcal{L} = left( frac{ Gamma( alpha - 0.5)}{4 Gamma( alpha) lambda sqrt beta} right) left( 2 beta(1 + lambda) + (2 alpha -1) lambda(y_i - hat{y})^2 right)$$ . output . The model has 4 outputs . $ hat{y}$ | $ alpha$ | $ beta$ | $ lambda$ | . Regularizer . Regularizer is required to penalize the loss function for OOD predictions. . $$ | y - hat{y} | ^2 (2 alpha + lambda)$$ . class EvidentialLoss(torch.nn.Module): def __init__(self, weight=None, size_average=True): super(EvidentialLoss, self).__init__() def forward(self, inputs, targets, smooth=1): targets = targets.view(-1) y = inputs[:,0].view(-1) #first column is mu,delta, predicted value loga = inputs[:,1].view(-1) #alpha logb = inputs[:,2].view(-1) #beta logl = inputs[:,3].view(-1) #lamda a = torch.exp(loga) b = torch.exp(logb) l = torch.exp(logl) term1 = (torch.exp(torch.lgamma(a - 0.5)))/(4 * torch.exp(torch.lgamma(a)) * l * torch.sqrt(b)) #print(&quot;term1 :&quot;, term1) term2 = 2 * b *(1 + l) + (2*a - 1)*l*(y - targets)**2 #print(&quot;term2 :&quot;, term2) J = term1 * term2 #print(&quot;J :&quot;, J) Kl_divergence = torch.abs(y - targets) * (2*a + l) #Kl_divergence = ((y - targets)**2) * (2*a + l) #print (&quot;KL &quot;,Kl_divergence.data.numpy()) loss = J + Kl_divergence #print (&quot;loss :&quot;, loss) return loss.mean() evnet = Net(n_feature=1, n_hidden=10, n_output=4) # define the network print(evnet) # net architecture optimizer = torch.optim.Adam(evnet.parameters(), lr=0.001) loss_func = EvidentialLoss() # train the network for t in range(20000): prediction = evnet(x) # input x and predict based on x loss = loss_func(prediction, y) # must be (1. nn output, 2. target) optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if t%10000 == 0: print(loss) prediction = evnet(test_x) mu = prediction[:,0].view(-1) #first column is mu,delta, predicted value a = prediction[:,1].view(-1) #alpha b = prediction[:,2].view(-1) #beta l = prediction[:,3].view(-1) #lamda a = torch.exp(a); b = torch.exp(b); l = torch.exp(l) var = b / ((a -1)*l) #epistemic/ model/prediciton uncertaitnty e = b / (a - 1) # aleatoric uncertainty/ data uncertainty plot_prediction(test_x, test_y, mu, var, e) . Net( (hidden): Linear(in_features=1, out_features=10, bias=True) (hidden_2): Linear(in_features=10, out_features=10, bias=True) (predict): Linear(in_features=10, out_features=4, bias=True) ) tensor(3.1019, grad_fn=&lt;MeanBackward0&gt;) tensor(0.1492, grad_fn=&lt;MeanBackward0&gt;) . Conclusions . Loss functions of maximum likelihood and evidence have been implemented from formulas from the paper. . Important: Leasons Learned - how to treat loss function when one of the output variable is always positive. (log and exponential to rescue) -The difference between optimize and loss function and do you sum and add constant ones or do you add constant in all equation and sum. | The trianing is working on some runs not all. | The model learns the function easily but learning the uncertainty in unkonw region takes longer number of iterations. Needs to be further investigated. . Note: ToDo : Test functions for the loss function written . Note: ToDo : What is stopping condition? when to stop learning? . Note: ToDo : Replace the regularizer in regularizer in likelihood loss with actual regularizer. . Warning: Why is uncertatiny different for both the models. Evidential model seems to be very confident in the known region, which seems to be fishy . Warning: The loss function doesnt give same results all the time, so needs to be further investigated | . References . [1] Kendall, Alex, and Yarin Gal. “What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?” ArXiv:1703.04977 [Cs], October 5, 2017. http://arxiv.org/abs/1703.04977. . [2] Amini, Alexander, Wilko Schwarting, Ava Soleimany, and Daniela Rus. “Deep Evidential Regression.” ArXiv:1910.02600 [Cs, Stat], October 7, 2019. http://arxiv.org/abs/1910.02600. . [3] Sensoy, Murat, Lance Kaplan, and Melih Kandemir. “Evidential Deep Learning to Quantify Classification Uncertainty,” n.d., 11. . [4] Malinin, Andrey, and Mark Gales. &quot;Predictive uncertainty estimation via prior networks.&quot; Advances in Neural Information Processing Systems. 2018. .",
            "url": "https://deebuls.github.io/devblog/probability/regression/uncertainty/2020/05/27/uncertainty-regression.html",
            "relUrl": "/probability/regression/uncertainty/2020/05/27/uncertainty-regression.html",
            "date": " • May 27, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Dataset shift",
            "content": "Dataset shift . Dataset shift is still an unsolved problems when it comes to deploying learning models “in the wild”. . There are 2 different categories of dataset shift . Co-variate shift | label shift | . Let $ mathbb{S} $ be the source data distribution and $ mathbb{T} $ be the target data distribution. If we denote the input variables as x and output variables as y, then . Covariate shift . s(x)≠t(x)s(x) neq t(x)s(x)=t(x) input distribution of both source and target are different . but . s(y∣x)=t(y∣x)s(y|x) = t(y|x)s(y∣x)=t(y∣x) conditional output distirbution is invariant to dataset shift. . Label shift . s(y)≠t(y)s(y) neq t(y)s(y)=t(y) output distribution of both source and target are different . but . s(x∣y)=t(x∣y)s(x|y) = t(x|y)s(x∣y)=t(x∣y) conditional input distirbution is invariant to dataset shift .   Covariate Shift Label Shift . input distribution | $s(x) neq t(x)$ | $?$ | . output distribution | $?$ | $s(y) neq t(y)$ | . conditional output distribution | $s(y vert x) = t(y vert x)$ | $?$ | . conditional input Distribution | $?$ | ${s(x vert y) = t(x vert y)}$ | . Examples . ToDo . Simluated Dataset {ReDo with examples} . The problem can be simulated in image based calssification dataset like MNIST and CIFAR. . Tweak-One shift . refers to the case where we set a class to have probability $ p &gt; 0.1$, while the distribution over the rest of the classes is uniform. . Minority-Class Shiftis . A more general version of Tweak-One shift, where a fixed number of classes to have probability $p &lt; 0.1$, while the distribution over the rest of the classes isuniform. . Dirichlet shift . we draw a probability vector $p$ from the Dirichlet distribution with concentration parameter set to $ alpha$ for all classes, before including sample points which correspond to the multinomial label variable according top. .",
            "url": "https://deebuls.github.io/devblog/dataset/2020/05/21/Explanation-Learning-DatasetShift.html",
            "relUrl": "/dataset/2020/05/21/Explanation-Learning-DatasetShift.html",
            "date": " • May 21, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Plotting Normal Inverse Gamma Distirbution",
            "content": ". Scipy stats doesnt have Normal Inverse Gamma distirbution. . We would like to incorporate Normal Inverse Gamma distirbution in &quot;scipy.stats&quot; package. . Learning about Normal Inverse Gamma(NIG) distribution will lead you to a plot like this from wikipedia. . . It was intruiging enough to find out how to plot this graph in python and was sure that there will be some already plots available. But to my suprise there is no blogs or docs to plot NIG in python. The closest I found was in R langugage in [1] by Frank Portman. . So I spent some time to plot NIG in python below is the snippet for it. Special thanks to Jake Vadendeplas[2] for his wonderful blogs about visualization in python. . Normal Inverse Gamma Distribution . Let the input $x$ on which its modelled be : $$ x = [ mu, sigma^2] $$ . Probability density function (PDF) . $$ f(x | delta, alpha, beta, lambda ) = sqrt{ left( frac{ lambda}{2 pi x[ sigma^2]} right)} frac{ beta^ alpha}{ Gamma( alpha)} left( frac{1}{x[ sigma^2]} right)^{( alpha + 1)} exp{ left( - frac{2 beta + lambda left(x[ mu] - delta right)^2 }{ 2 x[ sigma]^2} right)} $$ . from scipy.stats import rv_continuous from scipy.stats import norm from scipy.stats import gengamma from scipy.special import gamma from scipy.stats import expon import numpy as np %matplotlib inline import matplotlib.pyplot as plt plt.style.use(&#39;seaborn-white&#39;) class norminvgamma(): r&quot;&quot;&quot;A normal inverse gamma random variable. The mu (``mu``) keyword specifies the parmaeter mu. %(before_notes)s Notes -- The probability density function for `norminvgamma` is: .. math:: x = [ mu, sigma^2] f(x | delta, alpha, beta, lamda) = sqrt( frac{ lamda}{2 pi x[ sigma^2}]) frac{ beta^ alpha}{ gamma( alpha)} frac{1}{x[ sigma^2]}^( alpha + 1) exp(- frac{2 beta + lamda(x[ mu] - delta)^2}{2 x[ sigma^2] }) for a real number :math:`x` and for positive number :math: ` sigma^2` &gt; 0 %(after_notes)s %(example)s &quot;&quot;&quot; def __init__(self, delta, alpha, beta, lamda): self.argcheck(delta, alpha, beta, lamda) self.delta = delta self.alpha = alpha self.beta = beta self.lamda = lamda def argcheck(self, delta, alpha, beta, lamda): return (alpha &gt; 0) def rvs(self, size=1): sigma_2 = gengamma.rvs(self.alpha, self.beta, size=size) sigma_2 = np.array(sigma_2) return [[norm.rvs(self.delta, s/self.lamda), s] for s in sigma_2] def pdf(self, xmu, xsigma2): t1 = ((self.lamda)**0.5) * ((self.beta)**self.alpha) t2 = (xsigma2 * (2 * 3.15)**0.5) * gamma(self.alpha) t3 = (1 / xsigma2**2)**(self.alpha + 1) t4 = expon.pdf((2*self.beta + self.lamda*(self.delta-xmu)**2)/(2*xsigma2**2)) #print (t1, t2, t3, t4) return (t1/t2)*t3*t4 def stats(self): #ToDo return def plot(self,zoom=0.9, axs=None): steps = 50 max_sig_sq = gengamma.ppf(zoom, self.alpha, self.beta) * self.lamda #print(max_sig_sq) mu_range = np.linspace(self.delta - 1 * max_sig_sq, self.delta + 1 * max_sig_sq, num=steps) #print (mu_range[0], mu_range[-1]) sigma_range = np.linspace(0.01, max_sig_sq, num=steps) mu_grid, sigma_grid = np.meshgrid(mu_range, sigma_range) pdf_mesh = self.pdf(mu_grid, sigma_grid) if axs: contours = axs.contour(mu_grid, sigma_grid, pdf_mesh, 20, cmap=&#39;RdGy&#39;); plt.clabel(contours, inline=True, fontsize=8) #extent=[mu_range[0], mu_range[-1], sigma_range[0], sigma_range[-1]] axs.imshow(pdf_mesh, extent=[mu_range[0], mu_range[-1], sigma_range[0], sigma_range[-1]], origin=&#39;lower&#39;, cmap=&#39;Blues&#39;, alpha=0.5) axs.axis(&#39;equal&#39;) axs.set_title(&quot;(&quot;+str(self.delta)+&quot;,&quot;+str(self.alpha)+&quot;,&quot; +str(self.beta)+&quot;,&quot;+str(self.lamda)+&quot;)&quot;) #plt.colorbar(); else: assert True, &quot;Pass the axes to plot from matplotlib&quot; . Varying different range of $ alpha$ . #norminvgamma = norminvgamma_gen() fig, axs = plt.subplots(1, 3, sharey=True, figsize=(15,5)) fig.suptitle(&#39;Vertically alpha&#39;) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=1) samples = nig.rvs(size=10) nig.plot(axs=axs[0]) nig = norminvgamma(delta=0,alpha=2,beta=1, lamda=1) nig.plot(zoom=0.7, axs=axs[1]) nig = norminvgamma(delta=0,alpha=4,beta=1, lamda=1) nig.plot(zoom=0.2, axs=axs[2]) . Varying different range of $ beta$ . fig, axs = plt.subplots(1, 3, sharey=True, figsize=(15,5)) fig.suptitle(&#39;Varying beta&#39;) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=1) samples = nig.rvs(size=10) nig.plot(axs=axs[0]) nig = norminvgamma(delta=0,alpha=1,beta=2, lamda=1) nig.plot( axs=axs[1]) nig = norminvgamma(delta=0,alpha=1,beta=3, lamda=1) nig.plot(axs=axs[2]) . Varying different range of $ lambda$ . fig, axs = plt.subplots(1, 3, sharey=True, figsize=(15,5)) fig.suptitle(&#39;Verying lamda &#39;) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=1) samples = nig.rvs(size=10) nig.plot(axs=axs[0]) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=2) nig.plot( axs=axs[1]) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=4) nig.plot(axs=axs[2]) . References . https://frankportman.github.io/bayesAB/reference/plotNormalInvGamma.html | https://jakevdp.github.io/PythonDataScienceHandbook/04.04-density-and-contour-plots.html |",
            "url": "https://deebuls.github.io/devblog/probability/python/plotting/matplotlib/2020/05/19/probability-normalinversegamma.html",
            "relUrl": "/probability/python/plotting/matplotlib/2020/05/19/probability-normalinversegamma.html",
            "date": " • May 19, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Regression Uncertainty Robustness",
            "content": "Robustness . Robustness in learning is the capacity of the network to handle corrupted data (training data or test data). . Robustness during training vs testing . Training time robustness is capability of the model to overcome corrupted training data(input or labels). . Testing time robustness is the capability of the model to corrupted data during testing eg. Adversarial test. . Objective . The objective of this blog is to understand the robustness capability of different loss fucntions with respect to training time dataset corruption. . The loss functions in considerations are: . Gaussian negative log likelihood | Laplace negative log likelihood | Cauchy negative log likelihood | Data . Lets start with plotting our toy dataset. . As you can see the dataset has some corrupted labels which should not be considered while learning. . fig, ax = plt.subplots(figsize=(5,5)) ax.scatter(x.data.numpy(),y.data.numpy()) ax.axis(&#39;equal&#39;) ax.set_xlabel(&#39;$x$&#39;) ax.set_ylabel(&#39;$y$&#39;) ax.axis(&quot;equal&quot;) ax.annotate(&#39;noisy labels&#39;, xy=(0.25, 0.8), xytext=(0.0, 0.6), arrowprops=dict(facecolor=&#39;red&#39;, shrink=0.05)) ax.annotate(&#39;noisy labels&#39;, xy=(0.85, 0.1), xytext=(0.7, 0.3), arrowprops=dict(facecolor=&#39;red&#39;, shrink=0.05)) . Text(0.7, 0.3, &#39;noisy labels&#39;) . Model . We will train a simple linear model . # this is one way to define a network class Net(torch.nn.Module): def __init__(self, n_feature, n_hidden, n_output): super(Net, self).__init__() self.hidden = torch.nn.Linear(n_feature, n_hidden) # hidden layer self.predict = torch.nn.Linear(n_hidden, n_output) # output layer def forward(self, x): x = F.relu(self.hidden(x)) # activation function for hidden layer x = self.predict(x) # linear output return x . Mean Square Loss robustness analysis . loss_func = torch.nn.MSELoss() # this is for regression mean squared loss # Fit a linear regression using mean squared error. regression = Net(n_feature=1, n_hidden=1, n_output=1) # RegressionModel() params = regression.parameters() optimizer = torch.optim.Adam(params, lr = 0.001) . Analysis MSE Regression . Gaussian Loss Robustness analysis . For gaussian loss the model in addition to the prediction also predicts the confidence in the prediction in the form of the gaussian variance. . # this is one way to define a network class GaussianNet(torch.nn.Module): def __init__(self, n_feature, n_hidden, n_output): super(GaussianNet, self).__init__() self.hidden = torch.nn.Linear(n_feature, n_hidden) # hidden layer self.predict = torch.nn.Linear(n_hidden, n_output) # output layer self.variance = torch.nn.Linear(n_hidden, 1) # variance layer #torch.nn.init.xavier_uniform_(self.variance.weight) #torch.nn.init.normal_(self.variance.weight, mean=1.0) #torch.nn.init.normal_(self.variance.bias, mean=0.0) def forward(self, x): x = F.relu(self.hidden(x)) # activation function for hidden layer out = self.predict(x) # linear output var = F.softplus(self.variance(x)) return out, var . loss_func = torch.nn.GaussianNLLLoss( reduction=&#39;none&#39;) # Fit a linear regression using mean squared error. regression = GaussianNet(n_feature=1, n_hidden=2, n_output=1) # RegressionModel() params = regression.parameters() optimizer = torch.optim.Adam(params, lr = 0.001) . Smaller comparable network . Laplace loss function . def LaplaceNLLLoss(input, target, scale, eps=1e-06, reduction=&#39;mean&#39;): loss = torch.log(2*scale) + torch.abs(input - target)/scale # Inputs and targets much have same shape input = input.view(input.size(0), -1) target = target.view(target.size(0), -1) if input.size() != target.size(): raise ValueError(&quot;input and target must have same size&quot;) # Second dim of scale must match that of input or be equal to 1 scale = scale.view(input.size(0), -1) if scale.size(1) != input.size(1) and scale.size(1) != 1: raise ValueError(&quot;scale is of incorrect size&quot;) # Check validity of reduction mode if reduction != &#39;none&#39; and reduction != &#39;mean&#39; and reduction != &#39;sum&#39;: raise ValueError(reduction + &quot; is not valid&quot;) # Entries of var must be non-negative if torch.any(scale &lt; 0): raise ValueError(&quot;scale has negative entry/entries&quot;) # Clamp for stability scale = scale.clone() with torch.no_grad(): scale.clamp_(min=eps) # Calculate loss (without constant) loss = (torch.log(2*scale) + torch.abs(input - target) / scale).view(input.size(0), -1).sum(dim=1) # Apply reduction if reduction == &#39;mean&#39;: return loss.mean() elif reduction == &#39;sum&#39;: return loss.sum() else: return loss . loss_func = LaplaceNLLLoss # Fit a linear regression using mean squared error. regression = GaussianNet(n_feature=1, n_hidden=2, n_output=1) # RegressionModel() params = regression.parameters() optimizer = torch.optim.Adam(params, lr = 0.001) ##################### # Training #################### my_images = [] fig, (ax1, ax2) = plt.subplots(figsize=(20,7), nrows=1, ncols=2) # train the network for epoch in range(4000): prediction, scales = regression(x) loss_all = loss_func(prediction, y, scales, reduction=&#39;none&#39;) # must be (1. nn output, 2. target) loss = torch.mean(loss_all) #if t%10 == 0: print (loss) optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if np.mod(epoch, 100) == 0: sort_x, _ = torch.sort(x, dim=0) sort_prediction, sort_scales = regression(sort_x) print (loss) # plot and show learning process plt.cla() ax1.cla() ax1.set_title(&#39;Regression Analysis&#39;, fontsize=35) ax1.set_xlabel(&#39;Independent variable&#39;, fontsize=24) ax1.set_ylabel(&#39;Dependent variable&#39;, fontsize=24) ax1.set_xlim(-0.05, 1.0) ax1.set_ylim(-0.1, 1.0) ax1.scatter(x.data.numpy(), y.data.numpy(), color = &quot;orange&quot;) ax1.plot(sort_x.data.numpy(), sort_prediction.data.numpy(), &#39;g-&#39;, lw=3) dyfit = 2 * sort_scales.data.numpy() # 2*sigma ~ 95% confidence region ax1.fill_between( np.squeeze(sort_x.data.numpy()), np.squeeze(sort_prediction.data.numpy() - dyfit), np.squeeze(sort_prediction.data.numpy() + dyfit), color=&#39;gray&#39;, alpha=0.2) #l2_loss_plot_x = np.linspace(0,1,num=100) #y_plot_true = l2_loss_plot_x * scale_true + shift_true #ax1.plot(l2_loss_plot_x, y_plot_true, &#39;k&#39;) ax1.text(1.0, 0.1, &#39;Step = %d&#39; % epoch, fontdict={&#39;size&#39;: 24, &#39;color&#39;: &#39;red&#39;}) ax1.text(1.0, 0, &#39;Loss = %.4f&#39; % loss.data.numpy(), fontdict={&#39;size&#39;: 24, &#39;color&#39;: &#39;red&#39;}) diff = prediction.data.numpy() - y.data.numpy() ax2.cla() l2_loss_plot_x = np.linspace(-1,1,num=100) ax2.plot(l2_loss_plot_x, 0.5*l2_loss_plot_x**2, color=&quot;green&quot;, lw=3, alpha=0.5) ax2.scatter(diff, loss_all.data.numpy()) ax2.set_title(&#39;Loss &#39;, fontsize=35) ax2.set_xlabel(&#39;y - y_pred&#39;) ax2.set_ylim(-3.1, 3) ax2.set_xlim(-1, 1) # Used to return the plot as an image array # (https://ndres.me/post/matplotlib-animated-gifs-easily/) fig.canvas.draw() # draw the canvas, cache the renderer image = np.frombuffer(fig.canvas.tostring_rgb(), dtype=&#39;uint8&#39;) image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,)) my_images.append(image) . tensor(1.4615, grad_fn=&lt;MeanBackward0&gt;) tensor(0.8189, grad_fn=&lt;MeanBackward0&gt;) tensor(0.1342, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.2028, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.6679, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8267, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8590, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8677, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8730, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8773, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8809, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8837, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8857, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8873, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8879, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8891, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8894, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8898, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8901, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8902, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8901, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8901, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8903, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8901, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8902, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8901, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8903, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8904, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8902, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8900, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8903, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8903, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8902, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8905, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8903, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8903, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8905, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8900, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8904, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8905, grad_fn=&lt;MeanBackward0&gt;) . Analysis Laplace NLL Regression . The line is better fit to the data by avoiding to the noisy data. | . Cauchy NLL Loss function Robustness analysis . def CauchyNLLLoss(input, target, scale, eps=1e-06, reduction=&#39;mean&#39;): # Inputs and targets much have same shape input = input.view(input.size(0), -1) target = target.view(target.size(0), -1) if input.size() != target.size(): raise ValueError(&quot;input and target must have same size&quot;) # Second dim of scale must match that of input or be equal to 1 scale = scale.view(input.size(0), -1) if scale.size(1) != input.size(1) and scale.size(1) != 1: raise ValueError(&quot;scale is of incorrect size&quot;) # Check validity of reduction mode if reduction != &#39;none&#39; and reduction != &#39;mean&#39; and reduction != &#39;sum&#39;: raise ValueError(reduction + &quot; is not valid&quot;) # Entries of var must be non-negative if torch.any(scale &lt; 0): raise ValueError(&quot;scale has negative entry/entries&quot;) # Clamp for stability scale = scale.clone() with torch.no_grad(): scale.clamp_(min=eps) # Calculate loss (without constant) loss = (torch.log(3.14*scale) + torch.log(1 + ((input - target)**2)/scale**2)) .view(input.size(0), -1).sum(dim=1) # Apply reduction if reduction == &#39;mean&#39;: return loss.mean() elif reduction == &#39;sum&#39;: return loss.sum() else: return loss . loss_func = CauchyNLLLoss # Fit a linear regression using mean squared error. regression = GaussianNet(n_feature=1, n_hidden=2, n_output=1) # RegressionModel() params = regression.parameters() optimizer = torch.optim.Adam(params, lr = 0.001) ##################### # Training #################### my_images = [] fig, (ax1, ax2) = plt.subplots(figsize=(20,7), nrows=1, ncols=2) # train the network for epoch in range(4000): prediction, sigmas = regression(x) # input x and predict based on x loss_all = loss_func(prediction, y, sigmas, reduction=&#39;none&#39;) # must be (1. nn output, 2. target) loss = torch.mean(loss_all) #if t%10 == 0: print (loss) optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if np.mod(epoch, 100) == 0: #print (loss) sort_x, _ = torch.sort(x, dim=0) sort_prediction, sort_sigmas = regression(sort_x) # input x and predict based on x # plot and show learning process plt.cla() ax1.cla() ax1.set_title(&#39;Regression Analysis&#39;, fontsize=35) ax1.set_xlabel(&#39;Independent variable&#39;, fontsize=24) ax1.set_ylabel(&#39;Dependent variable&#39;, fontsize=24) ax1.set_xlim(-0.05, 1.0) ax1.set_ylim(-0.1, 1.0) ax1.scatter(x.data.numpy(), y.data.numpy(), color = &quot;orange&quot;) ax1.plot(sort_x.data.numpy(), sort_prediction.data.numpy(), &#39;g-&#39;, lw=3) dyfit = 2 * np.sqrt(sort_sigmas.data.numpy()) # 2*sigma ~ 95% confidence region ax1.fill_between( np.squeeze(sort_x.data.numpy()), np.squeeze(sort_prediction.data.numpy() - dyfit), np.squeeze(sort_prediction.data.numpy() + dyfit), color=&#39;gray&#39;, alpha=0.2) #l2_loss_plot_x = np.linspace(0,1,num=100) #y_plot_true = l2_loss_plot_x * scale_true + shift_true #ax1.plot(l2_loss_plot_x, y_plot_true, &#39;k&#39;) ax1.text(1.0, 0.1, &#39;Step = %d&#39; % epoch, fontdict={&#39;size&#39;: 24, &#39;color&#39;: &#39;red&#39;}) ax1.text(1.0, 0, &#39;Loss = %.4f&#39; % loss.data.numpy(), fontdict={&#39;size&#39;: 24, &#39;color&#39;: &#39;red&#39;}) diff = prediction.data.numpy() - y.data.numpy() ax2.cla() l2_loss_plot_x = np.linspace(-1,1,num=100) ax2.plot(l2_loss_plot_x, 0.5*l2_loss_plot_x**2, color=&quot;green&quot;, lw=3, alpha=0.5) ax2.scatter(diff, loss_all.data.numpy()) ax2.set_title(&#39;Loss &#39;, fontsize=35) ax2.set_xlabel(&#39;y - y_pred&#39;) ax2.set_ylim(-3.1, 3) ax2.set_xlim(-1, 1) # Used to return the plot as an image array # (https://ndres.me/post/matplotlib-animated-gifs-easily/) fig.canvas.draw() # draw the canvas, cache the renderer image = np.frombuffer(fig.canvas.tostring_rgb(), dtype=&#39;uint8&#39;) image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,)) my_images.append(image) . Analysis Cauchy NLL Regression . The line is better fit to the data than the gaussian by avoiding to the noisy data. | . Plotting Loss Surface . sigma_2 = np.logspace(0.1, 1, num=70, base=10 ) print (sigma_2.max(), sigma_2.min()) diff = np.linspace(-3, 5, num=70) def gauss_logL(xbar, sigma_2, mu): return -0.5*np.log(2*np.pi)-0.5*np.log(sigma_2)-0.5*(xbar -mu)**2/sigma_2 xbar = 1 logL = gauss_logL(xbar, sigma_2[:, np.newaxis], diff) logL -= logL.max() x_grid, sigma_grid = np.meshgrid(diff, sigma_2) logL = gauss_logL(xbar, sigma_grid, x_grid) logL = logL*-1 . 10.0 1.2589254117941673 . fig, ax = plt.subplots(figsize=(10,8),constrained_layout=True) ax = plt.axes(projection=&#39;3d&#39;) ax.plot_surface(x_grid, sigma_grid, logL, rstride=1, cstride=1, cmap=&#39;viridis&#39;, edgecolor=&#39;none&#39;) ax.set_title(&#39;NLL loss surface&#39;); . from itertools import cycle cycol = cycle(&#39;bgrcmk&#39;) fig, ax = plt.subplots(figsize=(10,7)) #for s_2 in [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 2]: for s_2 in [1e-3, 1e-2, 2e-3]: x = np.linspace(-3, 3, num=100) logL = (-1*gauss_logL(0, s_2, x))/1000 ax.plot(x, 0.5*x**2, color=&quot;cyan&quot;, lw=3, alpha=0.5) ax.plot(x,logL, c=next(cycol)) .",
            "url": "https://deebuls.github.io/devblog/probability/regression/uncertainty/robustness/2020/05/05/uncertainty-regression-robustness.html",
            "relUrl": "/probability/regression/uncertainty/robustness/2020/05/05/uncertainty-regression-robustness.html",
            "date": " • May 5, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Likelihood",
            "content": "What is Likelihood? . Likelihood and probablity seems to be same word in the layman domain, but in the stats domain they are different. . In the stats domain the likelihood or likelihood function is a measurement. It measures the distance between a statistical model and the input data. . What is a statistical model? . The diferent probability distributions available. For example, Gausian, gama, beta distribution, exponential for continuous data while Bernoulli, Dirichlet, multinomila distributions for discrete data. . How are statistical models represented? . By their parameters. For example for gaussian distribution the parameters are $ mu$ and $ sigma$ . . How do we select the statictical model? . Depends on many factors. This is the main decision to be made while designing a statistical model based learning. The different factors include: what is the data type: Continuous or discrete? Is it symmetrical or asymetrical? . Domain of the data, binary, real, etc | Does it decay or increase? | . . . etc | . A complete knowledge about the type data and the type of distribution is required to make the appropriate decision. . Good blog on likelihood with scipy.stats . https://www.kaggle.com/code/tentotheminus9/so-you-have-a-diagnostic-test-result/notebook | . Common Probability distribution . Data Type Domain Distribution Python (numpy.random) Parameters . univariate, discrete, binary | $$ x in {0,1 } $$ | Bernoulli | binomial(1, p) | $$ p in[0,1]$$ | . univariate, discrete, multivalued | $$ x in { 1,2, dots, K }$$ | multinomial | multinomial(n, pvals) | $$pvals = [p_1, dots , p_k] $$ $$ sum_{i=1}^{K} p_i = 1 $$ | . univariate, continuous, unbounded | $$ x in mathbb{R} $$ | normal | normal(mu, sigma) | $$ mu in mathbb{R} $$ $$ sigma in mathbb{R}$$ | . #Lets make some distributions and find the likelihood to some data import numpy as np import matplotlib.pyplot as plt number_of_samples = 20; #parameters ; sample data from distribution (continuous data) mu, sigma = 12, 0.1 ; univariate_gaussian_samples = np.random.normal(mu, sigma, number_of_samples) mean = [0, 0]; cov = [[1, 0], [0, 100]]; multivariate_gaussian_samples = np.random.multivariate_normal(mean, cov, number_of_samples) #parameters ; sample data from distribution (discreta data) p = 0.8 ; bernoulli_samples = np.random.binomial(1, p, number_of_samples) pvals = [0.2, 0.6, 0.2] ; multinomial_samples = np.random.multinomial(number_of_samples, pvals) alpha, beta = 10, 20 ; beta_samples = np.random.beta(alpha, beta, number_of_samples) alpha = [10,20,10,90] ; dirchilet_samples = np.random.dirichlet(alpha, number_of_samples) . Goal of Likelihood . The goal of likelihood would be given the samples as shown above (beta_samples, dirichlet_samples etc) find the parameters of the corresponding distribution ((alpha, beta), alphas respectively) . Lets look into this process in the comming post . #hide .",
            "url": "https://deebuls.github.io/devblog/probability/python/2020/03/20/probability-likelihood.html",
            "relUrl": "/probability/python/2020/03/20/probability-likelihood.html",
            "date": " • Mar 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I’m a researcher at the Bonn-Aachen International Center for Information Technology (b-it) at the Autonomous Systems group in Bonn, Germany. . My research focus is on developing strategies for safe and reliable incorporation of deep learning methods into robotics. . I am also the lead develper with the b-it-bots team which won the RoboCup WorldCup and GermanOpen in the work league. . Publications . b-it-bots: Our Approach for Autonomous Robotics in Industrial Environments . Deebul Nair, Santosh Thoduka, Iman Awaad, Sven Schneider, Paul G. Plöger, Gerhard K. Kraetzschmar and students . Robot World Cup. Springer, Cham, 2019. . This paper presents the approach of our team, b-it-bots, in the RoboCup@Work competition which resulted in us winning the World Championship in Sydney in 2019. We describe our current hardware, including modifications made to the KUKA youBot, the underlying software framework and components developed for navigation, manipulation, perception and task planning for scenarios in industrial environments. Our combined 2D and 3D approach for object recognition has improved robustness and performance compared to previous years, and our task planning framework has moved us away from large state machines for high-level control. Future work includes closing the perception-manipulation loop for more robust grasping. Our open-source repository is available at https://github.com/b-it-bots/mas_industrial_robotics. . Performance Evaluation of Low-Cost Machine Vision Cameras forImage-Based Grasp Verification . Performance Evaluation of Low-Cost Machine Vision Cameras forImage-Based Grasp Verification . Under review in IROS 2020 . Grasp verification is advantageous for au-tonomous manipulation robots as they provide the feedbackrequired for higher level planning components about successfultask completion. However, a major obstacle in doing graspverification is sensor selection. In this paper, we propose a visionbased grasp verification system using machine vision cameras,with the verification problem formulated as an image classifi-cation task. Machine vision cameras consist of a camera anda processing unit capable of on-board deep learning inference.The inference in these low-power hardware are done near thedata source, reducing the robots dependence on a centralizedserver, leading to reduced latency, and improved reliability.Machine vision cameras provide the deep learning inferencecapabilities using different neural accelerators. Although, it isnot clear from the documentation of these cameras what is theeffect of these neural accelerators on performance metrics suchas latency and throughput. To systematically benchmark thesemachine vision cameras, we propose a parameterized modelgenerator that generates end to end models of ConvolutionalNeural Networks(CNN). Using these generated models webenchmark latency and throughput of two machine visioncameras, JeVois A33 and Sipeed Maix Bit. Our experimentsdemonstrate that the selected machine vision camera and thedeep learning models can robustly verify grasp with 97% perframe accuracy. . Open Source Contributions . BayesPy – Bayesian Python . BayesPy provides tools for Bayesian inference with Python. The user constructs a model as a Bayesian network, observes data and runs posterior inference. The goal is to provide a tool which is efficient, flexible and extendable enough for expert use but also accessible for more casual users. . b-it-bots . ROS software packages of b-it-bots for different robots. . . This website is powered by fastpages. Logo and favicon was designed by Darius Dan .",
          "url": "https://deebuls.github.io/devblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://deebuls.github.io/devblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}