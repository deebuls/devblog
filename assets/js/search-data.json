{
  
    
        "post0": {
            "title": "Plotting Normal Inverse Gamma Distirbution",
            "content": ". Scipy stats doesnt have Normal Inverse Gamma distirbution. . We would like to incorporate Normal Inverse Gamma distirbution in &quot;scipy.stats&quot; package. . Learning about Normal Inverse Gamma(NIG) distribution will lead you to a plot like this from wikipedia. . . It was intruiging enough to find out how to plot this graph in python and was sure that there will be some already plots available. But to my suprise there is no blogs or docs to plot NIG in python. The closest I found was in R langugage in [1] by Frank Portman. . So I spent some time to plot NIG in python below is the snippet for it. Special thanks to Jake Vadendeplas[2] for his wonderful blogs about visualization in python. . Normal Inverse Gamma Distribution . Let the input $x$ on which its modelled be : $$ x = [ mu, sigma^2] $$ . Probability density function (PDF) . $$ f(x | delta, alpha, beta, lambda ) = sqrt{ left( frac{ lambda}{2 pi x[ sigma^2]} right)} frac{ beta^ alpha}{ Gamma( alpha)} left( frac{1}{x[ sigma^2]} right)^{( alpha + 1)} exp{ left( - frac{2 beta + lambda left(x[ mu] - delta right)^2 }{ 2 x[ sigma]^2} right)} $$ . from scipy.stats import rv_continuous from scipy.stats import norm from scipy.stats import gengamma from scipy.special import gamma from scipy.stats import expon import numpy as np %matplotlib inline import matplotlib.pyplot as plt plt.style.use(&#39;seaborn-white&#39;) class norminvgamma(): r&quot;&quot;&quot;A normal inverse gamma random variable. The mu (``mu``) keyword specifies the parmaeter mu. %(before_notes)s Notes -- The probability density function for `norminvgamma` is: .. math:: x = [ mu, sigma^2] f(x | delta, alpha, beta, lamda) = sqrt( frac{ lamda}{2 pi x[ sigma^2}]) frac{ beta^ alpha}{ gamma( alpha)} frac{1}{x[ sigma^2]}^( alpha + 1) exp(- frac{2 beta + lamda(x[ mu] - delta)^2}{2 x[ sigma^2] }) for a real number :math:`x` and for positive number :math: ` sigma^2` &gt; 0 %(after_notes)s %(example)s &quot;&quot;&quot; def __init__(self, delta, alpha, beta, lamda): self.argcheck(delta, alpha, beta, lamda) self.delta = delta self.alpha = alpha self.beta = beta self.lamda = lamda def argcheck(self, delta, alpha, beta, lamda): return (alpha &gt; 0) def rvs(self, size=1): sigma_2 = gengamma.rvs(self.alpha, self.beta, size=size) sigma_2 = np.array(sigma_2) return [[norm.rvs(self.delta, s/self.lamda), s] for s in sigma_2] def pdf(self, xmu, xsigma2): t1 = ((self.lamda)**0.5) * ((self.beta)**self.alpha) t2 = (xsigma2 * (2 * 3.15)**0.5) * gamma(self.alpha) t3 = (1 / xsigma2**2)**(self.alpha + 1) t4 = expon.pdf((2*self.beta + self.lamda*(self.delta-xmu)**2)/(2*xsigma2**2)) #print (t1, t2, t3, t4) return (t1/t2)*t3*t4 def stats(self): #ToDo return def plot(self,zoom=0.9, axs=None): steps = 50 max_sig_sq = gengamma.ppf(zoom, self.alpha, self.beta) * self.lamda #print(max_sig_sq) mu_range = np.linspace(self.delta - 1 * max_sig_sq, self.delta + 1 * max_sig_sq, num=steps) #print (mu_range[0], mu_range[-1]) sigma_range = np.linspace(0.01, max_sig_sq, num=steps) mu_grid, sigma_grid = np.meshgrid(mu_range, sigma_range) pdf_mesh = self.pdf(mu_grid, sigma_grid) if axs: contours = axs.contour(mu_grid, sigma_grid, pdf_mesh, 20, cmap=&#39;RdGy&#39;); plt.clabel(contours, inline=True, fontsize=8) #extent=[mu_range[0], mu_range[-1], sigma_range[0], sigma_range[-1]] axs.imshow(pdf_mesh, extent=[mu_range[0], mu_range[-1], sigma_range[0], sigma_range[-1]], origin=&#39;lower&#39;, cmap=&#39;Blues&#39;, alpha=0.5) axs.axis(&#39;equal&#39;) axs.set_title(&quot;(&quot;+str(self.delta)+&quot;,&quot;+str(self.alpha)+&quot;,&quot; +str(self.beta)+&quot;,&quot;+str(self.lamda)+&quot;)&quot;) #plt.colorbar(); else: assert True, &quot;Pass the axes to plot from matplotlib&quot; . Varying different range of $ alpha$ . #norminvgamma = norminvgamma_gen() fig, axs = plt.subplots(1, 3, sharey=True, figsize=(15,5)) fig.suptitle(&#39;Vertically alpha&#39;) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=1) samples = nig.rvs(size=10) nig.plot(axs=axs[0]) nig = norminvgamma(delta=0,alpha=2,beta=1, lamda=1) nig.plot(zoom=0.7, axs=axs[1]) nig = norminvgamma(delta=0,alpha=4,beta=1, lamda=1) nig.plot(zoom=0.2, axs=axs[2]) . Varying different range of $ beta$ . fig, axs = plt.subplots(1, 3, sharey=True, figsize=(15,5)) fig.suptitle(&#39;Varying beta&#39;) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=1) samples = nig.rvs(size=10) nig.plot(axs=axs[0]) nig = norminvgamma(delta=0,alpha=1,beta=2, lamda=1) nig.plot( axs=axs[1]) nig = norminvgamma(delta=0,alpha=1,beta=3, lamda=1) nig.plot(axs=axs[2]) . Varying different range of $ lambda$ . fig, axs = plt.subplots(1, 3, sharey=True, figsize=(15,5)) fig.suptitle(&#39;Verying lamda &#39;) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=1) samples = nig.rvs(size=10) nig.plot(axs=axs[0]) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=2) nig.plot( axs=axs[1]) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=4) nig.plot(axs=axs[2]) . References . https://frankportman.github.io/bayesAB/reference/plotNormalInvGamma.html | https://jakevdp.github.io/PythonDataScienceHandbook/04.04-density-and-contour-plots.html |",
            "url": "https://deebuls.github.io/devblog/probability/python/plotting/matplotlib/2020/05/19/probability-normalinversegamma.html",
            "relUrl": "/probability/python/plotting/matplotlib/2020/05/19/probability-normalinversegamma.html",
            "date": " • May 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Likelihood",
            "content": "What is Likelihood? . Likelihood and probablity seems to be same word in the layman domain, but in the stats domain they are different. . In the stats domain the likelihood or likelihood function is a measurement. It measures the distance between a statistical model and the input data. . What is a statistical model? . The diferent probability distributions available. For example, Gausian, gama, beta distribution, exponential for continuous data while Bernoulli, Dirichlet, multinomila distributions for discrete data. . How are statistical models represented? . By their parameters. For example for gaussian distribution the parameters are $ mu$ and $ sigma$ . . How do we select the statictical model? . Depends on many factors. This is the main decision to be made while designing a statistical model based learning. The different factors include: what is the data type: Continuous or discrete? Is it symmetrical or asymetrical? . Domain of the data, binary, real, etc | Does it decay or increase? | . . . etc | . A complete knowledge about the type data and the type of distribution is required to make the appropriate decision. . Common Probability distribution . Data Type Domain Distribution Python (numpy.random) Parameters . univariate, discrete, binary | $$ x in {0,1 } $$ | Bernoulli | binomial(1, p) | $$ p in[0,1]$$ | . univariate, discrete, multivalued | $$ x in { 1,2, dots, K }$$ | multinomial | multinomial(n, pvals) | $$pvals = [p_1, dots , p_k] $$ $$ sum_{i=1}^{K} p_i = 1 $$ | . univariate, continuous, unbounded | $$ x in mathbb{R} $$ | normal | normal(mu, sigma) | $$ mu in mathbb{R} $$ $$ sigma in mathbb{R}$$ | . #Lets make some distributions and find the likelihood to some data import numpy as np import matplotlib.pyplot as plt number_of_samples = 20; #parameters ; sample data from distribution (continuous data) mu, sigma = 12, 0.1 ; univariate_gaussian_samples = np.random.normal(mu, sigma, number_of_samples) mean = [0, 0]; cov = [[1, 0], [0, 100]]; multivariate_gaussian_samples = np.random.multivariate_normal(mean, cov, number_of_samples) #parameters ; sample data from distribution (discreta data) p = 0.8 ; bernoulli_samples = np.random.binomial(1, p, number_of_samples) pvals = [0.2, 0.6, 0.2] ; multinomial_samples = np.random.multinomial(number_of_samples, pvals) alpha, beta = 10, 20 ; beta_samples = np.random.beta(alpha, beta, number_of_samples) alpha = [10,20,10,90] ; dirchilet_samples = np.random.dirichlet(alpha, number_of_samples) . Goal of Likelihood . The goal of likelihood would be given the samples as shown above (beta_samples, dirichlet_samples etc) find the parameters of the corresponding distribution ((alpha, beta), alphas respectively) . Lets look into this process in the comming post . #hide .",
            "url": "https://deebuls.github.io/devblog/probability/python/2020/03/20/probability-likelihood.html",
            "relUrl": "/probability/python/2020/03/20/probability-likelihood.html",
            "date": " • Mar 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I’m a researcher at the Bonn-Aachen International Center for Information Technology (b-it) at the Autonomous Systems group in Bonn, Germany. . My research focus is on developing strategies for safe and reliable incorporation of deep learning methods into robotics. . I am also the lead develper with the b-it-bots team which won the RoboCup WorldCup and GermanOpen in the work league. . Publications . b-it-bots: Our Approach for Autonomous Robotics in Industrial Environments . Deebul Nair, Santosh Thoduka, Iman Awaad, Sven Schneider, Paul G. Plöger, Gerhard K. Kraetzschmar and students . Robot World Cup. Springer, Cham, 2019. . This paper presents the approach of our team, b-it-bots, in the RoboCup@Work competition which resulted in us winning the World Championship in Sydney in 2019. We describe our current hardware, including modifications made to the KUKA youBot, the underlying software framework and components developed for navigation, manipulation, perception and task planning for scenarios in industrial environments. Our combined 2D and 3D approach for object recognition has improved robustness and performance compared to previous years, and our task planning framework has moved us away from large state machines for high-level control. Future work includes closing the perception-manipulation loop for more robust grasping. Our open-source repository is available at https://github.com/b-it-bots/mas_industrial_robotics. . Performance Evaluation of Low-Cost Machine Vision Cameras forImage-Based Grasp Verification . Performance Evaluation of Low-Cost Machine Vision Cameras forImage-Based Grasp Verification . Under review in IROS 2020 . Grasp verification is advantageous for au-tonomous manipulation robots as they provide the feedbackrequired for higher level planning components about successfultask completion. However, a major obstacle in doing graspverification is sensor selection. In this paper, we propose a visionbased grasp verification system using machine vision cameras,with the verification problem formulated as an image classifi-cation task. Machine vision cameras consist of a camera anda processing unit capable of on-board deep learning inference.The inference in these low-power hardware are done near thedata source, reducing the robots dependence on a centralizedserver, leading to reduced latency, and improved reliability.Machine vision cameras provide the deep learning inferencecapabilities using different neural accelerators. Although, it isnot clear from the documentation of these cameras what is theeffect of these neural accelerators on performance metrics suchas latency and throughput. To systematically benchmark thesemachine vision cameras, we propose a parameterized modelgenerator that generates end to end models of ConvolutionalNeural Networks(CNN). Using these generated models webenchmark latency and throughput of two machine visioncameras, JeVois A33 and Sipeed Maix Bit. Our experimentsdemonstrate that the selected machine vision camera and thedeep learning models can robustly verify grasp with 97% perframe accuracy. . Open Source Contributions . BayesPy – Bayesian Python . BayesPy provides tools for Bayesian inference with Python. The user constructs a model as a Bayesian network, observes data and runs posterior inference. The goal is to provide a tool which is efficient, flexible and extendable enough for expert use but also accessible for more casual users. . b-it-bots . ROS software packages of b-it-bots for different robots. . . This website is powered by fastpages. Logo and favicon was designed by Darius Dan .",
          "url": "https://deebuls.github.io/devblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://deebuls.github.io/devblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}