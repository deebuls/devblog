{
  
    
        "post0": {
            "title": "Using uncertainties in DNN application",
            "content": "Application of Uncertainty Estimation in DNN . Objective . We would like to look into two papers which have developed methodologies to use the uncertatiny estimated by the deep neural network(DNN) in their system. The goal is to find the pros and cons of these methodologies . Paper 1: Benchmarking uncertatinty estimation methods in Deep learning with Safety-Related Metrics . In this paper they propose two new Safety-Related Metrics: Remaining Error Rate(RER) and Remaining Accuracy Rate(Rate). Here is definition as per the paper: . A system which relies on these estimates is expected to be in functional mode if the predictions are certain and in fall-back/mitigation mode if the prediction is uncertain. However, the most critical result regarding safety are the predictions where the model is certain about its prediction but incorrect (CI). We call the ratio of the number of certain but incorrect samples to all samples the Remaining Error Rate (RER). For minimizing the overall risk, it needs to be as low as possible. Nonetheless, if a model would always give a low confidence as output, the system would constantly remain in fall-back mode and will be unable to provide the intended functionality. Therefore, the ratio of the number of certain and correct samples to all samples - we call it the Remaining Accuracy Rate (RAR) - needs to be as high as possible to stay in performance mode for most of the time. . Interpretation . The predictions from the DNN are classified into 4 sections as below (Table 1 from paper) .   Certain Uncertain . Correct | CC | UC | . Incorrect | CI | UI | . The definition of the metrics are : . $RER = frac{CI}{CC+CI+UC+UI}$ $RAR = frac{CC}{CC+CI+UC+UI}$ . Pros . Its a very simple metric. | Simplicity of metric is a very important thing for usability of metrics. | . Cons . A minor issue will be on the threshold which seprates Certain vs Uncertain. Is it 99% or 90% etc. All will yield different results | A major problem which we consider is the assumption in which the uncertatiny is being planned to be used in the system. A system which relies on these estimates is expected to be in functional mode if the predictions are certain and in fall-back/mitigation mode if the prediction is uncertain. . | This means that the system has 2 modes A functional mode | A fall-back/mitigation mode | . | Is this a safe assumption with regards to deployment of DNN? | Can an application deploying DNN have 2 modes ? | What should an autonomous car in fall-back mode do ? | . :bangbang: | The assumption on how a system uses uncertatiny is that the system has 2 modes functional and fall-back | :-: | :- | . Paper 2 : Fail-Safe Execution of Deep learning based Systems through Uncertatiny Monitoring . In this paper, as the title suggests they create a separate model called the Supervisor Model which will monitor the uncertainty of Deep learning and avoid any faults in the system | What is a supervisor model : Network supervision can be viewed as a binary classification task: malicious samples, i.e., inputs which lead to a misclassification (for classification problems) or to severe imprecision (in regression problems) are positive samples that have to be rejected. Other samples, also called benign samples, are negative samples in the binary classification task. An uncertainty based supervisor accepts an input i as a benign sample if its uncertainty u(i) is lower than some threshold t. The choice of t is a crucial setting, as a high t will fail to reject many malicious samples (false negatives) and a low t will cause too many false alerts (false positives). . | Thus the supervisor is a binary classification task to avoid beningn samples. They also define a metric S-Score which combined measures the performance of both the model and the supervisor model | There is lot of similarity with respect to the above paper here also | . Pros . They have made a library out of it such that any model can be used. | The threshold on which to make the decission is now being learned by the data. | . Cons . Again, these method is based on the assumption that the system which uses DNN has 2 modes of operation( normal mode and fall-back mode) | . :bangbang: | The same assumption on how a system uses uncertatiny, that the system has 2 modes functional and fall-back | :—: | :— | . Conclusion . All methods are based on the assumption that the system has 2 modes of operation | The uncertatiny estimation is used to determine whethere the DNN output should be trusted or should be avoided | . This is not enough . The methods which use DNN dont have a fall back mode. If there was an non DNN based method then by “First rule of Machine/Deep Learning” that will be used for solving the problem | . | There can be argument to say that there are redundant DNN systems and this method can be used to kick-off redundant system Even this argument is not valid as if you have redundant system, you should use all of them and make a decision | . | . Solutions . The one solution which I have been workin is about not binarizing the probability but the propagating it through the system | The best example is of the filters which have been developed over years to handle uncertain sensors. | . References . [1]M. Weiss and P. Tonella, “Fail-Safe Execution of Deep Learning based Systems through Uncertainty Monitoring,” arXiv:2102.00902 [cs], Feb. 2021, Accessed: Apr. 13, 2021. [Online]. Available: http://arxiv.org/abs/2102.00902. . | [2]M. Henne, A. Schwaiger, K. Roscher, and G. Weiss, “Benchmarking Uncertainty Estimation Methods for Deep Learning With Safety-Related Metrics,” p. 8, 2020. . | .",
            "url": "https://deebuls.github.io/devblog/uncertainty/2021/04/13/how-to-use-uncertainty-estimate-from-dnn.html",
            "relUrl": "/uncertainty/2021/04/13/how-to-use-uncertainty-estimate-from-dnn.html",
            "date": " • Apr 13, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "One-vs-All Classifier",
            "content": ". The paper . Padhy, S. et al. (2020) ‘Revisiting One-vs-All Classifiers for Predictive Uncertainty and Out-of-Distribution Detection in Neural Networks’, arXiv:2007.05134 [cs, stat]. Available at: http://arxiv.org/abs/2007.05134 (Accessed: 26 January 2021). . Abstract . Problem . Out-of-Distribution - finding on which samples the classifier should not predict | OOD using uncertainty estimate | uncertainty estimate - correctly estimate the confidence (uncertainty) in the prediction | . How . Using oves-vs-all classifier | distance-based logit representation to encode uncertainty | . Introduction . Capturing epistemic uncertainty is more important, as it captures model&#39;s lack of knowledge about data . | Three different paradigms to measure uncertainty . in-distribution calibration of models measured on an i.i.d. test set, | robustness under dataset shift, and | anomaly/out-of-distribution (OOD) detection, which measures the ability of models to assign low confidence predictions on inputs far away from the training data. | | . Unique selling points . we first study the contribution of the loss function used during training to the quality of predictive uncertainty of models. | Specifically, we show why the parametrization of the probabilities underlying softmax cross-entropy loss are ill-suited for uncertainty estimation. | We then propose two simple replacements to the parametrization of the probability distribution: a one-vs-all normalization scheme that does not force all points in the input space to map to one of the classes, thereby naturally capturing the notion of “none of the above”, and | a distance-based logit representation to encode uncertainty as a function of distance to the training manifold. | | . Experiments . Under Dataset Shift . CIFAR 10 Corrupted different intensity | CIFAR 100 Corrupted | . | OOD . CIFAR 100 vs CIFAR10 | CIFAR100 vs SVHN | . | Comparison of learned class centers . | Reliability Diagrams . | Imagenet . | CLINC Intent Classification Dataset . |",
            "url": "https://deebuls.github.io/devblog/probability/regression/uncertainty/robustness/2021/01/01/one-vs-all-classifier-revisiting.html",
            "relUrl": "/probability/regression/uncertainty/robustness/2021/01/01/one-vs-all-classifier-revisiting.html",
            "date": " • Jan 1, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "DNN Wiki",
            "content": "What to learn? . I wanted to develop glossary/Wiki of DNN related topics and my explanation of them so that I can be sure that I know the topics. But for that I needed a list of relevant topics. DNN is an exponentially exploding field and with low signa to noise ratio. So it becomes really difficult in fooling up with the new work without having a firm understanding of what is firm knowledge. This selection of topics should help any new commer to be sure that if I know what these topics are then you can claim that you know a little about deep learning. . Deep Learning vs Other Machine Learning Approaches | The Essential Math of Artificial Neurons | The Essential Math of Neural Networks | Activation Functions | Cost/Loss Functions | Stochastic Gradient Descent | Backpropagation | Mini-Batches | Learning Rate | Optimizers (e.g., Adam, Nadam) | Glorot/He Weight Initialization | Dense Layers | Softmax Layers | Dropout | Data Augmentation | . References . https://aiplus.odsc.com/courses/deep-learning-with-tensorflow-2-and-pytorch-1 | |",
            "url": "https://deebuls.github.io/devblog/learning/2020/07/22/Learning-DNN.html",
            "relUrl": "/learning/2020/07/22/Learning-DNN.html",
            "date": " • Jul 22, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Different types of uncertainty in DNN systems?",
            "content": "What are the uncertainties in DNN? . This question is easily answered if you have read a little bit of DNN literature. The most popular answer is . Aleatoric uncertainty | Epistemic uncertainty | . An additional statement is always added to these different types of uncertainty which will claim that . Aleatoric uncertatinty is model uncertainty | Epistemic uncertatinty is data uncertainty | . But what actually are these values and what do they explain ? Lets do a brief overview of different literature to answer these questions. . Alternative definitions . “Aleatory uncertainty is also referred to in the literature as variability, irreducible uncertainty, inherent uncertainty, and stochastic uncertainty. Epistemic uncertainty is also termed reducible uncertainty, subjective uncertainty, and state-of-knowledge uncertainty.” [2] . “Aleatory uncertainties are described as arising from inherent variabilities or randomness in systems, whereas epistemic uncertainties are due to imperfect knowledge.” [1] . Important points . Epistemic sources of uncertatinty is reducible but aleatory uncertainty is not reducible[1] | References . [1] Probability is Perfect, but we Can’t Elicitit Perfectly Anthony O’Hagan &amp; Jeremy E. Oakley http://www.yaroslavvb.com/papers/epistemic.pdf . [2] Challenge problems: uncertainty in system response given uncertain parameters Author links open overlay panelWilliam L.Oberkam https://www.sciencedirect.com/science/article/pii/S0951832004000493 .",
            "url": "https://deebuls.github.io/devblog/uncertainty/2020/05/29/Explanation-DNN-Uncertainty.html",
            "relUrl": "/uncertainty/2020/05/29/Explanation-DNN-Uncertainty.html",
            "date": " • May 29, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Regression Uncertainty",
            "content": "Regression . In machine learning problems we take input data $x$ and use it to predict the output data $y$. If the predicted data is continuous or discrete. When the output data $y$ is continuous then the problem is called regression while if its is discrete then the problem is called classification . . Deep neural networks have prooved to be a good tool for solving regression tasks. Currently there are new improvements in making the regression task robust by not only predicting the output data $y$ but also the corresponding uncertainty. . In this blog we will look into 2 methods of representing uncertatinty from literature. . Epistemic uncertatinty (Data uncertatinty) [1] | Aleotary uncertainty (Model uncertatinty) [2] | . Hierarchical Modelling . In regression task can be modelled in 3 different methods based on which all uncertainty needs to be accounted for during architecture of the deep network. The different layers and their hierarchichal structre is represented in the image above. . Name loss function uncertatinty output distribution . Deterministic Regression | mean square error | not modeled | $ hat{y}$ | - | . Likelihood Regression | Maximum likelihood estimation | data | $ hat{y}$, $ sigma^2$ | Normal | . Evidential Regression | Maximum likelihood estimation | data + model | $ hat{y}$,$ alpha$, $ beta$, $ lambda$ | Normal inverse gamma | . Dataset . We will start by creating a one dimension synthetic regression dataset. The synthetic dataset has added priori noise. The dataset is generated from $$ y = frac{ sin(3x)}{3x} + epsilon $$ where $ epsilon sim mathcal{N}(0, 0.02)$ . The training dataset consist of $x$ values between $-3 leq x leq 3$; while the test dataset consist of values outside of training $-4 leq x leq 4$ . Model . We will be using a common model for all the different learning formulation. . # this is one way to define a network class Net(torch.nn.Module): def __init__(self, n_feature, n_hidden, n_output): super(Net, self).__init__() self.hidden = torch.nn.Linear(n_feature, n_hidden) # hidden layer self.hidden_2 = torch.nn.Linear(n_hidden, n_hidden) self.predict = torch.nn.Linear(n_hidden, n_output) # output layer def forward(self, x): x = self.hidden(x) x = F.relu(self.hidden_2(x)) # activation function for hidden layer x = self.predict(x) # linear output return x . Deterministic Regression . Loss function . Mean square error $$ mathcal{L} = frac{1}{2} vert vert y_i - hat{y} vert vert ^2$$ . Output . The model will have single output $ hat{y}$ . msenet = Net(n_feature=1, n_hidden=10, n_output=1) # define the network print(msenet) # net architecture #optimizer = torch.optim.SGD(net.parameters(), lr=0.01) optimizer = torch.optim.Adam(msenet.parameters(), lr=0.001) loss_func = torch.nn.MSELoss() # this is for regression mean squared loss # train the network for t in range(10000): prediction = msenet(x) # input x and predict based on x loss = loss_func(prediction, y) # must be (1. nn output, 2. target) optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if t%1000 == 0: print(loss) prediction = msenet(test_x) plot_prediction(test_x, test_y, prediction) . Net( (hidden): Linear(in_features=1, out_features=10, bias=True) (hidden_2): Linear(in_features=10, out_features=10, bias=True) (predict): Linear(in_features=10, out_features=1, bias=True) ) tensor(0.5153, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0478, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0023, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;) . Likelihood Regression . Loss function . In here we will use the maximum likelihood estimation error, which is also called as the Negative log likelihood. . $$ mathcal{L} = - left(-0.5 log(2 pi) - 0.5 sum log( sigma^2) - 0.5 sum frac{(y - hat{y})^2}{ sigma^2} right) $$ . Output . We will create a deep net with 2 output variables. . predicted output $ hat{y}$ | the variance of the normal distribution $ sigma^2$ | . Note . We aer also adding a regularizer $ frac{ | y - hat{y} | }{ sigma^2}$. This helps to regluarize the learning. It is inversely related to $ sigma^2$ and it scales based on distance from the actual value. . The regularizer ensures that values predicted far from true values have higher variance.So for values near to the predicted it gives less loss but values far away from prediction gives a large loss. . # Loss Function class MLELoss(torch.nn.Module): def __init__(self, weight=None, size_average=True): super(MLELoss, self).__init__() def forward(self, inputs, targets, smooth=1): targets = targets.view(-1) #converting variable to single dimension mu = inputs[:,0].view(-1) #extracting mu and sigma_2 logsigma_2 = inputs[:,1].view(-1) #logsigma and exp drives the variable to positive values always sigma_2 = torch.exp(logsigma_2) kl_divergence = (targets - mu)**2/sigma_2 #Regularizer mse = -0.5 * torch.sum(((targets - mu)**2)/sigma_2) sigma_trace = -0.5 * torch.sum(sigma_2) log2pi = -0.5 * np.log(2 * np.pi) J = mse + sigma_trace + log2pi loss = -J + kl_divergence.sum() return loss mlenet = Net(n_feature=1, n_hidden=10, n_output=2) # define the network print(mlenet) # net architecture optimizer = torch.optim.Adam(mlenet.parameters(), lr=0.001) loss_func = MLELoss() # this is for regression mean squared loss # train the network for t in range(10000): prediction = mlenet(x) # input x and predict based on x loss = loss_func(prediction, y) # must be (1. nn output, 2. target) optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if t%1000 == 0: print(loss) prediction = mlenet(test_x) mu = prediction[:,0] sigma2 = torch.exp(prediction[:,1]) sigma = torch.sqrt(sigma2) plot_prediction(test_x, test_y, mu, sigma) . Net( (hidden): Linear(in_features=1, out_features=10, bias=True) (hidden_2): Linear(in_features=10, out_features=10, bias=True) (predict): Linear(in_features=10, out_features=2, bias=True) ) tensor(68.1900, grad_fn=&lt;AddBackward0&gt;) tensor(6.4617, grad_fn=&lt;AddBackward0&gt;) tensor(6.1683, grad_fn=&lt;AddBackward0&gt;) tensor(6.1068, grad_fn=&lt;AddBackward0&gt;) tensor(6.0325, grad_fn=&lt;AddBackward0&gt;) tensor(5.9853, grad_fn=&lt;AddBackward0&gt;) tensor(5.9657, grad_fn=&lt;AddBackward0&gt;) tensor(5.9485, grad_fn=&lt;AddBackward0&gt;) tensor(5.9929, grad_fn=&lt;AddBackward0&gt;) tensor(5.9366, grad_fn=&lt;AddBackward0&gt;) . Evidential Regression . Evidential regression is based on paper [2] (Amini &amp; e.t.al, 2019), which is based on the ideas of [3, 4] that if we represent the output of the model with a higher order data distribution its possible to model the data and model uncertainties. . Loss . $$ mathcal{L} = left( frac{ Gamma( alpha - 0.5)}{4 Gamma( alpha) lambda sqrt beta} right) left( 2 beta(1 + lambda) + (2 alpha -1) lambda(y_i - hat{y})^2 right)$$ . output . The model has 4 outputs . $ hat{y}$ | $ alpha$ | $ beta$ | $ lambda$ | . Regularizer . Regularizer is required to penalize the loss function for OOD predictions. . $$ | y - hat{y} | ^2 (2 alpha + lambda)$$ . class EvidentialLoss(torch.nn.Module): def __init__(self, weight=None, size_average=True): super(EvidentialLoss, self).__init__() def forward(self, inputs, targets, smooth=1): targets = targets.view(-1) y = inputs[:,0].view(-1) #first column is mu,delta, predicted value loga = inputs[:,1].view(-1) #alpha logb = inputs[:,2].view(-1) #beta logl = inputs[:,3].view(-1) #lamda a = torch.exp(loga) b = torch.exp(logb) l = torch.exp(logl) term1 = (torch.exp(torch.lgamma(a - 0.5)))/(4 * torch.exp(torch.lgamma(a)) * l * torch.sqrt(b)) #print(&quot;term1 :&quot;, term1) term2 = 2 * b *(1 + l) + (2*a - 1)*l*(y - targets)**2 #print(&quot;term2 :&quot;, term2) J = term1 * term2 #print(&quot;J :&quot;, J) Kl_divergence = torch.abs(y - targets) * (2*a + l) #Kl_divergence = ((y - targets)**2) * (2*a + l) #print (&quot;KL &quot;,Kl_divergence.data.numpy()) loss = J + Kl_divergence #print (&quot;loss :&quot;, loss) return loss.mean() evnet = Net(n_feature=1, n_hidden=10, n_output=4) # define the network print(evnet) # net architecture optimizer = torch.optim.Adam(evnet.parameters(), lr=0.001) loss_func = EvidentialLoss() # train the network for t in range(20000): prediction = evnet(x) # input x and predict based on x loss = loss_func(prediction, y) # must be (1. nn output, 2. target) optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if t%10000 == 0: print(loss) prediction = evnet(test_x) mu = prediction[:,0].view(-1) #first column is mu,delta, predicted value a = prediction[:,1].view(-1) #alpha b = prediction[:,2].view(-1) #beta l = prediction[:,3].view(-1) #lamda a = torch.exp(a); b = torch.exp(b); l = torch.exp(l) var = b / ((a -1)*l) #epistemic/ model/prediciton uncertaitnty e = b / (a - 1) # aleatoric uncertainty/ data uncertainty plot_prediction(test_x, test_y, mu, var, e) . Net( (hidden): Linear(in_features=1, out_features=10, bias=True) (hidden_2): Linear(in_features=10, out_features=10, bias=True) (predict): Linear(in_features=10, out_features=4, bias=True) ) tensor(3.1019, grad_fn=&lt;MeanBackward0&gt;) tensor(0.1492, grad_fn=&lt;MeanBackward0&gt;) . Conclusions . Loss functions of maximum likelihood and evidence have been implemented from formulas from the paper. . Important: Leasons Learned - how to treat loss function when one of the output variable is always positive. (log and exponential to rescue) -The difference between optimize and loss function and do you sum and add constant ones or do you add constant in all equation and sum. | The trianing is working on some runs not all. | The model learns the function easily but learning the uncertainty in unkonw region takes longer number of iterations. Needs to be further investigated. . Note: ToDo : Test functions for the loss function written . Note: ToDo : What is stopping condition? when to stop learning? . Note: ToDo : Replace the regularizer in regularizer in likelihood loss with actual regularizer. . Warning: Why is uncertatiny different for both the models. Evidential model seems to be very confident in the known region, which seems to be fishy . Warning: The loss function doesnt give same results all the time, so needs to be further investigated | . References . [1] Kendall, Alex, and Yarin Gal. “What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?” ArXiv:1703.04977 [Cs], October 5, 2017. http://arxiv.org/abs/1703.04977. . [2] Amini, Alexander, Wilko Schwarting, Ava Soleimany, and Daniela Rus. “Deep Evidential Regression.” ArXiv:1910.02600 [Cs, Stat], October 7, 2019. http://arxiv.org/abs/1910.02600. . [3] Sensoy, Murat, Lance Kaplan, and Melih Kandemir. “Evidential Deep Learning to Quantify Classification Uncertainty,” n.d., 11. . [4] Malinin, Andrey, and Mark Gales. &quot;Predictive uncertainty estimation via prior networks.&quot; Advances in Neural Information Processing Systems. 2018. .",
            "url": "https://deebuls.github.io/devblog/probability/regression/uncertainty/2020/05/27/uncertainty-regression.html",
            "relUrl": "/probability/regression/uncertainty/2020/05/27/uncertainty-regression.html",
            "date": " • May 27, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Dataset shift",
            "content": "Dataset shift . Dataset shift is still an unsolved problems when it comes to deploying learning models “in the wild”. . There are 2 different categories of dataset shift . Co-variate shift | label shift | . Let $ mathbb{S} $ be the source data distribution and $ mathbb{T} $ be the target data distribution. If we denote the input variables as x and output variables as y, then . Covariate shift . s(x)≠t(x)s(x) neq t(x)s(x)​=t(x) input distribution of both source and target are different . but . s(y∣x)=t(y∣x)s(y|x) = t(y|x)s(y∣x)=t(y∣x) conditional output distirbution is invariant to dataset shift. . Label shift . s(y)≠t(y)s(y) neq t(y)s(y)​=t(y) output distribution of both source and target are different . but . s(x∣y)=t(x∣y)s(x|y) = t(x|y)s(x∣y)=t(x∣y) conditional input distirbution is invariant to dataset shift .   Covariate Shift Label Shift . input distribution | $s(x) neq t(x)$ | $?$ | . output distribution | $?$ | $s(y) neq t(y)$ | . conditional output distribution | $s(y vert x) = t(y vert x)$ | $?$ | . conditional input Distribution | $?$ | ${s(x vert y) = t(x vert y)}$ | . Examples . ToDo . Simluated Dataset {ReDo with examples} . The problem can be simulated in image based calssification dataset like MNIST and CIFAR. . Tweak-One shift . refers to the case where we set a class to have probability $ p &gt; 0.1$, while the distribution over the rest of the classes is uniform. . Minority-Class Shiftis . A more general version of Tweak-One shift, where a fixed number of classes to have probability $p &lt; 0.1$, while the distribution over the rest of the classes isuniform. . Dirichlet shift . we draw a probability vector $p$ from the Dirichlet distribution with concentration parameter set to $ alpha$ for all classes, before including sample points which correspond to the multinomial label variable according top. .",
            "url": "https://deebuls.github.io/devblog/dataset/2020/05/21/Explanation-Learning-DatasetShift.html",
            "relUrl": "/dataset/2020/05/21/Explanation-Learning-DatasetShift.html",
            "date": " • May 21, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Plotting Normal Inverse Gamma Distirbution",
            "content": ". Scipy stats doesnt have Normal Inverse Gamma distirbution. . We would like to incorporate Normal Inverse Gamma distirbution in &quot;scipy.stats&quot; package. . Learning about Normal Inverse Gamma(NIG) distribution will lead you to a plot like this from wikipedia. . . It was intruiging enough to find out how to plot this graph in python and was sure that there will be some already plots available. But to my suprise there is no blogs or docs to plot NIG in python. The closest I found was in R langugage in [1] by Frank Portman. . So I spent some time to plot NIG in python below is the snippet for it. Special thanks to Jake Vadendeplas[2] for his wonderful blogs about visualization in python. . Normal Inverse Gamma Distribution . Let the input $x$ on which its modelled be : $$ x = [ mu, sigma^2] $$ . Probability density function (PDF) . $$ f(x | delta, alpha, beta, lambda ) = sqrt{ left( frac{ lambda}{2 pi x[ sigma^2]} right)} frac{ beta^ alpha}{ Gamma( alpha)} left( frac{1}{x[ sigma^2]} right)^{( alpha + 1)} exp{ left( - frac{2 beta + lambda left(x[ mu] - delta right)^2 }{ 2 x[ sigma]^2} right)} $$ . from scipy.stats import rv_continuous from scipy.stats import norm from scipy.stats import gengamma from scipy.special import gamma from scipy.stats import expon import numpy as np %matplotlib inline import matplotlib.pyplot as plt plt.style.use(&#39;seaborn-white&#39;) class norminvgamma(): r&quot;&quot;&quot;A normal inverse gamma random variable. The mu (``mu``) keyword specifies the parmaeter mu. %(before_notes)s Notes -- The probability density function for `norminvgamma` is: .. math:: x = [ mu, sigma^2] f(x | delta, alpha, beta, lamda) = sqrt( frac{ lamda}{2 pi x[ sigma^2}]) frac{ beta^ alpha}{ gamma( alpha)} frac{1}{x[ sigma^2]}^( alpha + 1) exp(- frac{2 beta + lamda(x[ mu] - delta)^2}{2 x[ sigma^2] }) for a real number :math:`x` and for positive number :math: ` sigma^2` &gt; 0 %(after_notes)s %(example)s &quot;&quot;&quot; def __init__(self, delta, alpha, beta, lamda): self.argcheck(delta, alpha, beta, lamda) self.delta = delta self.alpha = alpha self.beta = beta self.lamda = lamda def argcheck(self, delta, alpha, beta, lamda): return (alpha &gt; 0) def rvs(self, size=1): sigma_2 = gengamma.rvs(self.alpha, self.beta, size=size) sigma_2 = np.array(sigma_2) return [[norm.rvs(self.delta, s/self.lamda), s] for s in sigma_2] def pdf(self, xmu, xsigma2): t1 = ((self.lamda)**0.5) * ((self.beta)**self.alpha) t2 = (xsigma2 * (2 * 3.15)**0.5) * gamma(self.alpha) t3 = (1 / xsigma2**2)**(self.alpha + 1) t4 = expon.pdf((2*self.beta + self.lamda*(self.delta-xmu)**2)/(2*xsigma2**2)) #print (t1, t2, t3, t4) return (t1/t2)*t3*t4 def stats(self): #ToDo return def plot(self,zoom=0.9, axs=None): steps = 50 max_sig_sq = gengamma.ppf(zoom, self.alpha, self.beta) * self.lamda #print(max_sig_sq) mu_range = np.linspace(self.delta - 1 * max_sig_sq, self.delta + 1 * max_sig_sq, num=steps) #print (mu_range[0], mu_range[-1]) sigma_range = np.linspace(0.01, max_sig_sq, num=steps) mu_grid, sigma_grid = np.meshgrid(mu_range, sigma_range) pdf_mesh = self.pdf(mu_grid, sigma_grid) if axs: contours = axs.contour(mu_grid, sigma_grid, pdf_mesh, 20, cmap=&#39;RdGy&#39;); plt.clabel(contours, inline=True, fontsize=8) #extent=[mu_range[0], mu_range[-1], sigma_range[0], sigma_range[-1]] axs.imshow(pdf_mesh, extent=[mu_range[0], mu_range[-1], sigma_range[0], sigma_range[-1]], origin=&#39;lower&#39;, cmap=&#39;Blues&#39;, alpha=0.5) axs.axis(&#39;equal&#39;) axs.set_title(&quot;(&quot;+str(self.delta)+&quot;,&quot;+str(self.alpha)+&quot;,&quot; +str(self.beta)+&quot;,&quot;+str(self.lamda)+&quot;)&quot;) #plt.colorbar(); else: assert True, &quot;Pass the axes to plot from matplotlib&quot; . Varying different range of $ alpha$ . #norminvgamma = norminvgamma_gen() fig, axs = plt.subplots(1, 3, sharey=True, figsize=(15,5)) fig.suptitle(&#39;Vertically alpha&#39;) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=1) samples = nig.rvs(size=10) nig.plot(axs=axs[0]) nig = norminvgamma(delta=0,alpha=2,beta=1, lamda=1) nig.plot(zoom=0.7, axs=axs[1]) nig = norminvgamma(delta=0,alpha=4,beta=1, lamda=1) nig.plot(zoom=0.2, axs=axs[2]) . Varying different range of $ beta$ . fig, axs = plt.subplots(1, 3, sharey=True, figsize=(15,5)) fig.suptitle(&#39;Varying beta&#39;) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=1) samples = nig.rvs(size=10) nig.plot(axs=axs[0]) nig = norminvgamma(delta=0,alpha=1,beta=2, lamda=1) nig.plot( axs=axs[1]) nig = norminvgamma(delta=0,alpha=1,beta=3, lamda=1) nig.plot(axs=axs[2]) . Varying different range of $ lambda$ . fig, axs = plt.subplots(1, 3, sharey=True, figsize=(15,5)) fig.suptitle(&#39;Verying lamda &#39;) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=1) samples = nig.rvs(size=10) nig.plot(axs=axs[0]) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=2) nig.plot( axs=axs[1]) nig = norminvgamma(delta=0,alpha=1,beta=1, lamda=4) nig.plot(axs=axs[2]) . References . https://frankportman.github.io/bayesAB/reference/plotNormalInvGamma.html | https://jakevdp.github.io/PythonDataScienceHandbook/04.04-density-and-contour-plots.html |",
            "url": "https://deebuls.github.io/devblog/probability/python/plotting/matplotlib/2020/05/19/probability-normalinversegamma.html",
            "relUrl": "/probability/python/plotting/matplotlib/2020/05/19/probability-normalinversegamma.html",
            "date": " • May 19, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Regression Uncertainty Robustness",
            "content": "Robustness . Robustness in learning is the capacity of the network to handle corrupted data (training data or test data). . Robustness during training vs testing . Training time robustness is capability of the model to overcome corrupted training data(input or labels). . Testing time robustness is the capability of the model to corrupted data during testing eg. Adversarial test. . Objective . The objective of this blog is to understand the robustness capability of different loss fucntions with respect to training time dataset corruption. . The loss functions in considerations are: . Gaussian negative log likelihood | Laplace negative log likelihood | Cauchy negative log likelihood | Data . Lets start with plotting our toy dataset. . As you can see the dataset has some corrupted labels which should not be considered while learning. . fig, ax = plt.subplots(figsize=(5,5)) ax.scatter(x.data.numpy(),y.data.numpy()) ax.axis(&#39;equal&#39;) ax.set_xlabel(&#39;$x$&#39;) ax.set_ylabel(&#39;$y$&#39;) . Text(0, 0.5, &#39;$y$&#39;) . Model . We will train a simple linear model . # this is one way to define a network class Net(torch.nn.Module): def __init__(self, n_feature, n_hidden, n_output): super(Net, self).__init__() self.hidden = torch.nn.Linear(n_feature, n_hidden) # hidden layer self.predict = torch.nn.Linear(n_hidden, n_output) # output layer def forward(self, x): x = F.relu(self.hidden(x)) # activation function for hidden layer x = self.predict(x) # linear output return x . Mean Square Loss robustness analysis . loss_func = torch.nn.MSELoss() # this is for regression mean squared loss # Fit a linear regression using mean squared error. regression = Net(n_feature=1, n_hidden=1, n_output=1) # RegressionModel() params = regression.parameters() optimizer = torch.optim.Adam(params, lr = 0.001) . Analysis MSE Regression . Gaussian Loss Robustness analysis . For gaussian loss the model in addition to the prediction also predicts the confidence in the prediction in the form of the gaussian variance. . # this is one way to define a network class GaussianNet(torch.nn.Module): def __init__(self, n_feature, n_hidden, n_output): super(GaussianNet, self).__init__() self.hidden = torch.nn.Linear(n_feature, n_hidden) # hidden layer self.predict = torch.nn.Linear(n_hidden, n_output) # output layer self.variance = torch.nn.Linear(n_hidden, 1) # variance layer #torch.nn.init.xavier_uniform_(self.variance.weight) #torch.nn.init.normal_(self.variance.weight, mean=1.0) #torch.nn.init.normal_(self.variance.bias, mean=0.0) def forward(self, x): x = F.relu(self.hidden(x)) # activation function for hidden layer out = self.predict(x) # linear output var = F.softplus(self.variance(x)) return out, var . loss_func = torch.nn.GaussianNLLLoss( reduction=&#39;none&#39;) # Fit a linear regression using mean squared error. regression = GaussianNet(n_feature=1, n_hidden=2, n_output=1) # RegressionModel() params = regression.parameters() optimizer = torch.optim.Adam(params, lr = 0.001) . Smaller comparable network . Laplace loss function . def LaplaceNLLLoss(input, target, scale, eps=1e-06, reduction=&#39;mean&#39;): loss = torch.log(2*scale) + torch.abs(input - target)/scale # Inputs and targets much have same shape input = input.view(input.size(0), -1) target = target.view(target.size(0), -1) if input.size() != target.size(): raise ValueError(&quot;input and target must have same size&quot;) # Second dim of scale must match that of input or be equal to 1 scale = scale.view(input.size(0), -1) if scale.size(1) != input.size(1) and scale.size(1) != 1: raise ValueError(&quot;scale is of incorrect size&quot;) # Check validity of reduction mode if reduction != &#39;none&#39; and reduction != &#39;mean&#39; and reduction != &#39;sum&#39;: raise ValueError(reduction + &quot; is not valid&quot;) # Entries of var must be non-negative if torch.any(scale &lt; 0): raise ValueError(&quot;scale has negative entry/entries&quot;) # Clamp for stability scale = scale.clone() with torch.no_grad(): scale.clamp_(min=eps) # Calculate loss (without constant) loss = (torch.log(2*scale) + torch.abs(input - target) / scale).view(input.size(0), -1).sum(dim=1) # Apply reduction if reduction == &#39;mean&#39;: return loss.mean() elif reduction == &#39;sum&#39;: return loss.sum() else: return loss . loss_func = LaplaceNLLLoss # Fit a linear regression using mean squared error. regression = GaussianNet(n_feature=1, n_hidden=2, n_output=1) # RegressionModel() params = regression.parameters() optimizer = torch.optim.Adam(params, lr = 0.001) ##################### # Training #################### my_images = [] fig, (ax1, ax2) = plt.subplots(figsize=(20,7), nrows=1, ncols=2) # train the network for epoch in range(4000): prediction, sigmas = regression(x) # input x and predict based on x loss_all = loss_func(prediction, y, sigmas, reduction=&#39;none&#39;) # must be (1. nn output, 2. target) loss = torch.mean(loss_all) #if t%10 == 0: print (loss) optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if np.mod(epoch, 100) == 0: print (loss) # plot and show learning process plt.cla() ax1.cla() ax1.set_title(&#39;Regression Analysis&#39;, fontsize=35) ax1.set_xlabel(&#39;Independent variable&#39;, fontsize=24) ax1.set_ylabel(&#39;Dependent variable&#39;, fontsize=24) ax1.set_xlim(-0.05, 1.0) ax1.set_ylim(-0.1, 1.0) ax1.scatter(x.data.numpy(), y.data.numpy(), color = &quot;orange&quot;) ax1.plot(x.data.numpy(), prediction.data.numpy(), &#39;g-&#39;, lw=3) dyfit = 2 * np.sqrt(sigmas.data.numpy()) # 2*sigma ~ 95% confidence region ax1.fill_between( np.squeeze(x.data.numpy()), np.squeeze(prediction.data.numpy() - dyfit), np.squeeze(prediction.data.numpy() + dyfit), color=&#39;gray&#39;, alpha=0.2) #l2_loss_plot_x = np.linspace(0,1,num=100) #y_plot_true = l2_loss_plot_x * scale_true + shift_true #ax1.plot(l2_loss_plot_x, y_plot_true, &#39;k&#39;) ax1.text(1.0, 0.1, &#39;Step = %d&#39; % epoch, fontdict={&#39;size&#39;: 24, &#39;color&#39;: &#39;red&#39;}) ax1.text(1.0, 0, &#39;Loss = %.4f&#39; % loss.data.numpy(), fontdict={&#39;size&#39;: 24, &#39;color&#39;: &#39;red&#39;}) diff = prediction.data.numpy() - y.data.numpy() ax2.cla() l2_loss_plot_x = np.linspace(-1,1,num=100) ax2.plot(l2_loss_plot_x, 0.5*l2_loss_plot_x**2, color=&quot;green&quot;, lw=3, alpha=0.5) ax2.scatter(diff, loss_all.data.numpy()) ax2.set_title(&#39;Loss &#39;, fontsize=35) ax2.set_xlabel(&#39;y - y_pred&#39;) ax2.set_ylim(-3.1, 3) ax2.set_xlim(-1, 1) # Used to return the plot as an image array # (https://ndres.me/post/matplotlib-animated-gifs-easily/) fig.canvas.draw() # draw the canvas, cache the renderer image = np.frombuffer(fig.canvas.tostring_rgb(), dtype=&#39;uint8&#39;) image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,)) my_images.append(image) . tensor(0.9880, grad_fn=&lt;MeanBackward0&gt;) tensor(0.7645, grad_fn=&lt;MeanBackward0&gt;) tensor(0.5300, grad_fn=&lt;MeanBackward0&gt;) tensor(0.3391, grad_fn=&lt;MeanBackward0&gt;) tensor(0.1639, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.0097, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.1699, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.3447, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.5617, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.6943, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.7138, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.7235, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.7313, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.7394, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.7475, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.7540, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.7607, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.7673, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.7731, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.7786, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.7843, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.7893, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.7944, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.7993, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8039, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8081, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8125, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8162, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8205, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8241, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8277, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8310, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8345, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8378, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8408, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8439, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8468, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8496, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8517, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.8545, grad_fn=&lt;MeanBackward0&gt;) . Analysis Laplace NLL Regression . The line is better fit to the data by avoiding to the noisy data. | . Cauchy NLL Loss function Robustness analysis . def CauchyNLLLoss(input, target, scale, eps=1e-06, reduction=&#39;mean&#39;): # Inputs and targets much have same shape input = input.view(input.size(0), -1) target = target.view(target.size(0), -1) if input.size() != target.size(): raise ValueError(&quot;input and target must have same size&quot;) # Second dim of scale must match that of input or be equal to 1 scale = scale.view(input.size(0), -1) if scale.size(1) != input.size(1) and scale.size(1) != 1: raise ValueError(&quot;scale is of incorrect size&quot;) # Check validity of reduction mode if reduction != &#39;none&#39; and reduction != &#39;mean&#39; and reduction != &#39;sum&#39;: raise ValueError(reduction + &quot; is not valid&quot;) # Entries of var must be non-negative if torch.any(scale &lt; 0): raise ValueError(&quot;scale has negative entry/entries&quot;) # Clamp for stability scale = scale.clone() with torch.no_grad(): scale.clamp_(min=eps) # Calculate loss (without constant) loss = (torch.log(3.14*scale) + torch.log(1 + ((input - target)**2)/scale**2)) .view(input.size(0), -1).sum(dim=1) # Apply reduction if reduction == &#39;mean&#39;: return loss.mean() elif reduction == &#39;sum&#39;: return loss.sum() else: return loss . loss_func = CauchyNLLLoss # Fit a linear regression using mean squared error. regression = GaussianNet(n_feature=1, n_hidden=2, n_output=1) # RegressionModel() params = regression.parameters() optimizer = torch.optim.Adam(params, lr = 0.01) ##################### # Training #################### my_images = [] fig, (ax1, ax2) = plt.subplots(figsize=(20,7), nrows=1, ncols=2) # train the network for epoch in range(4000): prediction, sigmas = regression(x) # input x and predict based on x loss_all = loss_func(prediction, y, sigmas, reduction=&#39;none&#39;) # must be (1. nn output, 2. target) loss = torch.mean(loss_all) #if t%10 == 0: print (loss) optimizer.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if np.mod(epoch, 100) == 0: print (loss) # plot and show learning process plt.cla() ax1.cla() ax1.set_title(&#39;Regression Analysis&#39;, fontsize=35) ax1.set_xlabel(&#39;Independent variable&#39;, fontsize=24) ax1.set_ylabel(&#39;Dependent variable&#39;, fontsize=24) ax1.set_xlim(-0.05, 1.0) ax1.set_ylim(-0.1, 1.0) ax1.scatter(x.data.numpy(), y.data.numpy(), color = &quot;orange&quot;) ax1.plot(x.data.numpy(), prediction.data.numpy(), &#39;g-&#39;, lw=3) dyfit = 2 * np.sqrt(sigmas.data.numpy()) # 2*sigma ~ 95% confidence region ax1.fill_between( np.squeeze(x.data.numpy()), np.squeeze(prediction.data.numpy() - dyfit), np.squeeze(prediction.data.numpy() + dyfit), color=&#39;gray&#39;, alpha=0.2) #l2_loss_plot_x = np.linspace(0,1,num=100) #y_plot_true = l2_loss_plot_x * scale_true + shift_true #ax1.plot(l2_loss_plot_x, y_plot_true, &#39;k&#39;) ax1.text(1.0, 0.1, &#39;Step = %d&#39; % epoch, fontdict={&#39;size&#39;: 24, &#39;color&#39;: &#39;red&#39;}) ax1.text(1.0, 0, &#39;Loss = %.4f&#39; % loss.data.numpy(), fontdict={&#39;size&#39;: 24, &#39;color&#39;: &#39;red&#39;}) diff = prediction.data.numpy() - y.data.numpy() ax2.cla() l2_loss_plot_x = np.linspace(-1,1,num=100) ax2.plot(l2_loss_plot_x, 0.5*l2_loss_plot_x**2, color=&quot;green&quot;, lw=3, alpha=0.5) ax2.scatter(diff, loss_all.data.numpy()) ax2.set_title(&#39;Loss &#39;, fontsize=35) ax2.set_xlabel(&#39;y - y_pred&#39;) ax2.set_ylim(-3.1, 3) ax2.set_xlim(-1, 1) # Used to return the plot as an image array # (https://ndres.me/post/matplotlib-animated-gifs-easily/) fig.canvas.draw() # draw the canvas, cache the renderer image = np.frombuffer(fig.canvas.tostring_rgb(), dtype=&#39;uint8&#39;) image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,)) my_images.append(image) . tensor(1.0078, grad_fn=&lt;MeanBackward0&gt;) tensor(-0.4010, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5053, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5102, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5142, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5175, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5139, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5223, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5241, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5173, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5267, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5276, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5244, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5288, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5292, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5294, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5296, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5298, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5299, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5299, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5300, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5300, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5300, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5247, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5300, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5301, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5300, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5300, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5301, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5300, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5301, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5301, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5301, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5300, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5301, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5300, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5301, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5301, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5282, grad_fn=&lt;MeanBackward0&gt;) tensor(-1.5301, grad_fn=&lt;MeanBackward0&gt;) . Analysis Cauchy NLL Regression . The line is better fit to the data than the gaussian by avoiding to the noisy data. | . Plotting Loss Surface . sigma_2 = np.logspace(0.1, 1, num=70, base=10 ) print (sigma_2.max(), sigma_2.min()) diff = np.linspace(-3, 5, num=70) def gauss_logL(xbar, sigma_2, mu): return -0.5*np.log(2*np.pi)-0.5*np.log(sigma_2)-0.5*(xbar -mu)**2/sigma_2 xbar = 1 logL = gauss_logL(xbar, sigma_2[:, np.newaxis], diff) logL -= logL.max() x_grid, sigma_grid = np.meshgrid(diff, sigma_2) logL = gauss_logL(xbar, sigma_grid, x_grid) logL = logL*-1 . 10.0 1.2589254117941673 . fig, ax = plt.subplots(figsize=(10,8),constrained_layout=True) ax = plt.axes(projection=&#39;3d&#39;) ax.plot_surface(x_grid, sigma_grid, logL, rstride=1, cstride=1, cmap=&#39;viridis&#39;, edgecolor=&#39;none&#39;) ax.set_title(&#39;NLL loss surface&#39;); . from itertools import cycle cycol = cycle(&#39;bgrcmk&#39;) fig, ax = plt.subplots(figsize=(10,7)) #for s_2 in [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 2]: for s_2 in [1e-3, 1e-2, 2e-3]: x = np.linspace(-3, 3, num=100) logL = (-1*gauss_logL(0, s_2, x))/1000 ax.plot(x, 0.5*x**2, color=&quot;cyan&quot;, lw=3, alpha=0.5) ax.plot(x,logL, c=next(cycol)) .",
            "url": "https://deebuls.github.io/devblog/probability/regression/uncertainty/robustness/2020/05/05/uncertainty-regression-robustness.html",
            "relUrl": "/probability/regression/uncertainty/robustness/2020/05/05/uncertainty-regression-robustness.html",
            "date": " • May 5, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Likelihood",
            "content": "What is Likelihood? . Likelihood and probablity seems to be same word in the layman domain, but in the stats domain they are different. . In the stats domain the likelihood or likelihood function is a measurement. It measures the distance between a statistical model and the input data. . What is a statistical model? . The diferent probability distributions available. For example, Gausian, gama, beta distribution, exponential for continuous data while Bernoulli, Dirichlet, multinomila distributions for discrete data. . How are statistical models represented? . By their parameters. For example for gaussian distribution the parameters are $ mu$ and $ sigma$ . . How do we select the statictical model? . Depends on many factors. This is the main decision to be made while designing a statistical model based learning. The different factors include: what is the data type: Continuous or discrete? Is it symmetrical or asymetrical? . Domain of the data, binary, real, etc | Does it decay or increase? | . . . etc | . A complete knowledge about the type data and the type of distribution is required to make the appropriate decision. . Common Probability distribution . Data Type Domain Distribution Python (numpy.random) Parameters . univariate, discrete, binary | $$ x in {0,1 } $$ | Bernoulli | binomial(1, p) | $$ p in[0,1]$$ | . univariate, discrete, multivalued | $$ x in { 1,2, dots, K }$$ | multinomial | multinomial(n, pvals) | $$pvals = [p_1, dots , p_k] $$ $$ sum_{i=1}^{K} p_i = 1 $$ | . univariate, continuous, unbounded | $$ x in mathbb{R} $$ | normal | normal(mu, sigma) | $$ mu in mathbb{R} $$ $$ sigma in mathbb{R}$$ | . #Lets make some distributions and find the likelihood to some data import numpy as np import matplotlib.pyplot as plt number_of_samples = 20; #parameters ; sample data from distribution (continuous data) mu, sigma = 12, 0.1 ; univariate_gaussian_samples = np.random.normal(mu, sigma, number_of_samples) mean = [0, 0]; cov = [[1, 0], [0, 100]]; multivariate_gaussian_samples = np.random.multivariate_normal(mean, cov, number_of_samples) #parameters ; sample data from distribution (discreta data) p = 0.8 ; bernoulli_samples = np.random.binomial(1, p, number_of_samples) pvals = [0.2, 0.6, 0.2] ; multinomial_samples = np.random.multinomial(number_of_samples, pvals) alpha, beta = 10, 20 ; beta_samples = np.random.beta(alpha, beta, number_of_samples) alpha = [10,20,10,90] ; dirchilet_samples = np.random.dirichlet(alpha, number_of_samples) . Goal of Likelihood . The goal of likelihood would be given the samples as shown above (beta_samples, dirichlet_samples etc) find the parameters of the corresponding distribution ((alpha, beta), alphas respectively) . Lets look into this process in the comming post . #hide .",
            "url": "https://deebuls.github.io/devblog/probability/python/2020/03/20/probability-likelihood.html",
            "relUrl": "/probability/python/2020/03/20/probability-likelihood.html",
            "date": " • Mar 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I’m a researcher at the Bonn-Aachen International Center for Information Technology (b-it) at the Autonomous Systems group in Bonn, Germany. . My research focus is on developing strategies for safe and reliable incorporation of deep learning methods into robotics. . I am also the lead develper with the b-it-bots team which won the RoboCup WorldCup and GermanOpen in the work league. . Publications . b-it-bots: Our Approach for Autonomous Robotics in Industrial Environments . Deebul Nair, Santosh Thoduka, Iman Awaad, Sven Schneider, Paul G. Plöger, Gerhard K. Kraetzschmar and students . Robot World Cup. Springer, Cham, 2019. . This paper presents the approach of our team, b-it-bots, in the RoboCup@Work competition which resulted in us winning the World Championship in Sydney in 2019. We describe our current hardware, including modifications made to the KUKA youBot, the underlying software framework and components developed for navigation, manipulation, perception and task planning for scenarios in industrial environments. Our combined 2D and 3D approach for object recognition has improved robustness and performance compared to previous years, and our task planning framework has moved us away from large state machines for high-level control. Future work includes closing the perception-manipulation loop for more robust grasping. Our open-source repository is available at https://github.com/b-it-bots/mas_industrial_robotics. . Performance Evaluation of Low-Cost Machine Vision Cameras forImage-Based Grasp Verification . Performance Evaluation of Low-Cost Machine Vision Cameras forImage-Based Grasp Verification . Under review in IROS 2020 . Grasp verification is advantageous for au-tonomous manipulation robots as they provide the feedbackrequired for higher level planning components about successfultask completion. However, a major obstacle in doing graspverification is sensor selection. In this paper, we propose a visionbased grasp verification system using machine vision cameras,with the verification problem formulated as an image classifi-cation task. Machine vision cameras consist of a camera anda processing unit capable of on-board deep learning inference.The inference in these low-power hardware are done near thedata source, reducing the robots dependence on a centralizedserver, leading to reduced latency, and improved reliability.Machine vision cameras provide the deep learning inferencecapabilities using different neural accelerators. Although, it isnot clear from the documentation of these cameras what is theeffect of these neural accelerators on performance metrics suchas latency and throughput. To systematically benchmark thesemachine vision cameras, we propose a parameterized modelgenerator that generates end to end models of ConvolutionalNeural Networks(CNN). Using these generated models webenchmark latency and throughput of two machine visioncameras, JeVois A33 and Sipeed Maix Bit. Our experimentsdemonstrate that the selected machine vision camera and thedeep learning models can robustly verify grasp with 97% perframe accuracy. . Open Source Contributions . BayesPy – Bayesian Python . BayesPy provides tools for Bayesian inference with Python. The user constructs a model as a Bayesian network, observes data and runs posterior inference. The goal is to provide a tool which is efficient, flexible and extendable enough for expert use but also accessible for more casual users. . b-it-bots . ROS software packages of b-it-bots for different robots. . . This website is powered by fastpages. Logo and favicon was designed by Darius Dan .",
          "url": "https://deebuls.github.io/devblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://deebuls.github.io/devblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}