<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2020-05-27">
<meta name="description" content="Learning uncertainty for regression using higher order distribution">

<title>devblog - Regression Uncertainty</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">devblog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Regression Uncertainty</h1>
                  <div>
        <div class="description">
          Learning uncertainty for regression using higher order distribution
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">probability</div>
                <div class="quarto-category">regression</div>
                <div class="quarto-category">uncertainty</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 27, 2020</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="regression" class="level2">
<h2 class="anchored" data-anchor-id="regression">Regression</h2>
<p>In machine learning problems we take input data <span class="math inline">\(x\)</span> and use it to predict the output data <span class="math inline">\(y\)</span>. If the predicted data is continuous or discrete. When the output data <span class="math inline">\(y\)</span> is continuous then the problem is called <strong>regression</strong> while if its is discrete then the problem is called <strong>classification</strong> .</p>
<p>Deep neural networks have prooved to be a good tool for solving regression tasks. Currently there are new improvements in making the regression task robust by not only predicting the output data <span class="math inline">\(y\)</span> but also the corresponding uncertainty.</p>
<p>In this blog we will look into 2 methods of representing uncertatinty from literature. 1. Epistemic uncertatinty (Data uncertatinty) [1] 2. Aleotary uncertainty (Model uncertatinty) [2]</p>
<p><img src="https://github.com/deebuls/devblog/blob/master/_notebooks/images/nig_regression.png?raw=1" class="img-fluid"></p>
<section id="hierarchical-modelling" class="level3">
<h3 class="anchored" data-anchor-id="hierarchical-modelling">Hierarchical Modelling</h3>
<p>In regression task can be modelled in 3 different methods based on which all uncertainty needs to be accounted for during architecture of the deep network. The different layers and their hierarchichal structre is represented in the image above.</p>
<table class="table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>loss function</th>
<th>uncertatinty</th>
<th>output</th>
<th>distribution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Deterministic Regression</td>
<td>mean square error</td>
<td>not modeled</td>
<td><span class="math inline">\(\hat{y}\)</span></td>
<td>-</td>
</tr>
<tr class="even">
<td>Likelihood Regression</td>
<td>Maximum likelihood estimation</td>
<td>data</td>
<td><span class="math inline">\(\hat{y}\)</span>, <span class="math inline">\(\sigma^2\)</span></td>
<td>Normal</td>
</tr>
<tr class="odd">
<td>Evidential Regression</td>
<td>Maximum likelihood estimation</td>
<td>data + model</td>
<td><span class="math inline">\(\hat{y}\)</span>,<span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\lambda\)</span></td>
<td>Normal inverse gamma</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="dataset" class="level2">
<h2 class="anchored" data-anchor-id="dataset">Dataset</h2>
<p>We will start by creating a one dimension synthetic regression dataset. The synthetic dataset has added <em>priori</em> noise. The dataset is generated from <span class="math display">\[ y =\frac{ \sin(3x)}{3x} + \epsilon \]</span> where <span class="math inline">\(\epsilon \sim \mathcal{N}(0, 0.02)\)</span></p>
<p>The training dataset consist of <span class="math inline">\(x\)</span> values between <span class="math inline">\(-3 \leq x \leq 3\)</span>; while the test dataset consist of values outside of training <span class="math inline">\(-4 \leq x \leq 4\)</span></p>
<div class="cell" data-outputid="349c2312-eb05-402b-e3de-fa26958b3c23" data-execution_count="0">
<div class="cell-output cell-output-display">
<p><img src="2020-05-27-uncertainty-regression_files/figure-html/cell-3-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="model" class="level2">
<h2 class="anchored" data-anchor-id="model">Model</h2>
<p>We will be using a common model for all the different learning formulation.</p>
<div class="cell" data-execution_count="0">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># this is one way to define a network</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Net(torch.nn.Module):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_feature, n_hidden, n_output):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Net, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden <span class="op">=</span> torch.nn.Linear(n_feature, n_hidden)   <span class="co"># hidden layer</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_2 <span class="op">=</span> torch.nn.Linear(n_hidden, n_hidden)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.predict <span class="op">=</span> torch.nn.Linear(n_hidden, n_output)   <span class="co"># output layer</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.hidden(x)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.hidden_2(x))      <span class="co"># activation function for hidden layer</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.predict(x)             <span class="co"># linear output</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="deterministic-regression" class="level2">
<h2 class="anchored" data-anchor-id="deterministic-regression">Deterministic Regression</h2>
<section id="loss-function" class="level3">
<h3 class="anchored" data-anchor-id="loss-function">Loss function</h3>
<p>Mean square error <span class="math display">\[ \mathcal{L} = \frac{1}{2} \vert \vert y_i - \hat{y} \vert \vert ^2\]</span></p>
</section>
<section id="output" class="level3">
<h3 class="anchored" data-anchor-id="output">Output</h3>
<p>The model will have single output <span class="math inline">\(\hat{y}\)</span></p>
<div class="cell" data-outputid="80a05ae7-abef-4188-f863-64543fb06986" data-execution_count="0">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>msenet <span class="op">=</span> Net(n_feature<span class="op">=</span><span class="dv">1</span>, n_hidden<span class="op">=</span><span class="dv">10</span>, n_output<span class="op">=</span><span class="dv">1</span>)     <span class="co"># define the network</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(msenet)  <span class="co"># net architecture</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">#optimizer = torch.optim.SGD(net.parameters(), lr=0.01)</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(msenet.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>loss_func <span class="op">=</span> torch.nn.MSELoss()  <span class="co"># this is for regression mean squared loss</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># train the network</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10000</span>):</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    prediction <span class="op">=</span> msenet(x)     <span class="co"># input x and predict based on x</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_func(prediction, y)     <span class="co"># must be (1. nn output, 2. target)</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()   <span class="co"># clear gradients for next train</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    loss.backward()         <span class="co"># backpropagation, compute gradients</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    optimizer.step()        <span class="co"># apply gradients</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> t<span class="op">%</span><span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>: <span class="bu">print</span>(loss)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>prediction <span class="op">=</span> msenet(test_x)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>plot_prediction(test_x, test_y, prediction)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Net(
  (hidden): Linear(in_features=1, out_features=10, bias=True)
  (hidden_2): Linear(in_features=10, out_features=10, bias=True)
  (predict): Linear(in_features=10, out_features=1, bias=True)
)
tensor(0.5153, grad_fn=&lt;MseLossBackward&gt;)
tensor(0.0478, grad_fn=&lt;MseLossBackward&gt;)
tensor(0.0023, grad_fn=&lt;MseLossBackward&gt;)
tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;)
tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;)
tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;)
tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;)
tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;)
tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;)
tensor(0.0019, grad_fn=&lt;MseLossBackward&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2020-05-27-uncertainty-regression_files/figure-html/cell-6-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="likelihood-regression" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-regression">Likelihood Regression</h2>
<section id="loss-function-1" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-1">Loss function</h3>
<p>In here we will use the maximum likelihood estimation error, which is also called as the Negative log likelihood.</p>
<p><span class="math display">\[ \mathcal{L} = -\left(-0.5\log(2\pi) - 0.5\sum\log(\sigma^2) - 0.5\sum\frac{(y - \hat{y})^2}{\sigma^2} \right) \]</span></p>
</section>
<section id="output-1" class="level3">
<h3 class="anchored" data-anchor-id="output-1">Output</h3>
<p>We will create a deep net with 2 output variables. - predicted output <span class="math inline">\(\hat{y}\)</span> - the variance of the normal distribution <span class="math inline">\(\sigma^2\)</span></p>
</section>
</section>
<section id="note" class="level2">
<h2 class="anchored" data-anchor-id="note">Note</h2>
<p>We aer also adding a regularizer <span class="math inline">\(\frac{ \| y - \hat{y} \| }{ \sigma^2}\)</span>. This helps to regluarize the learning. It is inversely related to <span class="math inline">\(\sigma^2\)</span> and it scales based on distance from the actual value.</p>
<p>The regularizer ensures that values predicted far from true values have higher variance.So for values near to the predicted it gives less loss but values far away from prediction gives a large loss.</p>
<div class="cell" data-outputid="5a84980f-151c-40a6-f01d-7acd27f5635c" data-execution_count="0">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss Function</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLELoss(torch.nn.Module):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, weight<span class="op">=</span><span class="va">None</span>, size_average<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">super</span>(MLELoss, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> forward(<span class="va">self</span>, inputs, targets, smooth<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> targets.view(<span class="op">-</span><span class="dv">1</span>) <span class="co">#converting variable to single dimension</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> inputs[:,<span class="dv">0</span>].view(<span class="op">-</span><span class="dv">1</span>) <span class="co">#extracting mu and sigma_2</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    logsigma_2 <span class="op">=</span> inputs[:,<span class="dv">1</span>].view(<span class="op">-</span><span class="dv">1</span>) <span class="co">#logsigma and exp drives the variable to positive values always</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    sigma_2 <span class="op">=</span> torch.exp(logsigma_2)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    kl_divergence <span class="op">=</span> (targets <span class="op">-</span> mu)<span class="op">**</span><span class="dv">2</span><span class="op">/</span>sigma_2 <span class="co">#Regularizer </span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    mse <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> torch.<span class="bu">sum</span>(((targets <span class="op">-</span> mu)<span class="op">**</span><span class="dv">2</span>)<span class="op">/</span>sigma_2)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    sigma_trace <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span>  <span class="op">*</span> torch.<span class="bu">sum</span>(sigma_2)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    log2pi <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span>  np.log(<span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    J <span class="op">=</span>  mse <span class="op">+</span> sigma_trace <span class="op">+</span> log2pi</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="op">-</span>J <span class="op">+</span> kl_divergence.<span class="bu">sum</span>()</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>mlenet <span class="op">=</span> Net(n_feature<span class="op">=</span><span class="dv">1</span>, n_hidden<span class="op">=</span><span class="dv">10</span>, n_output<span class="op">=</span><span class="dv">2</span>)     <span class="co"># define the network</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mlenet)  <span class="co"># net architecture</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(mlenet.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>loss_func <span class="op">=</span> MLELoss()  <span class="co"># this is for regression mean squared loss</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="co"># train the network</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10000</span>):</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    prediction <span class="op">=</span> mlenet(x)     <span class="co"># input x and predict based on x</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_func(prediction, y)     <span class="co"># must be (1. nn output, 2. target)</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()   <span class="co"># clear gradients for next train</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    loss.backward()         <span class="co"># backpropagation, compute gradients</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    optimizer.step()        <span class="co"># apply gradients</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> t<span class="op">%</span><span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>: </span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>      <span class="bu">print</span>(loss)</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>prediction <span class="op">=</span> mlenet(test_x)</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> prediction[:,<span class="dv">0</span>]</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>sigma2 <span class="op">=</span> torch.exp(prediction[:,<span class="dv">1</span>])</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> torch.sqrt(sigma2)</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>plot_prediction(test_x, test_y, mu, sigma)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Net(
  (hidden): Linear(in_features=1, out_features=10, bias=True)
  (hidden_2): Linear(in_features=10, out_features=10, bias=True)
  (predict): Linear(in_features=10, out_features=2, bias=True)
)
tensor(68.1900, grad_fn=&lt;AddBackward0&gt;)
tensor(6.4617, grad_fn=&lt;AddBackward0&gt;)
tensor(6.1683, grad_fn=&lt;AddBackward0&gt;)
tensor(6.1068, grad_fn=&lt;AddBackward0&gt;)
tensor(6.0325, grad_fn=&lt;AddBackward0&gt;)
tensor(5.9853, grad_fn=&lt;AddBackward0&gt;)
tensor(5.9657, grad_fn=&lt;AddBackward0&gt;)
tensor(5.9485, grad_fn=&lt;AddBackward0&gt;)
tensor(5.9929, grad_fn=&lt;AddBackward0&gt;)
tensor(5.9366, grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2020-05-27-uncertainty-regression_files/figure-html/cell-7-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="evidential-regression" class="level2">
<h2 class="anchored" data-anchor-id="evidential-regression">Evidential Regression</h2>
<p>Evidential regression is based on paper [2] (Amini &amp; e.t.al, 2019), which is based on the ideas of [3, 4] that if we represent the output of the model with a higher order data distribution its possible to model the data and model uncertainties.</p>
<section id="loss" class="level3">
<h3 class="anchored" data-anchor-id="loss">Loss</h3>
<p><span class="math display">\[ \mathcal{L} = \left( \frac{ \Gamma(\alpha - 0.5)}{4\Gamma(\alpha)\lambda\sqrt\beta} \right) \left( 2\beta(1 + \lambda) + (2\alpha -1)\lambda(y_i - \hat{y})^2 \right)\]</span></p>
</section>
<section id="output-2" class="level3">
<h3 class="anchored" data-anchor-id="output-2">output</h3>
<p>The model has 4 outputs - <span class="math inline">\(\hat{y}\)</span> - <span class="math inline">\(\alpha\)</span> - <span class="math inline">\(\beta\)</span> - <span class="math inline">\(\lambda\)</span></p>
</section>
<section id="regularizer" class="level3">
<h3 class="anchored" data-anchor-id="regularizer">Regularizer</h3>
<p>Regularizer is required to penalize the loss function for OOD predictions.</p>
<p><span class="math display">\[ \| y - \hat{y}\| ^2 (2 \alpha + \lambda)\]</span></p>
<div class="cell" data-outputid="c547f7bc-0b7c-45c8-9bf8-44437e99d596" data-execution_count="0">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EvidentialLoss(torch.nn.Module):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, weight<span class="op">=</span><span class="va">None</span>, size_average<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">super</span>(EvidentialLoss, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> forward(<span class="va">self</span>, inputs, targets, smooth<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> targets.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> inputs[:,<span class="dv">0</span>].view(<span class="op">-</span><span class="dv">1</span>) <span class="co">#first column is mu,delta, predicted value</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    loga <span class="op">=</span> inputs[:,<span class="dv">1</span>].view(<span class="op">-</span><span class="dv">1</span>) <span class="co">#alpha</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    logb <span class="op">=</span> inputs[:,<span class="dv">2</span>].view(<span class="op">-</span><span class="dv">1</span>) <span class="co">#beta</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    logl <span class="op">=</span> inputs[:,<span class="dv">3</span>].view(<span class="op">-</span><span class="dv">1</span>) <span class="co">#lamda</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> torch.exp(loga)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> torch.exp(logb)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    l <span class="op">=</span> torch.exp(logl)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    term1 <span class="op">=</span> (torch.exp(torch.lgamma(a <span class="op">-</span> <span class="fl">0.5</span>)))<span class="op">/</span>(<span class="dv">4</span> <span class="op">*</span> torch.exp(torch.lgamma(a)) <span class="op">*</span> l <span class="op">*</span> torch.sqrt(b))</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print("term1 :", term1)</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    term2 <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> b <span class="op">*</span>(<span class="dv">1</span> <span class="op">+</span> l) <span class="op">+</span> (<span class="dv">2</span><span class="op">*</span>a <span class="op">-</span> <span class="dv">1</span>)<span class="op">*</span>l<span class="op">*</span>(y <span class="op">-</span> targets)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print("term2 :", term2)</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    J <span class="op">=</span> term1 <span class="op">*</span> term2</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print("J :", J)</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    Kl_divergence <span class="op">=</span> torch.<span class="bu">abs</span>(y <span class="op">-</span> targets) <span class="op">*</span> (<span class="dv">2</span><span class="op">*</span>a <span class="op">+</span> l)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Kl_divergence = ((y - targets)**2) * (2*a + l)</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print ("KL ",Kl_divergence.data.numpy())</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> J <span class="op">+</span> Kl_divergence</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print ("loss :", loss)</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss.mean()</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>evnet <span class="op">=</span> Net(n_feature<span class="op">=</span><span class="dv">1</span>, n_hidden<span class="op">=</span><span class="dv">10</span>, n_output<span class="op">=</span><span class="dv">4</span>)     <span class="co"># define the network</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(evnet)  <span class="co"># net architecture</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(evnet.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>loss_func <span class="op">=</span> EvidentialLoss() </span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a><span class="co"># train the network</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20000</span>):</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>    prediction <span class="op">=</span> evnet(x)     <span class="co"># input x and predict based on x</span></span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_func(prediction, y)     <span class="co"># must be (1. nn output, 2. target)</span></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()   <span class="co"># clear gradients for next train</span></span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>    loss.backward()         <span class="co"># backpropagation, compute gradients</span></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>    optimizer.step()        <span class="co"># apply gradients</span></span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> t<span class="op">%</span><span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>: <span class="bu">print</span>(loss)</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>prediction <span class="op">=</span> evnet(test_x)</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> prediction[:,<span class="dv">0</span>].view(<span class="op">-</span><span class="dv">1</span>) <span class="co">#first column is mu,delta, predicted value</span></span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> prediction[:,<span class="dv">1</span>].view(<span class="op">-</span><span class="dv">1</span>) <span class="co">#alpha</span></span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> prediction[:,<span class="dv">2</span>].view(<span class="op">-</span><span class="dv">1</span>) <span class="co">#beta</span></span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> prediction[:,<span class="dv">3</span>].view(<span class="op">-</span><span class="dv">1</span>) <span class="co">#lamda</span></span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.exp(a)<span class="op">;</span> b <span class="op">=</span> torch.exp(b)<span class="op">;</span> l <span class="op">=</span> torch.exp(l)</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a>var <span class="op">=</span> b <span class="op">/</span> ((a <span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>l) <span class="co">#epistemic/ model/prediciton uncertaitnty</span></span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>e <span class="op">=</span> b <span class="op">/</span> (a <span class="op">-</span> <span class="dv">1</span>) <span class="co"># aleatoric uncertainty/ data uncertainty</span></span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>plot_prediction(test_x, test_y, mu, var, e)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Net(
  (hidden): Linear(in_features=1, out_features=10, bias=True)
  (hidden_2): Linear(in_features=10, out_features=10, bias=True)
  (predict): Linear(in_features=10, out_features=4, bias=True)
)
tensor(3.1019, grad_fn=&lt;MeanBackward0&gt;)
tensor(0.1492, grad_fn=&lt;MeanBackward0&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2020-05-27-uncertainty-regression_files/figure-html/cell-8-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<ul>
<li>Loss functions of maximum likelihood and evidence have been implemented from formulas from the paper. :::{.callout-important}</li>
</ul>
<p>Leasons Learned &nbsp;- how to treat loss function when one of the output variable is always positive. (log and exponential to rescue) &nbsp;-The difference between optimize and loss function and do you sum and add constant ones or do you add constant in all equation and sum.</p>
<p>:::</p>
<ul>
<li>The trianing is working on some runs not all.</li>
<li>The model learns the function easily but learning the uncertainty in unkonw region takes longer number of iterations. Needs to be further investigated.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>ToDo : Test functions for the loss function written</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>ToDo : What is stopping condition? when to stop learning?</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>ToDo : Replace the regularizer in regularizer in likelihood loss with actual regularizer.</p>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Why is uncertatiny different for both the models. Evidential model seems to be very confident in the known region, which seems to be fishy</p>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>The loss function doesnt give same results all the time, so needs to be further investigated</p>
</div>
</div>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>[1] Kendall, Alex, and Yarin Gal. “What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?” ArXiv:1703.04977 [Cs], October 5, 2017. http://arxiv.org/abs/1703.04977.</p>
<p>[2] Amini, Alexander, Wilko Schwarting, Ava Soleimany, and Daniela Rus. “Deep Evidential Regression.” ArXiv:1910.02600 [Cs, Stat], October 7, 2019. http://arxiv.org/abs/1910.02600.</p>
<p>[3] Sensoy, Murat, Lance Kaplan, and Melih Kandemir. “Evidential Deep Learning to Quantify Classification Uncertainty,” n.d., 11.</p>
<p>[4] Malinin, Andrey, and Mark Gales. “Predictive uncertainty estimation via prior networks.” Advances in Neural Information Processing Systems. 2018.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>