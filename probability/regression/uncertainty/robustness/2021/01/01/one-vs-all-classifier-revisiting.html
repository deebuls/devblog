<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">One-vs-All Classifier</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-01-01T00:00:00-06:00" itemprop="datePublished">
        Jan 1, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      1 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/devblog/categories/#probability">probability</a>
        &nbsp;
      
        <a class="category-tags-link" href="/devblog/categories/#regression">regression</a>
        &nbsp;
      
        <a class="category-tags-link" href="/devblog/categories/#uncertainty">uncertainty</a>
        &nbsp;
      
        <a class="category-tags-link" href="/devblog/categories/#robustness">robustness</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/deebuls/devblog/tree/master/_notebooks/2021-01-01-one-vs-all-classifier-revisiting.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/devblog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/deebuls/devblog/master?filepath=_notebooks%2F2021-01-01-one-vs-all-classifier-revisiting.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/devblog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/deebuls/devblog/blob/master/_notebooks/2021-01-01-one-vs-all-classifier-revisiting.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/devblog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#The-paper">The paper </a></li>
<li class="toc-entry toc-h2"><a href="#Abstract">Abstract </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Problem">Problem </a></li>
<li class="toc-entry toc-h3"><a href="#How">How </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Introduction">Introduction </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Unique-selling-points">Unique selling points </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Experiments">Experiments </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Imagenet-">Imagenet  </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-01-01-one-vs-all-classifier-revisiting.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://colab.research.google.com/github/deebuls/devblog/blob/master/_notebooks/2021-01-01-one-vs-all-classifier-revisiting.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-paper">
<a class="anchor" href="#The-paper" aria-hidden="true"><span class="octicon octicon-link"></span></a>The paper<a class="anchor-link" href="#The-paper"> </a>
</h2>
<p>Padhy, S. et al. (2020) ‘Revisiting One-vs-All Classifiers for Predictive Uncertainty and Out-of-Distribution Detection in Neural Networks’, arXiv:2007.05134 [cs, stat]. Available at: <a href="http://arxiv.org/abs/2007.05134">http://arxiv.org/abs/2007.05134</a> (Accessed: 26 January 2021).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Abstract">
<a class="anchor" href="#Abstract" aria-hidden="true"><span class="octicon octicon-link"></span></a>Abstract<a class="anchor-link" href="#Abstract"> </a>
</h2>
<h3 id="Problem">
<a class="anchor" href="#Problem" aria-hidden="true"><span class="octicon octicon-link"></span></a>Problem<a class="anchor-link" href="#Problem"> </a>
</h3>
<ul>
<li>Out-of-Distribution - finding on which samples the classifier should not predict </li>
<li>OOD using uncertainty estimate </li>
<li>uncertainty estimate - correctly estimate the confidence (uncertainty) in the prediction</li>
</ul>
<h3 id="How">
<a class="anchor" href="#How" aria-hidden="true"><span class="octicon octicon-link"></span></a>How<a class="anchor-link" href="#How"> </a>
</h3>
<ul>
<li>Using oves-vs-all classifier</li>
<li>distance-based logit representation to encode uncertainty</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Introduction">
<a class="anchor" href="#Introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction<a class="anchor-link" href="#Introduction"> </a>
</h1>
<ul>
<li>
<p>Capturing epistemic uncertainty is more important, as it captures model's lack of knowledge about data</p>
</li>
<li>
<p>Three different paradigms to measure uncertainty</p>
<ol>
<li>in-distribution calibration of models measured on an i.i.d. test set, </li>
<li>robustness under dataset shift, and </li>
<li>anomaly/out-of-distribution (OOD) detection,
which measures the ability of models to assign low confidence predictions on inputs far away from the training data.</li>
</ol>
</li>
</ul>
<h2 id="Unique-selling-points">
<a class="anchor" href="#Unique-selling-points" aria-hidden="true"><span class="octicon octicon-link"></span></a>Unique selling points<a class="anchor-link" href="#Unique-selling-points"> </a>
</h2>
<ul>
<li>we first study the contribution of the loss function used during training to the quality of predictive uncertainty of models. </li>
<li>Specifically, we show why the parametrization of the probabilities underlying softmax cross-entropy loss are ill-suited for uncertainty estimation. </li>
<li>We then propose two simple replacements to the parametrization of
the probability distribution: <ol>
<li>a one-vs-all normalization
scheme that does not force all points in the input space to
map to one of the classes, thereby naturally capturing the
notion of “none of the above”, and </li>
<li>a distance-based logit representation to encode uncertainty as a function of distance to the training manifold.</li>
</ol>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Experiments">
<a class="anchor" href="#Experiments" aria-hidden="true"><span class="octicon octicon-link"></span></a>Experiments<a class="anchor-link" href="#Experiments"> </a>
</h1>
<ol>
<li>
<p>Under Dataset Shift</p>
<ul>
<li>CIFAR 10 Corrupted different intensity</li>
<li>CIFAR 100 Corrupted</li>
</ul>
</li>
<li>
<p>OOD</p>
<ul>
<li>CIFAR 100  vs CIFAR10</li>
<li>CIFAR100 vs SVHN</li>
</ul>
</li>
<li>
<p>Comparison of learned class centers</p>
</li>
<li>
<p>Reliability Diagrams</p>
</li>
<li><h2 id="Imagenet-">
<a class="anchor" href="#Imagenet-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Imagenet <a class="anchor-link" href="#Imagenet-"> </a>
</h2></li>
<li>
<p>CLINC Intent Classification Dataset</p>
</li>
</ol>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="deebuls/devblog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/devblog/probability/regression/uncertainty/robustness/2021/01/01/one-vs-all-classifier-revisiting.html" hidden></a>
</article>